@Manual{pkg:Matrix,
    title = {\pkg{Matrix}: Sparse and Dense Matrix Classes and Methods},
    author = {Douglas Bates and Martin Maechler and Mikael Jagan},
    year = {2024},
    note = {\proglang{R} package version 1.7-0},
    doi = {10.32614/CRAN.package.Matrix},
}


@article{cantoni2001robust,
  title={Robust Inference for Generalized Linear Models},
  author={Cantoni, Eva and Ronchetti, Elvezio},
  journal={Journal of the American Statistical Association},
  volume={96},
  number={455},
  pages={1022--1030},
  year={2001},
  publisher={Taylor \& Francis},
  doi = {10.1198/016214501753209004}
}

@article{bernanke2005measuring,
  title={{Measuring the effects of monetary policy: a factor-augmented vector autoregressive (FAVAR) approach}},
  author={Bernanke, Ben S. and Boivin, Jean and Eliasz, Piotr},
  journal={The Quarterly journal of economics},
  volume={120},
  number={1},
  pages={387--422},
  year={2005},
  publisher={MIT Press}
}

@article{BeyelerKaufmannFAVAR,
  title={{Reduced-form factor augmented VAR—Exploiting sparsity to include meaningful factors}},
  author={Beyeler, Simon and Kaufmann, Sylvia},
  journal={Journal of Applied Econometrics},
  volume={36},
  number={7},
  pages={989--1012},
  year={2021},
  doi             = {https://doi.org/10.1002/jae.2852}
  }

  @article{BayComprRegr,
author = {Rajarshi Guhaniyogi and David B. Dunson},
title = {{Bayesian Compressed Regression}},
journal = {Journal of the American Statistical Association},
volume = {110},
number = {512},
pages = {1500-1514},
year  = {2015},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1080/01621459.2014.969425},
URL = {
        https://doi.org/10.1080/01621459.2014.969425},
eprint = {
        https://doi.org/10.1080/01621459.2014.969425}
}

@misc{piironen2017iterative,
      title={{Iterative Supervised Principal Components}},
      author={Juho Piironen and Aki Vehtari},
      year={2017},
      eprint={1710.06229},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{AdragniCook2009SDR,
author = {Adragni, Kofi P.  and Cook, R. Dennis },
title = {{Sufficient dimension reduction and prediction in regression}},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
volume = {367},
number = {1906},
pages = {4385-4405},
year = {2009},
doi = {https://doi.org/10.1098/rsta.2009.0110},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2009.0110},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2009.0110},
abstract = { Dimension reduction for regression is a prominent issue today because technological advances now allow scientists to routinely formulate regressions in which the number of predictors is considerably larger than in the past. While several methods have been proposed to deal with such regressions, principal components (PCs) still seem to be the most widely used across the applied sciences. We give a broad overview of ideas underlying a particular class of methods for dimension reduction that includes PCs, along with an introduction to the corresponding methodology. New methods are proposed for prediction in regressions with many predictors. }
}

@book{hastie2009elements,
  title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author={Hastie, T. and Tibshirani, R. and Friedman, J.H.},
  isbn={9780387848846},
  lccn={2008941148},
  series={Springer series in statistics},
  url={https://books.google.at/books?id=eBSgoAEACAAJ},
  year={2009},
  publisher={Springer}
}

@article{bair2006supervisedPC,
author = {Eric Bair and Trevor Hastie and Debashis Paul and Robert Tibshirani},
title = {{Prediction by Supervised Principal Components}},
journal = {Journal of the American Statistical Association},
volume = {101},
number = {473},
pages = {119-137},
year  = {2006},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1198/016214505000000628},
URL = {
        https://doi.org/10.1198/016214505000000628},
eprint = {
        https://doi.org/10.1198/016214505000000628}
}

@article{TSIONAS2022103952,
title = {{Estimation of large dimensional time varying VARs using copulas}},
journal = {European Economic Review},
volume = {141},
pages = {103952},
year = {2022},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2021.103952},
url = {https://www.sciencedirect.com/science/article/pii/S0014292121002439},
author = {Mike G. Tsionas and Marwan Izzeldin and Lorenzo Trapani},
keywords = {Vector AutoRegression, Time-varying parameters, Heteroskedasticity, Copulas},
abstract = {This paper provides a simple, yet reliable, alternative to the (Bayesian) estimation of large multivariate VARs with time variation in the conditional mean equations and/or in the covariance structure. The original multivariate, n-dimensional model is treated as a set of n univariate estimation problems, and cross-dependence is handled through the use of a copula. This makes it possible to run the estimation of each univariate equation in parallel. Thus, only univariate distribution functions are needed when estimating the individual equations, which are often available in closed form, and easy to handle with MCMC (or other techniques). Thereafter, the individual posteriors are combined with the copula, so obtaining a joint posterior which can be easily resampled. We illustrate our approach using various examples of large time-varying parameter VARs with 129 and even 215 macroeconomic variables.}
}

@article{Dunson2020TargRandProj,
author = { Minerva   Mukhopadhyay  and  David B.   Dunson },
title = {{Targeted Random Projection for Prediction From High-Dimensional Features}},
journal = {Journal of the American Statistical Association},
volume = {115},
number = {532},
pages = {1998-2010},
year  = {2020},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2019.1677240},
}

@article{CookNi2005IRforDR,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/27590565},
 abstract = {A family of dimension-reduction methods, the inverse regression (IR) family, is developed by minimizing a quadratic objective function. An optimal member of this family, the inverse regression estimator (IRE), is proposed, along with inference methods and a computational algorithm. The IRE has at least three desirable properties: (1) Its estimated basis of the central dimension reduction subspace is asymptotically efficient, (2) its test statistic for dimension has an asymptotic chi-squared distribution, and (3) it provides a chi-squared test of the conditional independence hypothesis that the response is independent of a selected subset of predictors given the remaining predictors. Current methods like sliced inverse regression belong to a suboptimal class of the IR family. Comparisons of these methods are reported through simulation studies. The approach developed here also allows a relatively straightforward derivation of the asymptotic null distribution of the test statistic for dimension used in sliced average variance estimation.},
 author = {R. Dennis Cook and Liqiang Ni},
 journal = {Journal of the American Statistical Association},
 number = {470},
 pages = {410--428},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {{Sufficient Dimension Reduction via Inverse Regression: A Minimum Discrepancy Approach}},
 volume = {100},
 year = {2005}
}

@article{Huber1985PP,
author = {Peter J. Huber},
title = {{Projection Pursuit}},
volume = {13},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {435 -- 475},
keywords = {Computer tomography, minimum entropy, multivariate data analysis, principal components, Projection pursuit, robust multivariate methods},
year = {1985},
doi = {https://doi.org/10.1214/aos/1176349519},
URL = {https://doi.org/10.1214/aos/1176349519}
}

@article{Reich2011SufficientDR,
  title={{Sufficient dimension reduction via bayesian mixture modeling.}},
  author={Brian J. Reich and Howard D. Bondell and Lexin Li},
  journal={Biometrics},
  year={2011},
  volume={67 3},
  pages={
          886-95
        }
}

@article{POWER2021BayMAvgSIR,
title = {{Bayesian model averaging sliced inverse regression}},
journal = {Statistics and Probability Letters},
volume = {174},
pages = {109103},
year = {2021},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2021.109103},
url = {https://www.sciencedirect.com/science/article/pii/S0167715221000651},
author = {Michael Declan Power and Yuexiao Dong},
keywords = {Central space, Markov chain Monte Carlo, Sufficient dimension reduction},
abstract = {As a popular sufficient dimension reduction method, sliced inverse regression (SIR) (Li, 1991) involves all the predictors. We propose Bayesian model averaging SIR when the central space only involves a subset of the predictors.}
}

@book{li2018sufficient,
  title={{Sufficient Dimension Reduction: Methods and Applications with R}},
  author={Li, B.},
  isbn={9781498704489},
  series={Chapman \& Hall/CRC Monographs on Statistics and Applied Probability},
  url={https://books.google.at/books?id=5pdYDwAAQBAJ},
  year={2018},
  publisher={CRC Press}
}

@article{Barshan2011SupervisedKPCA,
  title={{Supervised principal component analysis: Visualization, classification and regression on subspaces and submanifolds}},
  author={Elnaz Barshan and Ali Ghodsi and Zohreh Azimifar and Mansoor Zolghadri Jahromi},
  journal={Pattern Recognit.},
  year={2011},
  volume={44},
  pages={1357-1371}
}

@misc{ghojogh2021SDR_survey,
      title={{Sufficient Dimension Reduction for High-Dimensional Regression and Low-Dimensional Embedding: Tutorial and Survey}},
      author={Benyamin Ghojogh and Ali Ghodsi and Fakhri Karray and Mark Crowley},
      year={2021},
      eprint={2110.09620},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{BARCARU2019_SPP,
title = {{Supervised projection pursuit – A dimensionality reduction technique optimized for probabilistic classification}},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {194},
pages = {103867},
year = {2019},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2019.103867},
url = {https://www.sciencedirect.com/science/article/pii/S0169743919300309},
author = {Andrei Barcaru},
abstract = {An important step in multivariate analysis is the dimensionality reduction, which allows for a better classification and easier visualization of the class structures in the data. Techniques like PCA, PLS-DA and LDA are most often used to explore the patterns in the data and to reduce the dimensions. Yet the data does not always reveal properly the structures wen these techniques are applied. To this end, a supervised projection pursuit (SuPP) is proposed in this article, based on Jensen-Shannon divergence. The combination of this metric with powerful Monte Carlo based optimization algorithm, yielded a versatile dimensionality reduction technique capable of working with highly dimensional data and missing observations. Combined with Naïve Bayes (NB) classifier, SuPP proved to be a powerful preprocessing tool for classification. Namely, on the Iris data set, the prediction accuracy of SuPP-NB is significantly higher than the prediction accuracy of PCA-NB, (p-value ≤ 4.02E-05 in a 2D latent space, p-value ≤ 3.00E-03 in a 3D latent space) and significantly higher than the prediction accuracy of PLS-DA (p-value ≤ 1.17E-05 in a 2D latent space and p-value ≤ 3.08E-03 in a 3D latent space). The significantly higher accuracy for this particular data set is a strong evidence of a better class separation in the latent spaces obtained with SuPP.}
}

@article{ESPEZUA2015PP_HD_smalldata,
title = {A Projection Pursuit framework for supervised dimension reduction of high dimensional small sample datasets},
journal = {Neurocomputing},
volume = {149},
pages = {767-776},
year = {2015},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2014.07.057},
url = {https://www.sciencedirect.com/science/article/pii/S0925231214010091},
author = {Soledad Espezua and Edwin Villanueva and Carlos D. Maciel and André Carvalho},
keywords = {Projection Pursuit, Classification, Gene expression, Dimension reduction},
abstract = {The analysis and interpretation of datasets with large number of features and few examples has remained as a challenging problem in the scientific community, owing to the difficulties associated with the curse-of-the-dimensionality phenomenon. Projection Pursuit (PP) has shown promise in circumventing this phenomenon by searching low-dimensional projections of the data where meaningful structures are exposed. However, PP faces computational difficulties in dealing with datasets containing thousands of features (typical in genomics and proteomics) due to the vast quantity of parameters to optimize. In this paper we describe and evaluate a PP framework aimed at relieving such difficulties and thus ease the construction of classifier systems. The framework is a two-stage approach, where the first stage performs a rapid compaction of the data and the second stage implements the PP search using an improved version of the SPP method (Guo et al., 2000, [32]). In an experimental evaluation with eight public microarray datasets we showed that some configurations of the proposed framework can clearly overtake the performance of eight well-established dimension reduction methods in their ability to pack more discriminatory information into fewer dimensions.}
}

@article{Friedman1981PPR,
author = { Jerome H.   Friedman  and  Werner   Stuetzle },
title = {{Projection Pursuit Regression}},
journal = {Journal of the American Statistical Association},
volume = {76},
number = {376},
pages = {817-823},
year  = {1981},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1080/01621459.1981.10477729},
URL = {
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1981.10477729},
eprint = {
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1981.10477729  }
}

@article{HUANG2006489ELM,
title = {{Extreme learning machine: Theory and applications}},
journal = {Neurocomputing},
volume = {70},
number = {1},
pages = {489-501},
year = {2006},
note = {Neural Networks},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2005.12.126},
url = {https://www.sciencedirect.com/science/article/pii/S0925231206000385},
author = {Guang-Bin Huang and Qin-Yu Zhu and Chee-Kheong Siew},
keywords = {Feedforward neural networks, Back-propagation algorithm, Extreme learning machine, Support vector machine, Real-time learning, Random node},
abstract = {It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: (1) the slow gradient-based learning algorithms are extensively used to train neural networks, and (2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these conventional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide good generalization performance at extremely fast learning speed. The experimental results based on a few artificial and real benchmark function approximation and classification problems including very large complex applications show that the new algorithm can produce good generalization performance in most cases and can learn thousands of times faster than conventional popular learning algorithms for feedforward neural networks.11For the preliminary idea of the ELM algorithm, refer to “Extreme Learning Machine: A New Learning Scheme of Feedforward Neural Networks”, Proceedings of International Joint Conference on Neural Networks (IJCNN2004), Budapest, Hungary, 25–29 July, 2004.}
}
  @Manual{pkg:doParallel,
    title = {\pkg{doParallel}: Foreach Parallel Adaptor for the \pkg{parallel} Package},
    author = {{Microsoft Corporation} and Steve Weston},
    year = {2022},
    note = {\proglang{R} package version 1.0.17},
   doi = {10.32614/CRAN.package.doParallel},
  }
  @Article{pkg:robustbase,
    title = {An Object-Oriented Framework for Robust Multivariate Analysis},
    author = {Valentin Todorov and Peter Filzmoser},
    journal = {Journal of Statistical Software},
    year = {2009},
    volume = {32},
    number = {3},
    pages = {1--47},
    doi = {10.18637/jss.v032.i03},
  }
  @Manual{pkg:foreach,
    title = {\pkg{foreach}: Provides Foreach Looping Construct},
    author = {{Microsoft} and Steve Weston},
    year = {2022},
    note = {\proglang{R} package version 1.5.2},
    doi = {10.32614/CRAN.package.foreach},
  }
@ARTICLE{Xinwei2012LVMGP,
  author={Jiang, Xinwei and Gao, Junbin and Wang, Tianjiang and Zheng, Lihong},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  title={{Supervised Latent Linear Gaussian Process Latent Variable Model for Dimensionality Reduction}},
  year={2012},
  volume={42},
  number={6},
  pages={1620-1632},
  doi={10.1109/TSMCB.2012.2196995}}

@article{Dunson2016ComprGP,
  author  = {Rajarshi Guhaniyogi and David B. Dunson},
  title   = {{Compressed Gaussian Process for Manifold Regression}},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {69},
  pages   = {1-26},
  url     = {http://jmlr.org/papers/v17/14-230.html}
}

@article{GPJulia,
 title={GaussianProcesses.jl: A Nonparametric Bayes Package for the Julia Language},
 volume={102},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v102i01},
 doi={https://doi.org/10.18637/jss.v102.i01},
 abstract={&amp;lt;p&amp;gt;Gaussian processes are a class of flexible nonparametric Bayesian tools that are widely used across the sciences, and in industry, to model complex data sources. Key to applying Gaussian process models is the availability of well-developed open source software, which is available in many programming languages. In this paper, we present a tutorial of the GaussianProcesses.jl package that has been developed for the Julia programming language. GaussianProcesses.jl utilizes the inherent computational benefits of the Julia language, including multiple dispatch and just-in-time compilation, to produce a fast, flexible and user-friendly Gaussian processes package. The package provides many mean and kernel functions with supporting inference tools to fit exact Gaussian process models, as well as a range of alternative likelihood functions to handle non-Gaussian data (e.g., binary classification models) and sparse approximations for scalable Gaussian processes. The package makes efficient use of existing Julia packages to provide users with a range of optimization and plotting tools.&amp;lt;/p&amp;gt;},
 number={1},
 journal={Journal of Statistical Software},
 author={Fairbrother, Jamie and Nemeth, Christopher and Rischard, Maxime and Brea, Johanni and Pinder, Thomas},
 year={2022},
 pages={1–36}
}

@book{Rasmussen2006GP,
  added-at = {2020-07-17T00:00:00.000+0200},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  biburl = {https://www.bibsonomy.org/bibtex/2670a576a21065048f7ddede17e09b6b4/dblp},
  ee = {https://www.worldcat.org/oclc/61285753},
  interhash = {72c030472023000e0bdeeb06081c3764},
  intrahash = {670a576a21065048f7ddede17e09b6b4},
  isbn = {026218253X},
  keywords = {dblp},
  pages = {I-XVIII, 1-248},
  publisher = {MIT Press},
  series = {Adaptive computation and machine learning},
  timestamp = {2020-07-24T00:45:17.000+0200},
  title = {{Gaussian processes for machine learning.}},
  year = 2006
}


@article{Gagnon2021LPTN,
author = {Philippe Gagnon and Mylène Bédard and Alain Desgagné},
title = {{An automatic robust Bayesian approach to principal component regression}},
journal = {Journal of Applied Statistics},
volume = {48},
number = {1},
pages = {84-104},
year  = {2021},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1080/02664763.2019.1710478},

URL = {
        https://doi.org/10.1080/02664763.2019.1710478

},
eprint = {
        https://doi.org/10.1080/02664763.2019.1710478

}

}

@article{GagnonBayOutlRobLPTN,
author = {Philippe Gagnon and Alain Desgagné and Mylène Bédard},
title = {{A New Bayesian Approach to Robustness Against Outliers in Linear Regression}},
volume = {15},
journal = {Bayesian Analysis},
number = {2},
publisher = {International Society for Bayesian Analysis},
pages = {389 -- 414},
keywords = {ANCOVA, ANOVA, Built-in robustness, maximum likelihood estimation, super heavy-tailed distributions, Variable selection, whole robustness},
year = {2020},
doi = {https://doi.org/10.1214/19-BA1157},
URL = {https://doi.org/10.1214/19-BA1157}
}


@article{HAMURA2022107517,
	abstract = {Linear regression that employs the assumption of normality for the error distribution may lead to an undesirable posterior inference of regression coefficients due to potential outliers. A finite mixture of two components, one with thin and one with heavy tails, is considered as the error distribution in this study. For the heavily-tailed component, the novel class of distributions is introduced; their densities are log-regularly varying and have heavier tails than the Cauchy distribution. Yet, they are expressed as a scale mixture of normals which enables the efficient posterior inference when using a Gibbs sampler. The robustness of the posterior distributions is proved under the proposed models using a minimal set of assumptions, which justifies the use of shrinkage priors with unbounded densities for the coefficient vector in the presence of outliers. An extensive comparison with the existing methods via simulation study shows the improved performance of the proposed model in point and interval estimation, as well as its computational efficiency. Further, the posterior robustness of the proposed method is confirmed in an empirical study with shrinkage priors for regression coefficients.},
	author = {Yasuyuki Hamura and Kaoru Irie and Shonosuke Sugasawa},
	doi = {https://doi.org/10.1016/j.csda.2022.107517},
	issn = {0167-9473},
	journal = {Computational Statistics \& Data Analysis},
	keywords = {Robust statistics, Linear regression, Heavily-tailed distribution, Scale mixture of normals, Log-regularly varying density, Gibbs sampler},
	pages = {107517},
	title = {Log-regularly varying scale mixture of normals for robust regression},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947322000974},
	volume = {173},
	year = {2022}
}


@ARTICLE{Rabc,
  title     = "abc: an {R} package for approximate Bayesian computation
               ({ABC)}: {R} package: abc",
  author    = "Csill{\'e}ry, Katalin and Fran{\c c}ois, Olivier and Blum,
               Michael G B",
  journal   = "Methods Ecol. Evol.",
  publisher = "Wiley",
  volume    =  3,
  number    =  3,
  pages     = "475--479",
  month     =  jun,
  year      =  2012,
  url       = "http://dx.doi.org/10.1111/j.2041-210x.2011.00179.x",
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en",
  issn      = "2041-210X",
  doi       = "https://doi.org/10.1111/j.2041-210x.2011.00179.x"
}

@article{Rhmclearn,
author = {Samuel Thomas and Wanzhu Tu},
title = {{Learning Hamiltonian Monte Carlo in R}},
journal = {The American Statistician},
volume = {75},
number = {4},
pages = {403-413},
year  = {2021},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1080/00031305.2020.1865198},

URL = {
        https://doi.org/10.1080/00031305.2020.1865198

},
eprint = {
        https://doi.org/10.1080/00031305.2020.1865198

}

}


@Article{Vogelstein2021LOL,
author={Vogelstein, Joshua T.
and Bridgeford, Eric W.
and Tang, Minh
and Zheng, Da
and Douville, Christopher
and Burns, Randal
and Maggioni, Mauro},
title={{Supervised dimensionality reduction for big data}},
journal={Nature Communications},
year={2021},
month={5},
day={17},
volume={12},
number={1},
pages={2872},
abstract={To solve key biomedical problems, experimentalists now routinely measure millions or billions of features (dimensions) per sample, with the hope that data science techniques will be able to build accurate data-driven inferences. Because sample sizes are typically orders of magnitude smaller than the dimensionality of these data, valid inferences require finding a low-dimensional representation that preserves the discriminating information (e.g., whether the individual suffers from a particular disease). There is a lack of interpretable supervised dimensionality reduction methods that scale to millions of dimensions with strong statistical theoretical guarantees. We introduce an approach to extending principal components analysis by incorporating class-conditional moment estimates into the low-dimensional projection. The simplest version, Linear Optimal Low-rank projection, incorporates the class-conditional means. We prove, and substantiate with both synthetic and real data benchmarks, that Linear Optimal Low-Rank Projection and its generalizations lead to improved data representations for subsequent classification, while maintaining computational efficiency and scalability. Using multiple brain imaging datasets consisting of more than 150 million features, and several genomics datasets with more than 500,000 features, Linear Optimal Low-Rank Projection outperforms other scalable linear dimensionality reduction techniques in terms of accuracy, while only requiring a few minutes on a standard desktop computer.},
issn={2041-1723},
doi={https://doi.org/10.1038/s41467-021-23102-2},
url={https://doi.org/10.1038/s41467-021-23102-2}
}


@InProceedings{Skorski2021RP,
  title = 	 {Johnson-Lindenstrauss Transforms with Best Confidence},
  author =       {Skorski, Maciej},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {3989--4007},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {8},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v134/skorski21a/skorski21a.pdf},
  url = 	 {https://proceedings.mlr.press/v134/skorski21a.html},
  abstract = 	 {The seminal result of Johnson and Lindenstrauss on random embeddings has been intensively studied in applied and theoretical computer science. Despite that vast body of literature, we still lack of complete understanding of statistical properties of random projections; a particularly intriguing question is: why are the theoretical bounds that far behind the empirically observed performance? Motivated by this question, this work develops Johnson-Lindenstrauss distributions with optimal, data-oblivious, statistical confidence bounds. These bounds are numerically best possible, for any given data dimension, embedding dimension, and distortion tolerance. They improve upon prior works in terms of statistical accuracy, as well as exactly determine the no-go regimes for data-oblivious approaches. Furthermore, the projection matrices are efficiently samplable.  The construction relies on orthogonal matrices, and the proof uses certain elegant properties of the unit sphere. In particular, the following techniques introduced in this work are of independent interest: a) a compact expression for the projection distortion in terms of singular eigenvalues of the projection matrix, b) a parametrization linking the unit sphere and the Dirichlet distribution and c) anti-concentration bounds for the Dirichlet distribution.  Besides the technical contribution, the paper presents applications and numerical evaluation along with working implementation in Python (shared as a GitHub repository).}
}

@misc{Wang2019RobModAvg,
  doi = {https://doi.org/10.48550/ARXIV.1910.12210},

  url = {https://arxiv.org/abs/1910.12210},

  author = {Wang, Miaomiao and Zou, Guohua},

  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {An outlier-robust model averaging approach by Mallows-type criterion},

  publisher = {arXiv},

  year = {2019},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{KURNAZ2018enetLTS,
title = {Robust and sparse estimation methods for high-dimensional linear and logistic regression},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {172},
pages = {211-222},
year = {2018},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2017.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0169743917301247},
author = {Fatma Sevinç Kurnaz and Irene Hoffmann and Peter Filzmoser},
keywords = {Elastic net penalty, Least trimmed squares, C-step algorithm, High-dimensional data, Robustness, Sparse estimation},
abstract = {Fully robust versions of the elastic net estimator are introduced for linear and logistic regression. The algorithms used to compute the estimators are based on the idea of repeatedly applying the non-robust classical estimators to data subsets only. It is shown how outlier-free subsets can be identified efficiently, and how appropriate tuning parameters for the elastic net penalties can be selected. A final reweighting step improves the efficiency of the estimators. Simulation studies compare with non-robust and other competing robust estimators and reveal the superiority of the newly proposed methods. This is also supported by a reasonable computation time and by good performance in real data examples.}
}


@article{Fan2007SISforUHD,
author = {Fan, Jianqing and Lv, Jinchi},
year = {2007},
month = {01},
pages = {},
title = {Sure Independence Screening for Ultra-High Dimensional Feature Space},
volume = {B 70},
doi={10.1111/j.1467-9868.2008.00674.x},
journal = {Journal of the Royal Statistical Society B}
}

@Article{BRMS2017Buerkner,
  title = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {80},
  number = {1},
  pages = {1--28},
  doi = {https://doi.org/10.18637/jss.v080.i01},
  encoding = {UTF-8},
}

@Article{SCAD2011Breheny,
  author = {Patrick Breheny and Jian Huang},
  title = {Coordinate descent algorithms for nonconvex penalized regression,
	with applications to biological feature selection},
  journal = {Annals of Applied Statistics},
  year = {2011},
  volume = {5},
  pages = {232--253},
  number = {1},
}




@article{LanEtAl2006MiceGene,
    doi = {10.1371/journal.pgen.0020006},
    author = {Lan, Hong AND Chen, Meng AND Flowers, Jessica B AND Yandell, Brian S AND Stapleton, Donnie S AND Mata, Christine M AND Mui, Eric Ton-Keen AND Flowers, Matthew T AND Schueler, Kathryn L AND Manly, Kenneth F AND Williams, Robert W AND Kendziorski, Christina AND Attie, Alan D},
    journal = {PLOS Genetics},
    publisher = {Public Library of Science},
    title = {Combined Expression Trait Correlations and Expression Quantitative Trait Locus Mapping},
    year = {2006},
    month = {01},
    volume = {2},
    url = {https://doi.org/10.1371/journal.pgen.0020006},
    pages = {1-11},
    abstract = {Coordinated regulation of gene expression levels across a series of experimental conditions provides valuable information about the functions of correlated transcripts. The consideration of gene expression correlation over a time or tissue dimension has proved valuable in predicting gene function. Here, we consider correlations over a genetic dimension. In addition to identifying coregulated genes, the genetic dimension also supplies us with information about the genomic locations of putative regulatory loci. We calculated correlations among approximately 45,000 expression traits derived from 60 individuals in an F2 sample segregating for obesity and diabetes. By combining the correlation results with linkage mapping information, we were able to identify regulatory networks, make functional predictions for uncharacterized genes, and characterize novel members of known pathways. We found evidence of coordinate regulation of 174 G protein–coupled receptor protein signaling pathway expression traits. Of the 174 traits, 50 had their major LOD peak within 10 cM of a locus on Chromosome 2, and 81 others had a secondary peak in this region. We also characterized a Riken cDNA clone that showed strong correlation with stearoyl-CoA desaturase 1 expression. Experimental validation confirmed that this clone is involved in the regulation of lipid metabolism. We conclude that trait correlation combined with linkage mapping can reveal regulatory networks that would otherwise be missed if we studied only mRNA traits with statistically significant linkages in this small cross. The combined analysis is more sensitive compared with linkage mapping alone.},
    number = {1},

}

@ARTICLE{Beale2007PEPCK,
  title     = "{PCK1} and {PCK2} as candidate diabetes and obesity genes",
  author    = "Beale, Elmus G and Harvey, Brandy J and Forest, Claude",
  abstract  = "The PCK1 gene (Pck1 in rodents) encodes the cytosolic isozyme of
               phosphoenolpyruvate carboxykinase (PEPCK-C), which is well-known
               for its function as a gluconeogenic enzyme in the liver and
               kidney. Mouse studies involving whole body and tissue-specific
               Pck1 knockouts as well as tissue-specific over-expression of
               PEPCK-C have resulted in type 2 diabetes as well as several
               surprising phenotypes including obesity, lipodystrophy, fatty
               liver, and death. These phenotypes arise from perturbations not
               only in gluconeogenesis but in two additional metabolic
               functions of PEPCK-C: (1) cataplerosis which maintains metabolic
               flux through the Krebs cycle by removing excess oxaloacetate,
               and (2) glyceroneogenesis which produces glycerol-3-phosphate as
               a precursor for fatty acid esterification into triglycerides.
               PEPCK-C catalyzes the conversion of oxaloacetate + GTP to
               phosphoenolpyruvate + GDP + CO2. It is in part the
               tissue-specificity of this simple reaction that results in the
               variety of phenotypes listed above. Briefly: (1) A 7-fold
               over-expression of PEPCK-C in the livers of mice causes
               excessive glucose production. (2) Mice with a whole-body
               knockout of Pck1 die within 2-3 days of birth, not from
               hypoglycemia, but probably because the Krebs cycle slows to
               approximately 10\% of normal in the absence of cataplerosis. (3)
               Mice with a liver-specific knockout have an inability to remove
               oxaloacetate from the Krebs cycle, which leads to a fatty liver
               following a fast. (4) An adipose-specific knockout of Pck1
               results in a fraction of the mice developing lipodystrophy due
               to lost glyceroneogenesis and a consequent decrease in fatty
               acid re-esterification. (5) Finally, disregulated
               over-expression of PEPCK-C in adipose tissue increases fatty
               acid re-esterification leading to obesity. These varied
               experimental phenotypes in mice have led us to postulate that
               abnormal production of PEPCK isozymes encoded by two PEPCK
               genes, PCK1 and PCK2, in humans could have similar consequences
               (Beale, E. G. et al. (2004). Trends in Endocrinology and
               Metabolism, 15, 129-135). The purpose of this review is to
               further explore these possibilities.",
  journal   = "Cell Biochem. Biophys.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  48,
  number    = "2-3",
  pages     = "89--95",
  year      =  2007,
  language  = "en"
}

@ARTICLE{Mendez-Lucas2014-PEPCK,
  title     = "Mitochondrial phosphoenolpyruvate carboxykinase ({PEPCK-M}) is a
               pro-survival, endoplasmic reticulum ({ER}) stress response gene
               involved in tumor cell adaptation to nutrient availability",
  author    = "M{\'e}ndez-Lucas, Andr{\'e}s and Hyro{\v s}{\v s}ov{\'a}, Petra
               and Novellasdemunt, Laura and Vi{\~n}als, Francesc and Perales,
               Jose C",
  abstract  = "Mitochondrial phosphoenolpyruvate carboxykinase (PEPCK-M),
               encoded by the nuclear PCK2 gene, links TCA cycle intermediates
               and glycolytic pools through the conversion of mitochondrial
               oxaloacetate into phosphoenolpyruvate. In the liver PEPCK-M
               adjoins its profusely studied cytosolic isoform (PEPCK-C)
               potentiating gluconeogenesis and TCA flux. However, PEPCK-M is
               present in a variety of non-gluconeogenic tissues, including
               tumors of several origins. Despite its potential relevance to
               cancer metabolism, the mechanisms responsible for PCK2 gene
               regulation have not been elucidated. The present study
               demonstrates PEPCK-M overexpression in tumorigenic cells as well
               as the mechanism for the modulation of PCK2 abundance under
               several stress conditions. Amino acid limitation and ER stress
               inducers, conditions that activate the amino acid response (AAR)
               and the unfolded protein response (UPR), stimulate PCK2 gene
               transcription. Both the AAR and UPR lead to increased synthesis
               of ATF4, which mediates PCK2 transcriptional up-regulation
               through its binding to a putative ATF/CRE composite site within
               the PCK2 promoter functioning as an amino acid response element.
               In addition, activation of the GCN2-eIF2$\alpha$-ATF4 and
               PERK-eIF2$\alpha$-ATF4 signaling pathways are responsible for
               increased PEPCK-M levels. Finally, PEPCK-M knockdown using
               either siRNA or shRNA were sufficient to reduce MCF7 mammary
               carcinoma cell growth and increase cell death under glutamine
               deprivation or ER stress conditions. Our data demonstrate that
               this enzyme has a critical role in the survival program
               initiated upon stress and shed light on an unexpected and
               important role of mitochondrial PEPCK in cancer metabolism.",
  journal   = "J. Biol. Chem.",
  publisher = "Elsevier BV",
  volume    =  289,
  number    =  32,
  pages     = "22090--22102",
  month     =  aug,
  year      =  2014,
  keywords  = "AAR; AARE; ATF4; Cell Metabolism; ER Stress; PEPCK-M;
               Transcription Regulation; Tumor Metabolism; UPR; Unfolded
               Protein Response (UPR)",
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Hakimi2007-PEPCK_Mice,
  title     = "Overexpression of the cytosolic form of phosphoenolpyruvate
               carboxykinase ({GTP}) in skeletal muscle repatterns energy
               metabolism in the mouse",
  author    = "Hakimi, Parvin and Yang, Jianqi and Casadesus, Gemma and
               Massillon, Duna and Tolentino-Silva, Fatima and Nye, Colleen K
               and Cabrera, Marco E and Hagen, David R and Utter, Christopher B
               and Baghdy, Yacoub and Johnson, David H and Wilson, David L and
               Kirwan, John P and Kalhan, Satish C and Hanson, Richard W",
  abstract  = "Transgenic mice, containing a chimeric gene in which the cDNA
               for phosphoenolpyruvate carboxykinase (GTP) (PEPCK-C) (EC
               4.1.1.32) was linked to the alpha-skeletal actin gene promoter,
               express PEPCK-C in skeletal muscle (1-3 units/g). Breeding two
               founder lines together produced mice with an activity of PEPCK-C
               of 9 units/g of muscle (PEPCK-C(mus) mice). These mice were
               seven times more active in their cages than controls. On a mouse
               treadmill, PEPCK-C(mus) mice ran up to 6 km at a speed of 20
               m/min, whereas controls stopped at 0.2 km. PEPCK-C(mus) mice had
               an enhanced exercise capacity, with a VO(2max) of 156 +/- 8.0
               ml/kg/min, a maximal respiratory exchange ratio of 0.91 +/-
               0.03, and a blood lactate concentration of 3.7 +/- 1.0 mm after
               running for 32 min at a 25 degrees grade; the values for control
               animals were 112 +/- 21 ml/kg/min, 0.99 +/- 0.08, and 8.1 +/-
               5.0 mm respectively. The PEPCK-C(mus) mice ate 60\% more than
               controls but had half the body weight and 10\% the body fat as
               determined by magnetic resonance imaging. In addition, the
               number of mitochondria and the content of triglyceride in the
               skeletal muscle of PEPCK-C(mus) mice were greatly increased as
               compared with controls. PEPCK-C(mus) mice had an extended life
               span relative to control animals; mice up to an age of 2.5 years
               ran twice as fast as 6-12-month-old control animals. We conclude
               that overexpression of PEPCK-C repatterns energy metabolism and
               leads to greater longevity.",
  journal   = "J. Biol. Chem.",
  publisher = "Elsevier BV",
  volume    =  282,
  number    =  45,
  pages     = "32844--32855",
  month     =  nov,
  year      =  2007,
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Burgess2007-PEPCK_NotEasy,
  title     = "Cytosolic phosphoenolpyruvate carboxykinase does not solely
               control the rate of hepatic gluconeogenesis in the intact mouse
               liver",
  author    = "Burgess, Shawn C and He, Tianteng and Yan, Zheng and Lindner,
               Jill and Sherry, A Dean and Malloy, Craig R and Browning,
               Jeffrey D and Magnuson, Mark A",
  abstract  = "When dietary carbohydrate is unavailable, glucose required to
               support metabolism in vital tissues is generated via
               gluconeogenesis in the liver. Expression of phosphoenolpyruvate
               carboxykinase (PEPCK), commonly considered the control point for
               liver gluconeogenesis, is normally regulated by circulating
               hormones to match systemic glucose demand. However, this
               regulation fails in diabetes. Because other molecular and
               metabolic factors can also influence gluconeogenesis, the
               explicit role of PEPCK protein content in the control of
               gluconeogenesis was unclear. In this study, metabolic control of
               liver gluconeogenesis was quantified in groups of mice with
               varying PEPCK protein content. Surprisingly, livers with a 90\%
               reduction in PEPCK content showed only a approximately 40\%
               reduction in gluconeogenic flux, indicating a lower than
               expected capacity for PEPCK protein content to control
               gluconeogenesis. However, PEPCK flux correlated tightly with TCA
               cycle activity, suggesting that under some conditions in mice,
               PEPCK expression must coordinate with hepatic energy metabolism
               to control gluconeogenesis.",
  journal   = "Cell Metab.",
  publisher = "Elsevier BV",
  volume    =  5,
  number    =  4,
  pages     = "313--320",
  month     =  apr,
  year      =  2007,
  copyright = "https://www.elsevier.com/open-access/userlicense/1.0/",
  language  = "en"
}


@incollection{TAKAHASHI2017313_ACU,
title = {Chapter 13 - Animal Models of Liver Diseases},
editor = {P. Michael Conn},
booktitle = {Animal Models for the Study of Human Disease (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {313-339},
year = {2017},
isbn = {978-0-12-809468-6},
doi = {https://doi.org/10.1016/B978-0-12-809468-6.00013-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128094686000139},
author = {Yoshihisa Takahashi and Toshio Fukusato},
keywords = {liver disease, animal model, mouse, rat, transgenic mouse, diet},
abstract = {Animal models are indispensable for the elucidation of pathogenesis mechanisms,
the identification of potential therapeutic targets, and the development of novel therapies for various
liver diseases. Although large animals, including nonhuman primates, have occasionally been used, rodents
are used most frequently. In 2010, a gold standard publication checklist (GSPC) for animal studies was
 presented, with which all future studies should comply. Studies using animal models have various advantages
  over clinical or in vitro studies. However, animal experiments also have some limitations. As currently
  used animal models only partially represent the characteristics of human diseases, it is important
  to select an animal model that is suitable for the objective of the study. In this chapter, we present
  a systematic review of currently used animal models of various liver diseases, along with their advantages
  and disadvantages.}
}


@article{Zhang2020UHDQuantileMiceGene,
  author  = {Yuankun Zhang and Heng Lian and Yan Yu},
  title   = {Ultra-High Dimensional Single-Index Quantile Regression},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {224},
  pages   = {1--25},
  url     = {http://jmlr.org/papers/v21/19-173.html}
}

@article{Song2015HDL1VarSelMiceGene,
author = {Qifan Song and Faming Liang},
title = {High-Dimensional Variable Selection With Reciprocal L1-Regularization},
journal = {Journal of the American Statistical Association},
volume = {110},
number = {512},
pages = {1607-1620},
year  = {2015},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1080/01621459.2014.984812},

URL = {
        https://doi.org/10.1080/01621459.2014.984812

},
eprint = {
        https://doi.org/10.1080/01621459.2014.984812

}

}

@article{ScheetzEtAl2006RatEye,
author = {Todd E. Scheetz  and Kwang-Youn A. Kim  and Ruth E. Swiderski  and Alisdair R. Philp  and Terry A. Braun  and Kevin L. Knudtson  and Anne M. Dorrance  and Gerald F. DiBona  and Jian Huang  and Thomas L. Casavant  and Val C. Sheffield  and Edwin M. Stone },
title = {Regulation of gene expression in the mammalian eye and its relevance to eye disease},
journal = {Proceedings of the National Academy of Sciences},
volume = {103},
number = {39},
pages = {14429-14434},
year = {2006},
doi = {https://doi.org/10.1073/pnas.0602562103},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0602562103},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0602562103}
}

@article{Trim32ScheetzRatEye,
author = {Annie P. Chiang  and John S. Beck  and Hsan-Jan Yen  and Marwan K. Tayeh  and Todd E. Scheetz  and Ruth E. Swiderski  and Darryl Y. Nishimura  and Terry A. Braun  and Kwang-Youn A. Kim  and Jian Huang  and Khalil Elbedour  and Rivka Carmi  and Diane C. Slusarski  and Thomas L. Casavant  and Edwin M. Stone  and Val C. Sheffield },
title = {Homozygosity mapping with SNP arrays identifies TRIM32, an E3 ubiquitin ligase, as a Bardet-Biedl syndrome gene (BBS11)},
journal = {Proceedings of the National Academy of Sciences},
volume = {103},
number = {16},
pages = {6287-6292},
year = {2006},
doi = {https://doi.org/10.1073/pnas.0600158103},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0600158103},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0600158103},
abstract = {The identification of mutations in genes that cause human diseases
has largely been accomplished through the use of positional cloning, which relies on
linkage mapping. In studies of rare diseases, the resolution of linkage mapping is limited by the
number of available meioses and informative marker density. One recent advance is the development of
high-density SNP microarrays for genotyping. The SNP arrays overcome low marker informativity by using a
large number of markers to achieve greater coverage at finer resolution. We used SNP microarray genotyping
for homozygosity mapping in a small consanguineous Israeli Bedouin family with autosomal recessive
Bardet–Biedl syndrome (BBS; obesity, pigmentary retinopathy, polydactyly, hypogonadism, renal and cardiac
abnormalities, and cognitive impairment) in which previous linkage studies using short tandem repeat
polymorphisms failed to identify a disease locus. SNP genotyping revealed a homozygous candidate region.
Mutation analysis in the region of homozygosity identified a conserved homozygous missense mutation in the
TRIM32 gene, a gene coding for an E3 ubiquitin ligase. Functional analysis of this gene in zebrafish
and expression correlation analyses among other BBS genes in an expression quantitative trait loci data
set demonstrate that TRIM32 is a BBS gene. This study shows the value of high-density SNP genotyping for
homozygosity mapping and the use of expression correlation data for evaluation of candidate genes and
identifies the proteasome degradation pathway as a pathway involved in BBS.}
}

@article{Huang2006AdLassoSparseRatEye,
author = {Huang, Jian and Ma, Shuangge and Zhang, Cun-Hui},
year = {2006},
month = {12},
pages = {},
title = {Adaptive LASSO for sparse high-dimensional regression},
volume = {18},
journal = {Statistica Sinica}
}

@article{FanLi2001SCAD,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/3085904},
 abstract = {Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coefficients simultaneously. Hence they enable us to construct confidence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on (0, ∞), and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications.},
 author = {Jianqing Fan and Runze Li},
 journal = {Journal of the American Statistical Association},
 number = {456},
 pages = {1348--1360},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties},
 urldate = {2022-09-14},
 volume = {96},
 year = {2001}
}

@article{Zou2006AdLASSO,
author = {Hui Zou},
title = {The Adaptive Lasso and Its Oracle Properties},
journal = {Journal of the American Statistical Association},
volume = {101},
number = {476},
pages = {1418-1429},
year  = {2006},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1198/016214506000000735},

URL = {
        https://doi.org/10.1198/016214506000000735

},
eprint = {
        https://doi.org/10.1198/016214506000000735

}

}

@article{Efron2004LARS,
	doi = {https://doi.org/10.1214/009053604000000067},

	url = {https://doi.org/10.1214%2F009053604000000067},

	year = {2004},
	month = {apr},

	publisher = {Institute of Mathematical Statistics},

	volume = {32},

	number = {2},

	author = {Bradley Efron and Trevor Hastie and Iain Johnstone and Robert Tibshirani},

	title = {Least angle regression},

	journal = {The Annals of Statistics}
}

@book{Hastie2015STwSparsityBook,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
title = {Statistical Learning with Sparsity: The Lasso and Generalizations},
year = {2015},
isbn = {1498712169},
publisher = {Chapman \& Hall/CRC},
url = {https://doi.org/10.1201/b18401},
abstract = {Discover New Methods for Dealing with High-Dimensional Data A sparse statistical model
has only a small number of nonzero parameters or weights; therefore, it is much easier to estimate
and interpret than a dense model. Statistical Learning with Sparsity: The Lasso and Generalizations
presents methods that exploit sparsity to help recover the underlying signal in a set of data. Top
experts in this rapidly evolving field, the authors describe the lasso for linear regression and a
simple coordinate descent algorithm for its computation. They discuss the application of 1 penalties
to generalized linear models and support vector machines, cover generalized penalties such as the
elastic net and group lasso, and review numerical methods for optimization. They also present
statistical inference methods for fitted (lasso) models, including the bootstrap, Bayesian methods,
and recently developed approaches. In addition, the book examines matrix decomposition,
sparse multivariate analysis, graphical models, and compressed sensing.
It concludes with a survey of theoretical results for the lasso. In this age of big data,
the number of features measured on a person or object can be large and might be larger than
the number of observations. This book shows how the sparsity assumption allows us to tackle these
problems and extract useful and reproducible patterns from big datasets. Data analysts,
computer scientists, and theorists will appreciate this thorough and up-to-date treatment of
sparse statistical modeling.}
}

@article{Kobak2020RidgeinHD,
author = {Kobak, Dmitry and Lomond, Jonathan and Sanchez, Benoit},
title = {The Optimal Ridge Penalty for Real-World High-Dimensional Data Can Be Zero or Negative
Due to the Implicit Ridge Regularization},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {A conventional wisdom in statistical learning is that large models require strong
regularization to prevent overfitting. Here we show that this rule can be violated by linear
regression in the underdetermined n ≪ p situation under realistic conditions. Using simulations
and real-life high-dimensional datasets, we demonstrate that an explicit positive ridge penalty
can fail to provide any improvement over the minimum-norm least squares estimator. Moreover,
the optimal value of ridge penalty in this situation can be negative. This happens when the
high-variance directions in the predictor space can predict the response variable, which is often
the case in the real-world high-dimensional data. In this regime, low-variance directions provide an
implicit ridge regularization and can make any further positive ridge penalty detrimental.
We prove that augmenting any linear model with random covariates and using minimum-norm estimator
is asymptotically equivalent to adding the ridge penalty. We use a spiked covariance model as an
analytically tractable example and prove that the optimal ridge penalty in this case is negative
when n ≪ p.},
journal = {J. Mach. Learn. Res.},
month = {1},
articleno = {169},
numpages = {16},
keywords = {ridge regression, regularization, high-dimensional}
}

@article{Alfons2013sparseLTS,
author = {Andreas Alfons and Christophe Croux and Sarah Gelper},
title = {{Sparse least trimmed squares regression for analyzing high-dimensional large data sets}},
volume = {7},
journal = {The Annals of Applied Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {226 -- 248},
keywords = {Breakdown point, Outliers, penalized regression, robust regression, Trimming},
year = {2013},
doi = {https://doi.org/10.1214/12-AOAS575},
URL = {https://doi.org/10.1214/12-AOAS575}
}

@article{Tibsh1996LASSO,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes
 the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant.
 Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives
 interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression.
 It produces interpretable models like subset selection and exhibits the stability of ridge regression.
 There is also an interesting relationship with recent work in adaptive function estimation by Donoho and
 Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models:
 extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society B},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2022-09-29},
 volume = {58},
 year = {1996}
}

@inproceedings{LiHastie2006VerySparseRP,
author = {Li, Ping and Hastie, Trevor J. and Church, Kenneth W.},
title = {Very Sparse Random Projections},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1150402.1150436},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining},
pages = {287–296},
numpages = {10},
keywords = {rates of convergence, random projections, sampling},
series = {KDD '06}
}


@article{WatsonHolmes2016AppModRobDec,
 ISSN = {08834237, 21688745},
 URL = {http://www.jstor.org/stable/26408074},
 abstract = {Decisions based partly or solely on predictions from probabilistic models may be sensitive
 to model misspecification. Statisticians are taught from an early stage that "all models are wrong, but
 some are useful"; however, little formal guidance exists on how to assess the impact of model approximation
 on decision making, or how to proceed when optimal actions appear sensitive to model fidelity.
 This article presents an overview of recent developments across different disciplines to address this.
  We review diagnostic techniques, including graphical approaches and summary statistics, to help highlight
  decisions made through minimised expected loss that are sensitive to model misspecification.
  We then consider formal methods for decision making under model misspecification by quantifying
  stability of optimal actions to perturbations to the model within a neighbourhood of model space.
  This neighbourhood is defined in either one of two ways. First, in a strong sense via an information
  (Kullback–Leibler) divergence around the approximating model. Second, using a Bayesian nonparametric model
  (prior) centred on the approximating model, in order to "average out" over possible misspecifications.
   This is presented in the context of recent work in the robust control, macroeconomics and financial
   mathematics literature. We adopt a Bayesian approach throughout although the presentation is agnostic
   to this position.},
 author = {James Watson and Chris Holmes},
 journal = {Statistical Science},
 number = {4},
 pages = {465--489},
 publisher = {Institute of Mathematical Statistics},
 title = {Approximate Models and Robust Decisions},
 urldate = {2022-10-20},
 volume = {31},
 year = {2016}
}

@article{Hoeting1999BayModAvgTutorial,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/2676803},
 abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select
  a model from some class of models and then proceed as if the selected model had generated the data.
  This approach ignores the uncertainty in model selection, leading to over-confident inferences and
   decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a
   coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA
   have recently emerged. We discuss these methods and present a number of examples. In these examples,
   BMA provides improved out-of-sample predictive performance. We also provide a catalogue of currently
   available BMA software.},
 author = {Jennifer A. Hoeting and David Madigan and Adrian E. Raftery and Chris T. Volinsky},
 journal = {Statistical Science},
 number = {4},
 pages = {382--401},
 publisher = {Institute of Mathematical Statistics},
 title = {Bayesian Model Averaging: A Tutorial},
 urldate = {2022-11-03},
 volume = {14},
 year = {1999}
}

@article{Raftery1997BayModAvgLinReg,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2291462},
 abstract = {We consider the problem of accounting for model uncertainty in linear regression models.
  Conditioning on a single selected model ignores model uncertainty, and thus leads to the underestimation
   of uncertainty when making inferences about quantities of interest. A Bayesian solution to this problem
   involves averaging over all possible models (i.e., combinations of predictors) when making inferences
   about quantities of interest. This approach is often not practical. In this article we offer
   two alternative approaches. First, we describe an ad hoc procedure, "Occam's window,"
   which indicates a small set of models over which a model average can be computed.
   Second, we describe a Markov chain Monte Carlo approach that directly approximates the exact solution.
   In the presence of model uncertainty, both of these model averaging procedures provide better predictive
   performance than any single model that might reasonably have been selected. In the extreme case where
   there are many candidate predictors but no relationship between any of them and the response,
   standard variable selection procedures often choose some subset of variables that yields a high R2
    and a highly significant overall F value. In this situation, Occam's window usually indicates the
    null model (or a small number of models including the null model) as the only one (or ones) to be
    considered thus largely resolving the problem of selecting significant models when there is no signal
     in the data. Software to implement our methods is available from StatLib.},
 author = {Adrian E. Raftery and David Madigan and Jennifer A. Hoeting},
 journal = {Journal of the American Statistical Association},
 number = {437},
 pages = {179--191},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Bayesian Model Averaging for Linear Regression Models},
 urldate = {2022-11-03},
 volume = {92},
 year = {1997}
}

@article{Filzmoser2020RobLinRegHDOverview,
author = {Filzmoser, Peter and Nordhausen, Klaus},
title = {Robust linear regression for high-dimensional data: An overview},
journal = {WIREs Computational Statistics},
volume = {13},
number = {4},
pages = {e1524},
keywords = {dimension reduction, high-dimensional data, Outlier, regression, sparsity},
doi = {https://doi.org/10.1002/wics.1524},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1524},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1524},
abstract = {Abstract Digitization as the process of converting information into numbers leads to bigger
and more complex data sets, bigger also with respect to the number of measured variables.
This makes it harder or impossible for the practitioner to identify outliers or observations that are
inconsistent with an underlying model. Classical least-squares based procedures can be affected by those
outliers. In the regression context, this means that the parameter estimates are biased, with consequences
on the validity of the statistical inference, on regression diagnostics, and on the prediction accuracy.
Robust regression methods aim at assigning appropriate weights to observations that deviate from the model.
While robust regression techniques are widely known in the low-dimensional case, researchers and
practitioners might still not be very familiar with developments in this direction for high-dimensional data.
 Recently, different strategies have been proposed for robust regression in the high-dimensional case,
 typically based on dimension reduction, on shrinkage, including sparsity, and on combinations of such
 techniques. A very recent concept is downweighting single cells of the data matrix rather than complete
 observations, with the goal to make better use of the model-consistent information, and thus to
 achieve higher efficiency of the parameter estimates. This article is categorized under:
 Statistical and Graphical Methods of Data Analysis > Robust Methods Statistical and Graphical Methods of
  Data Analysis > Analysis of High Dimensional Data Statistical and Graphical Methods of Data Analysis >
  Dimension Reduction},
year = {2021}
}

@misc{Gagnon2022StudenttProperties,
  doi = {https://doi.org/10.48550/ARXIV.2204.02299},

  url = {https://arxiv.org/abs/2204.02299},

  author = {Gagnon, Philippe and Hayashi, Yoshiko},

  keywords = {Statistics Theory (math.ST), Methodology (stat.ME), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Theoretical properties of Bayesian Student-$t$ linear regression},

  publisher = {arXiv},

  year = {2022},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Vehtari2016LOO,
	doi = {https://doi.org/10.1007/s11222-016-9696-4},

	url = {https://doi.org/10.1007%2Fs11222-016-9696-4},

	year = {2016},
	month = {8},

	publisher = {Springer Science and Business Media {LLC}
},

	volume = {27},

	number = {5},

	pages = {1413--1432},

	author = {Aki Vehtari and Andrew Gelman and Jonah Gabry},

	title = {Practical Bayesian model evaluation using leave-one-out cross-validation and {WAIC}},

	journal = {Statistics and Computing}
}
@article{cho2012high,
  title={High Dimensional Variable Selection Via Tilting},
  author={Cho, Haeran and Fryzlewicz, Piotr},
  journal={Journal of the Royal Statistical Society B},
  volume={74},
  doi={10.1111/j.1467-9868.2011.01023.x},
  number={3},
  pages={593--622},
  year={2012},
  publisher={Oxford University Press}
}

@article{ma2016robust,
  title={Robust Model-Free Feature Screening via Quantile Correlation},
  author={Ma, Xuejun and Zhang, Jingxiao},
  journal={Journal of Multivariate Analysis},
  volume={143},
  pages={472--480},
  doi={10.1016/j.jmva.2015.10.010},
  year={2016},
  publisher={Elsevier}
}


@misc{Wessel2015Ridge,
  doi = {https://doi.org/10.48550/ARXIV.1509.09169},

  url = {https://arxiv.org/abs/1509.09169},

  author = {van Wieringen, Wessel N.},

  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Lecture notes on ridge regression},

  publisher = {arXiv},

  year = {2015},

  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@article{Jia2015LASSOPrecond,
author = {Jinzhu Jia and Karl Rohe},
title = {{Preconditioning the Lasso for sign consistency}},
volume = {9},
journal = {Electronic Journal of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {1150 -- 1172},
keywords = {Irrepresentable Condition, preconditioning, sign consistency},
year = {2015},
doi = {https://doi.org/10.1214/15-EJS1029},
URL = {https://doi.org/10.1214/15-EJS1029}
}

@article{Buehlmann2014Riboflavin,
author = {B\"{u}hlmann, Peter and Kalisch, Markus and Meier, Lukas},
title = {High-Dimensional Statistics with a View Toward Applications in Biology},
journal = {Annual Review of Statistics and Its Application},
volume = {1},
number = {1},
pages = {255-278},
year = {2014},
doi = {https://doi.org/10.1146/annurev-statistics-022513-115545},

URL = {

        https://doi.org/10.1146/annurev-statistics-022513-115545



},
eprint = {

        https://doi.org/10.1146/annurev-statistics-022513-115545



}
,
    abstract = { We review statistical methods for high-dimensional data analysis and pay particular attention to recent developments for assessing uncertainties in terms of controlling false positive statements (type I error) and p-values. The main focus is on regression models, but we also discuss graphical modeling and causal inference based on observational data. We illustrate the concepts and methods with various packages from the statistical software using a high-throughput genomic data set about riboflavin production with Bacillus subtilis, which we make publicly available for the first time. }
}


@article{Christidis2019SplitReg,
author = {Anthony-Alexander Christidis and Laks Lakshmanan and Ezequiel Smucler and Ruben Zamar},
title = {Split Regularized Regression},
journal = {Technometrics},
volume = {62},
number = {3},
pages = {330-338},
year  = {2020},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1080/00401706.2019.1635533},
URL = {
        https://doi.org/10.1080/00401706.2019.1635533
},
eprint = {
        https://doi.org/10.1080/00401706.2019.1635533
}
}


@misc{Christidis2018Modeling,
  doi = {https://doi.org/10.48550/ARXIV.1812.05678},

  url = {https://arxiv.org/abs/1812.05678},

  author = {Christidis, Anthony and Van Aelst, Stefan and Zamar, Ruben},

  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Split Regression Modeling},

  publisher = {arXiv},

  year = {2018},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{christidis2022multimodel,
      title={Multi-Model Ensemble Optimization},
      author={Anthony-Alexander Christidis and Stefan Van Aelst and Ruben Zamar},
      year={2022},
      eprint={2204.08100},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{Fan2010OvVarSelHD,
author = {Fan, Jianqing and Lv, Jinchi},
year = {2010},
month = {01},
pages = {101-148},
title = {A Selective Overview of Variable Selection in High Dimensional Feature Space},
volume = {20},
journal = {Statistica Sinica}
}

@book{wainwright2019HDS_nonasy,
place={Cambridge},
series={Cambridge Series in Statistical and Probabilistic Mathematics},
title={High-Dimensional Statistics: A Non-Asymptotic Viewpoint},
DOI={10.1017/9781108627771},
publisher={Cambridge University Press},
author={Wainwright, Martin J.},
year={2019},
collection={Cambridge Series in Statistical and Probabilistic Mathematics}
}

@misc{SilinFan2020HDlinRegArxiv,
  doi = {https://doi.org/10.48550/ARXIV.2007.12313},

  url = {https://arxiv.org/abs/2007.12313},

  author = {Silin, Igor and Fan, Jianqing},

  keywords = {Statistics Theory (math.ST), Methodology (stat.ME), FOS: Mathematics, FOS: Mathematics,
   FOS: Computer and information sciences, FOS: Computer and information sciences, 62J05 (primary),
   62H12, 62H25 (secondary)},

  title = {Canonical thresholding for non-sparse high-dimensional linear regression},

  publisher = {arXiv},

  year = {2020},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{SilinFan2022HDlinReg,
author = {Igor Silin and Jianqing Fan},
title = {{Canonical thresholding for nonsparse high-dimensional linear regression}},
volume = {50},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {460 -- 486},
keywords = {covariance eigenvalues decay, high-dimensional linear regression, principal component regression, relative errors, thresholding},
year = {2022},
doi = {https://doi.org/10.1214/21-AOS2116},
URL = {https://doi.org/10.1214/21-AOS2116}
}


@article{Wang2009_extrCor_SuperMData,
author = {Hansheng Wang},
title = {Forward Regression for Ultra-High Dimensional Variable Screening},
journal = {Journal of the American Statistical Association},
volume = {104},
number = {488},
pages = {1512-1524},
year  = {2009},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1198/jasa.2008.tm08516},
URL = {
        https://doi.org/10.1198/jasa.2008.tm08516
},
eprint = {
        https://doi.org/10.1198/jasa.2008.tm08516
  }
}

@Manual{Python,
  author =	 {Guido {Van Rossum} and others},
  title =	 {\proglang{Python} Programming Language},
  url =		 {http://www.python.org},
  year =	 2011
}


  @article{BOTTMER2022SparseReg,
title = {Sparse regression for large data sets with outliers},
journal = {European Journal of Operational Research},
volume = {297},
number = {2},
pages = {782-794},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2021.05.049},
url = {https://www.sciencedirect.com/science/article/pii/S037722172100477X},
author = {Lea Bottmer and Christophe Croux and Ines Wilms},
keywords = {Data science, Lasso, Outliers, Robust regression, Variable selection},
abstract = {The linear regression model remains an important workhorse for data scientists. However, many data sets contain many more predictors than observations. Besides, outliers, or anomalies, frequently occur.
This paper proposes an algorithm for regression analysis that addresses these features typical
 for big data sets, which we call “sparse shooting S”. The resulting regression coefficients are sparse, meaning that many of them are set to zero, hereby selecting the most relevant predictors. A distinct feature
 of the method is its robustness with respect to outliers in the cells of the data matrix. The excellent performance of this robust variable selection and prediction method is shown in a simulation study.
 A real data application on car fuel consumption demonstrates its usefulness.}
}

@inproceedings{wojnowicz2016projecting,
  title={Projecting" better than randomly": How to reduce the dimensionality of very large datasets in a way that outperforms random projections},
  author={Wojnowicz, Michael and Zhang, Di and Chisholm, Glenn and Zhao, Xuan and Wolff, Matt},
  booktitle={2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
  pages={184--193},
  year={2016},
  organization={IEEE}
}

@article{Fan2011UHDVarEst,
    author = {Fan, Jianqing and Guo, Shaojun and Hao, Ning},
    title = {{Variance Estimation Using Refitted Cross-Validation in Ultrahigh Dimensional Regression}},
    journal = {Journal of the Royal Statistical Society B},
    volume = {74},
    number = {1},
    pages = {37-65},
    year = {2011},
    month = {10},
    abstract = {{Variance estimation is a fundamental problem in statistical modelling. In ultrahigh dimensional linear regression where the dimensionality is much larger than the sample size, traditional variance estimation techniques are not applicable. Recent advances in variable selection in ultrahigh dimensional linear regression make this problem accessible. One of the major problems in ultrahigh dimensional regression is the high spurious correlation between the unobserved realized noise and some of the predictors. As a result, the realized noises are actually predicted when extra irrelevant variables are selected, leading to a serious underestimate of the level of noise. We propose a two-stage refitted procedure via a data splitting technique, called refitted cross-validation, to attenuate the influence of irrelevant variables with high spurious correlations. Our asymptotic results show that the resulting procedure performs as well as the oracle estimator, which knows in advance the mean regression function. The simulation studies lend further support to our theoretical claims. The naive two-stage estimator and the plug-in one-stage estimators using the lasso and smoothly clipped absolute deviation are also studied and compared. Their performances can be improved by the refitted cross-validation method proposed.}},
    issn = {1369-7412},
    doi = {https://doi.org/10.1111/j.1467-9868.2011.01005.x},
    url = {https://doi.org/10.1111/j.1467-9868.2011.01005.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/74/1/37/49509126/jrsssb\_74\_1\_37.pdf},
}

@article{Lassance2023ShrinkageLinReg,
title = {An analytical shrinkage estimator for linear regression},
journal = {Statistics & Probability Letters},
volume = {194},
pages = {109760},
year = {2023},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2022.109760},
url = {https://www.sciencedirect.com/science/article/pii/S0167715222002735},
author = {Nathan Lassance},
keywords = {Linear regression, Prediction error, Shrinkage, Out-of-sample},
abstract = {We derive an analytical solution to the optimal shrinkage of OLS regression coefficients toward a constant target, under any first two moments of predictors. The estimator closely mimics the prediction performance of ridge penalty, which admits no general analytical solution.}
}


@inproceedings{Wang2015ConsistencyHOLP,
author = {Wang, Xiangyu and Leng, Chenlei and Dunson, David B.},
title = {On the Consistency Theory of High Dimensional Variable Screening},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2431–2439},
numpages = {9},
series = {NIPS'15}
}

@article{Liang2023ENSURE,
  author   = {Zhu, Rong and Liang, Hua and Ruppert, David},
  title    = {Ensemble Subset Regression (ENSURE): Efficient High-dimensional Prediction},
  journal  = {Statistica Sinica},
  year     = {2023},
  doi = {https://doi.org/10.5705/ss.202021.0187}
}

@Inbook{Thanei2017RPforHDR,
author="Thanei, Gian-Andrea
and Heinze, Christina
and Meinshausen, Nicolai",
title="Random Projections for Large-Scale Regression",
bookTitle="Big and Complex Data Analysis: Methodologies and Applications    ",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages={51-68},
doi="10.1007/978-3-319-41573-4_3",
}

@article{Cook2019PLSPredinHDR,
author = {Cook, R. and Forzani, Liliana},
year = {2019},
month = {04},
pages = {884-908},
title = {Partial least squares prediction in high-dimensional regression},
volume = {47},
journal = {Annals of Statistics},
doi = {10.1214/18-AOS1681}
}

@article{Hoerl1970Ridge,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1267351},
 author = {Hoerl, Arthur E. and  Kennard, Robert W.},
 journal = {Technometrics},
 number = {1},
 pages = {55--67},
 publisher = {Taylor & Francis Ltd., American Statistical Association American Society for Quality},
 title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
 urldate = {2023-04-03},
 volume = {12},
 year = {1970}
}

@article{Zou2005ElasticNet,
author = {Zou, Hui and Hastie, Trevor},
title = {Regularization and variable selection via the elastic net},
journal = {Journal of the Royal Statistical Society B},
volume = {67},
number = {2},
pages = {301-320},
keywords = {Grouping effect, LARS algorithm, Lasso, Penalization, p≫n problem, Variable selection},
doi = {https://doi.org/10.1111/j.1467-9868.2005.00503.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00503.x},
abstract = {Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
year = {2005}
}


@inproceedings{Zhou2007CompressedRegression,
 author = {Zhou, Shuheng and Wasserman, Larry and Lafferty, John},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 publisher = {Curran Associates, Inc.},
 location = {online},
 title = {Compressed Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf},
 volume = {20},
 year = {2007}
}

@book{Fan2020StatFoundofDataScience,
author = {Fan, Jianqing and Li, Runze and Zhang, Cun-Hui and Zou, Hui},
year = {2020},
month = {09},
pages = {},
title = {Statistical Foundations of Data Science},
isbn = {9780429096280},
doi = {https://doi.org/10.1201/9780429096280}
}

@article{Zhao2006LASSOModSel,
  author  = {Peng Zhao and Bin Yu},
  title   = {On Model Selection Consistency of Lasso},
  journal = {Journal of Machine Learning Research},
  year    = {2006},
  volume  = {7},
  number  = {90},
  pages   = {2541--2563},
  url     = {http://jmlr.org/papers/v7/zhao06a.html}
}

@article{Hazimeh2020FastBSSL0,
author = {Hazimeh, Hussein and Mazumder, Rahul},
title = {Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms},
journal = {Operations Research},
volume = {68},
number = {5},
pages = {1517-1537},
year = {2020},
doi = {https://doi.org/10.1287/opre.2019.1919},
URL = {https://doi.org/10.1287/opre.2019.1919},
eprint = {https://doi.org/10.1287/opre.2019.1919},
abstract = { In several scientific and industrial applications, it is desirable to build compact, interpretable learning
models where the output depends on a small number of input features. Recent work has shown that such best-subset
selection-type problems can be solved with modern mixed integer optimization solvers. Despite their promise, such solvers
often come at a steep computational price when compared with open-source, efficient specialized solvers based on convex
optimization and greedy heuristics. In “Fast Best-Subset Selection: Coordinate Descent and Local Combinatorial Optimization
 Algorithms,” Hussein Hazimeh and Rahul Mazumder push the frontiers of computation for best-subset-type problems.
 Their algorithms deliver near-optimal solutions for problems with up to a million features—in times comparable with the
 fast convex solvers. Their work suggests that principled optimization methods play a key role in devising tools central
 to interpretable machine learning, which can help in gaining a deeper understanding of their statistical properties. }
}


@article{Fan2008HDClasFAIR,
author = {Jianqing Fan and Yingying Fan},
title = {{High-dimensional classification using features annealed independence rules}},
volume = {36},
journal = {The Annals of Statistics},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {2605 -- 2637},
keywords = {classification, feature extraction, high dimensionality, independence rule, misclassification rates},
year = {2008},
doi = {https://doi.org/10.1214/07-AOS504},
URL = {https://doi.org/10.1214/07-AOS504}
}
@misc{wang2016LSinHD,
      title={No penalty no tears: Least squares in high-dimensional linear models},
      author={Xiangyu Wang and David Dunson and Chenlei Leng},
      year={2016},
      eprint={1506.02222},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{BurnhamAnderson2004MMIAIC,
author = {Kenneth P. Burnham and David R. Anderson},
title ={Multimodel Inference: Understanding AIC and BIC in Model Selection},
journal = {Sociological Methods \& Research},
volume = {33},
number = {2},
pages = {261-304},
year = {2004},
doi = {https://doi.org/10.1177/0049124104268644},

URL = {
        https://doi.org/10.1177/0049124104268644

},
eprint = {
        https://doi.org/10.1177/0049124104268644

}
,
    abstract = { The model selection literature has been generally poor at reflecting the deep foundations of the Akaike information criterion (AIC) and at making appropriate comparisons to the Bayesian information criterion (BIC). There is a clear philosophy, a sound criterion based in information theory, and a rigorous statistical foundation for AIC. AIC can be justified as Bayesian using a “savvy” prior on models that is a function of sample size and the number of model parameters. Furthermore, BIC can be derived as a non-Bayesian result. Therefore, arguments about using AIC versus BIC for model selection cannot be from a Bayes versus frequentist perspective. The philosophical context of what is assumed about reality, approximating models, and the intent of model-based inference should determine whether AIC or BIC is used. Various facets of such multimodel inference are presented here, particularly methods of model averaging. }
}

@article{CLAESKENS2016ForecastCombPuzzle,
title = {The forecast combination puzzle: A simple theoretical explanation},
journal = {International Journal of Forecasting},
volume = {32},
number = {3},
pages = {754-762},
year = {2016},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2015.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0169207016000327},
author = {Gerda Claeskens and Jan R. Magnus and Andrey L. Vasnev and Wendun Wang},
keywords = {Forecast combination, Optimal weights},
abstract = {This paper offers a theoretical explanation for the stylized fact that forecast combinations with estimated optimal weights often perform poorly in applications. The properties of the forecast combination are typically derived under the assumption that the weights are fixed, while in practice they need to be estimated. If the fact that the weights are random rather than fixed is taken into account during the optimality derivation, then the forecast combination will be biased (even when the original forecasts are unbiased), and its variance will be larger than in the fixed-weight case. In particular, there is no guarantee that the ‘optimal’ forecast combination will be better than the equal-weight case, or even improve on the original forecasts. We provide the underlying theory, some special cases, and a numerical illustration.}
}


@misc{fan2022bridging,
      title={Bridging factor and sparse models},
      author={Jianqing Fan and Ricardo Masini and Marcelo C. Medeiros},
      year={2022},
      eprint={2102.11341},
      archivePrefix={arXiv},
      primaryClass={econ.EM}
}

@article{Zhang2017RegressionPhalanxes,
  title={Regression Phalanxes},
  author={Hongyang Zhang and William J. Welch and Ruben H. Zamar},
  journal={arXiv: Machine Learning},
  year={2017}
}


@article{meinshausen2009stabilitySelection,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/40802220},
 abstract = {Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.},
 author = {Nicolai Meinshausen and Peter Bühlmann},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {4},
 pages = {417--473},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Stability selection},
 urldate = {2023-09-05},
 volume = {72},
 year = {2010}
}



@article{barber2018knockoffFilter,
author = {Rina Foygel Barber and Emmanuel J. Candes},
title = {{A knockoff filter for high-dimensional selective inference}},
volume = {47},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {2504 -- 2537},
keywords = {false discovery rate (FDR), high-dimensional regression, Knockoffs, Variable selection},
year = {2019},
doi = {https://doi.org/10.1214/18-AOS1755},
URL = {https://doi.org/10.1214/18-AOS1755}
}


@article{Reeve2018DivAndDFinRegEnsembles,
title = {Diversity and degrees of freedom in regression ensembles},
journal = {Neurocomputing},
volume = {298},
pages = {55-68},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.12.066},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218302133},
author = {Henry WJ Reeve and Gavin Brown},
keywords = {Degrees of freedom, Negative Correlation Learning, Tikhonov regularisation, Ensembles, Stein’s unbiased risk estimate, Deep neural networks},
abstract = {Ensemble methods are a cornerstone of modern machine learning. The performance of an ensemble depends crucially upon the level of diversity between its constituent learners. This paper establishes a connection between diversity and degrees of freedom (i.e. the capacity of the model), showing that diversity may be viewed as a form of inverse regularisation. This is achieved by focusing on a previously published algorithm Negative Correlation Learning (NCL), in which model diversity is explicitly encouraged through a diversity penalty term in the loss function. We provide an exact formula for the effective degrees of freedom in an NCL ensemble with fixed basis functions, showing that it is a continuous, convex and monotonically increasing function of the diversity parameter. We demonstrate a connection to Tikhonov regularisation and show that, with an appropriately chosen diversity parameter, an NCL ensemble can always outperform the unregularised ensemble in the presence of noise. We demonstrate the practical utility of our approach by deriving a method to efficiently tune the diversity parameter. Finally, we use a Monte-Carlo estimator to extend the connection between diversity and degrees of freedom to ensembles of deep neural networks.}
}

@article{CribariNeto2000InvMomentsBinom,
title = {A Note on Inverse Moments of Binomial Variates},
author = {Cribari-Neto, Francisco and Garcia, Nancy Lopes and Vasconcellos, Klaus L. P.},
year = {2000},
journal = {Brazilian Review of Econometrics},
volume = {20},
number = {2},
url = {https://EconPapers.repec.org/RePEc:sbe:breart:v:20:y:2000:i:2:a:2760}
}

@inproceedings{Maillard2009CompressedLS,
 author = {Maillard, Odalric and Munos, R\'{e}mi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
 publisher = {Curran Associates, Inc.},
 title = {Compressed Least-Squares Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/01882513d5fa7c329e940dda99b12147-Paper.pdf},
 volume = {22},
 year = {2009}
}


@article{ACHLIOPTAS2003JL,
title = {Database-Friendly Random Projections: Johnson-Lindenstrauss with Binary Coins},
journal = {Journal of Computer and System Sciences},
volume = {66},
number = {4},
pages = {671-687},
year = {2003},
note = {Special Issue on PODS 2001},
issn = {0022-0000},
doi = {10.1016/S0022-0000(03)00025-4},
author = {Dimitris Achlioptas},
abstract = {A classic result of Johnson and Lindenstrauss asserts that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space—where k is logarithmic in n and independent of d—so that all pairwise distances are maintained within an arbitrarily small factor. All known constructions of such embeddings involve projecting the n points onto a spherically random k-dimensional hyperplane through the origin. We give two constructions of such embeddings with the property that all elements of the projection matrix belong in {−1,0,+1}. Such constructions are particularly well suited for database environments, as the computation of the embedding reduces to evaluating a single aggregate over k random partitions of the attributes.}
}


@article{FRANKL1988JLSphere,
title = {The Johnson-Lindenstrauss Lemma and the Sphericity of Some Graphs},
journal = {Journal of Combinatorial Theory, Series B},
volume = {44},
number = {3},
pages = {355-362},
year = {1988},
issn = {0095-8956},
doi = {10.1016/0095-8956(88)90043-3},
author = {P Frankl and H Maehara},
}
@article{fan2001variable,
  title={Variable selection via nonconcave penalized likelihood and its oracle properties},
  author={Fan, Jianqing and Li, Runze},
  journal={Journal of the American statistical Association},
  volume={96},
  number={456},
  pages={1348--1360},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{yang2016,
	author = {Yun Yang and Martin J. Wainwright and Michael I. Jordan},
	doi = {10.1214/15-AOS1417},
	journal = {The Annals of Statistics},
	keywords = {Bayesian variable selection, high-dimensional inference, Markov chain, rapid mixing, spectral gap},
	number = {6},
	pages = {2497 -- 2532},
	publisher = {Institute of Mathematical Statistics},
	title = {On the computational complexity of high-dimensional {B}ayesian variable selection},
	url = {https://doi.org/10.1214/15-AOS1417},
	volume = {44},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1214/15-AOS1417}}

@article{ahfock2021statistical,
  title={Statistical Properties of Sketching Algorithms},
  author={Ahfock, Daniel C and Astle, William J and Richardson, Sylvia},
  journal={Biometrika},
  volume={108},
  number={2},
  pages={283--297},
  year={2021},
  publisher={Oxford University Press},
  url = {https://doi.org/10.1093/biomet/asaa062}
}

@MISC {Did2018StackExchRatioBinom,
    TITLE = {Ratio of two binomial distributions},
    AUTHOR = {Did (https://math.stackexchange.com/users/6179/did)},
    HOWPUBLISHED = {Mathematics Stack Exchange},
    NOTE = {URL:https://math.stackexchange.com/q/2863492 (version: 2022-10-17)},
    EPRINT = {https://math.stackexchange.com/q/2863492},
    URL = {https://math.stackexchange.com/q/2863492}
}


@article{Forsgren2001WeighLS,
author = {Forsgren, Anders and Sporre, G\"{o}ran},
title = {On Weighted Linear Least-Squares Problems Related to Interior Methods for Convex Quadratic Programming},
journal = {SIAM Journal on Matrix Analysis and Applications},
volume = {23},
number = {1},
pages = {42-56},
year = {2001},
doi = {https://doi.org/10.1137/S0895479800372298},
URL = {
        https://doi.org/10.1137/S0895479800372298
},
eprint = {
        https://doi.org/10.1137/S0895479800372298
}
,
    abstract = { It is known that the norm of the solution to a weighted linear least-squares problem is uniformly bounded for the set of diagonally dominant symmetric positive definite weight matrices. This result is extended to weight matrices that are nonnegative linear combinations of symmetric positive semidefinite matrices. Further, results are given concerning the strong connection between the boundedness of weighted projection onto a subspace and the projection onto its complementary subspace using the inverse weight matrix. In particular, explicit bounds are given for the Euclidean norm of the projections. These results are applied to the Newton equations arising in a primal-dual interior method for convex quadratic programming and boundedness is shown for the corresponding projection operator. }
}

@article{Silverstein1985SmEVWish,
 ISSN = {00911798},
 URL = {http://www.jstor.org/stable/2244186},
 abstract = {For positive integers s, n let Ms = (1/s)VsV T s, where Vs is an n × s matrix composed of i.i.d. N(0, 1) random variables. Assume n = n(s) and n/s → y ∈ (0, 1) as s → ∞. Then it is shown that the smallest eigenvalue of Ms converges almost surely to $(1 - \sqrt y)^2$ as s → ∞.},
 author = {Jack W. Silverstein},
 journal = {The Annals of Probability},
 number = {4},
 pages = {1364--1368},
 publisher = {Institute of Mathematical Statistics},
 title = {The Smallest Eigenvalue of a Large Dimensional Wishart Matrix},
 urldate = {2023-08-31},
 volume = {13},
 year = {1985}
}



@misc{karoui2003largestWishEV,
      title={On the largest eigenvalue of Wishart matrices with identity covariance when n, p and p/n tend to infinity},
      author={El Karoui, Noureddine},
      year={2003},
      eprint={math/0309355},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}

@article{TracyWidom1996,
  doi = {https://doi.org/10.1007/bf02099545},
  url = {https://doi.org/10.1007/bf02099545},
  year = {1996},
  month = apr,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {177},
  number = {3},
  pages = {727--754},
  author = {Craig A. Tracy and Harold Widom},
  title = {On orthogonal and symplectic matrix ensembles},
  journal = {Communications in Mathematical Physics}
}

@article{Johnstone2001LarEVinPCA,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2674106},
 abstract = {Let x(1) denote the square of the largest singular value of an n × p matrix X, all of whose entries are independent standard Gaussian variates. Equivalently, x(1) is the largest principal component variance of the covariance matrix X'X, or the largest eigenvalue of a p-variate Wishart distribution on n degrees of freedom with identity covariance. Consider the limit of large p and n with n/p = γ ≥ 1. When centered by $\mu_p = (\sqrt{n - 1} + \sqrt p)^2$ and scaled by $\sigma_p = (\sqrt{n - 1} + \sqrt p)(1/\sqrt{n - 1} + 1/ \sqrt p)^{1/3}$, the distribution of x(1) approaches the Tracy-Widom law of order 1, which is defined in terms of the Painleve II differential equation and can be numerically evaluated and tabulated in software. Simulations show the approximation to be informative for n and p as small as 5. The limit is derived via a corresponding result for complex Wishart matrices using methods from random matrix theory. The result suggests that some aspects of large p multivariate distribution theory may be easier to apply in practice than their fixed p counterparts.},
 author = {Iain M. Johnstone},
 journal = {The Annals of Statistics},
 number = {2},
 pages = {295--327},
 publisher = {Institute of Mathematical Statistics},
 title = {On the Distribution of the Largest Eigenvalue in Principal Components Analysis},
 urldate = {2023-09-05},
 volume = {29},
 year = {2001}
}

@Manual{spareg,
  title = {\pkg{spareg}: Sparse Projected Averaged Regression in \proglang{R}},
  author = {Laura Vana-Gür and Roman Parzer and Peter Filzmoser},
  year = {2025},
  note = {\proglang{R} package version 1.0.0},
  url = {https://CRAN.R-project.org/package=spareg},
}

@Inbook{TracyWidom2000,
author="Tracy, Craig A.
and Widom, Harold",
editor="van Diejen, Jan Felipe
and Vinet, Luc",
title="The Distribution of the Largest Eigenvalue in the Gaussian Ensembles: $\beta$ = 1, 2, 4",
bookTitle="Calogero---Moser--- Sutherland Models",
year="2000",
publisher="Springer New York",
address="New York, NY",
pages="461--472",
abstract="The focus of this survey is on the distribution function FN$\beta$ (t) for the largest eigenvalue in the finite N Gaussian Orthogonal Ensemble (GOE, $\beta$ = 1), the Gaussian Unitary Ensemble (GUE, $\beta$ = 2), and the Gaussian Symplectic Ensemble (GSE, $\beta$ = 4) in the edge scaling limit of N {\textrightarrow} ∝. These limiting distribution functions are expressible in terms of a particular Painlev{\'e} II function. Comparisons are made with finite N simulations and of the universality of these distribution functions is discussed.",
isbn="978-1-4612-1206-5",
doi="https://doi.org/10.1007/978-1-4612-1206-5_29",
url="https://doi.org/10.1007/978-1-4612-1206-5_29"
}

@article{Karoui2007LarEVComplexWish,
author = {Noureddine El Karoui},
title = {{Tracy–Widom limit for the largest eigenvalue of a large class of complex sample covariance matrices}},
volume = {35},
journal = {The Annals of Probability},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {663 -- 714},
keywords = {operator determinants, Random matrix theory, steepest descent analysis, Toeplitz matrices, trace class operators, Tracy–Widom distributions, Wishart matrices},
year = {2007},
doi = {https://doi.org/10.1214/009117906000000917},
URL = {https://doi.org/10.1214/009117906000000917}
}


@article{EDELMAN1991DistrSmEVWishart,
title = {The distribution and moments of the smallest eigenvalue of a random matrix of wishart type},
journal = {Linear Algebra and its Applications},
volume = {159},
pages = {55-80},
year = {1991},
issn = {0024-3795},
doi = {https://doi.org/10.1016/0024-3795(91)90076-9},
url = {https://www.sciencedirect.com/science/article/pii/0024379591900769},
author = {Alan Edelman},
abstract = {Given a random rectangular m × n matrix with elements from a normal distribution, what is the distribution of the smallest singular value? To pose an equivalent question in the language of multivariate statistics, what is the distribution of the smallest eigenvalue of a matrix from the central Wishart distribution in the null case? We give new results giving the distribution as a simple recursion. This includes the more difficult case when n – m is an even integer, without resorting to zonal polynomials and hypergeometric functions of matrix arguments. With the recursion, one can obtain exact expressions for the density and the moments of the distribution in terms of functions usually no more complicated than polynomials, exponentials, and at worst ordinary hypergeometric functions. We further elaborate on the special cases when n – m = 0, 1, 2, and 3 and give a numerical table of the expected values for 2 ⩽ m ⩽ 25 and 0 ⩽ n – m ⩽ 25.}
}

@article{Edelman1988CondNumberWIsh,
author = {Edelman, Alan},
title = {Eigenvalues and Condition Numbers of Random Matrices},
journal = {SIAM Journal on Matrix Analysis and Applications},
volume = {9},
number = {4},
pages = {543-560},
year = {1988},
doi = {https://doi.org/10.1137/0609045},
URL = {         https://doi.org/10.1137/0609045},
eprint = {         https://doi.org/10.1137/0609045},
abstract = { Given a random matrix, what condition number should be expected? This paper presents a proof that for real or complex \$n \times n\$ matrices with elements from a standard normal distribution, the expected value of the log of the 2-norm condition number is asymptotic to \$\log n\$ as \$n \to \infty\$. In fact, it is roughly \$\log n + 1.537\$ for real matrices and \$\log n + 0.982\$ for complex matrices as \$n \to \infty\$. The paper discusses how the distributions of the condition numbers behave for large n for real or complex and square or rectangular matrices. The exact distributions of the condition numbers of \$2 \times n\$ matrices are also given.Intimately related to this problem is the distribution of the eigenvalues of Wishart matrices. This paper studies in depth the largest and smallest eigenvalues, giving exact distributions in some cases. It also describes the behavior of all the eigenvalues, giving an exact formula for the expected characteristic polynomial. }
}

@article{Chen2005CondNumberRandMat,
author = {Chen, Zizhong and Dongarra, Jack J.},
title = {Condition Numbers of Gaussian Random Matrices},
journal = {SIAM Journal on Matrix Analysis and Applications},
volume = {27},
number = {3},
pages = {603-620},
year = {2005},
doi = {10.1137/040616413},
}



@Article{glmnetR,
    title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
    author = {Jerome Friedman and Robert Tibshirani and Trevor Hastie},
    journal = {Journal of Statistical Software},
    year = {2010},
    volume = {33},
    number = {1},
    pages = {1--22},
    doi = {10.18637/jss.v033.i01},
  }




  @article{Tenenbaum2000ISOMAPFaceData,
author = {Joshua B. Tenenbaum  and Vin de Silva  and John C. Langford },
title = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
journal = {Science},
volume = {290},
number = {5500},
pages = {2319-2323},
year = {2000},

URL = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2319},
abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs—30,000 auditory nerve fibers or 106 optic nerve fibers—a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.}}



@misc{gruber2023forecasting,
      title={Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It depends!},
      author={Luis Gruber and Gregor Kastner},
      year={2023},
      eprint={2206.04902},
      archivePrefix={arXiv},
      primaryClass={econ.EM},
      doi = {https://doi.org/10.48550/arXiv.2206.04902},
      url = {https://doi.org/10.48550/arXiv.2206.04902}
}

@BOOK{Anderson2003MVStat,
  title     = "An introduction to multivariate statistical analysis",
  author    = "Anderson, T W",
  publisher = "John Wiley \& Sons",
  series    = "Wiley Series in Probability and Statistics",
  edition   =  3,
  month     =  jul,
  year      =  2003,
  address   = "Nashville, TN",
  language  = "en"
}

@article{Geppert2015RPforBayReg,
  doi = {https://doi.org/10.1007/s11222-015-9608-z},
  url = {https://doi.org/10.1007/s11222-015-9608-z},
  year = {2015},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {27},
  number = {1},
  pages = {79--101},
  author = {Leo N. Geppert and Katja Ickstadt and Alexander Munteanu and Jens Quedenfeld and Christian Sohler},
  title = {Random projections for Bayesian regression},
  journal = {Statistics and Computing}
}


@article{HOU2011PPWentzell,
title = {Fast and simple methods for the optimization of kurtosis used as a projection pursuit index},
journal = {Analytica Chimica Acta},
volume = {704},
number = {1},
pages = {1-15},
year = {2011},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2011.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0003267011010804},
author = {S. Hou and P.D. Wentzell},
keywords = {Optimization, Quasi-power method, Univariate kurtosis, Multivariate kurtosis, Projection pursuit, Independent component analysis},
abstract = {As a powerful method for exploratory data analysis, projection pursuit (PP) often outperforms principal component analysis (PCA) to discover important data structure. PP was proposed in 1970s but has not been widely used in chemistry largely because of the difficulty in the optimization of projection indices. In this work, new algorithms, referred as “quasi-power methods”, are proposed to optimize kurtosis as a projection index. The new algorithms are simple, fast, and stable, which makes the search for the global optimum more efficient in the presence of multiple local optima. Maximization of kurtosis is helpful in the detection of outliers, while minimization tends to reveal clusters in the data, so the ability to search separately for the maximum and minimum of kurtosis is desirable. The proposed algorithms can search for either with only minor changes. Unlike other methods, no optimization of step size is required and sphering or whitening of the data is not necessary. Both univariate and multivariate kurtosis can be optimized by the proposed algorithms. The performance of the algorithms is evaluated using three simulated data sets and its utility is demonstrated with three experimental data sets relevant to analytical chemistry.}
}

@Inbook{Pappu2014HDCOverview,
author="Pappu, Vijay
and Pardalos, Panos M.",
title="High-Dimensional Data Classification",
bookTitle="Clusters, Orders, and Trees: Methods and Applications: In Honor of Boris Mirkin's 70th Birthday",
year="2014",
publisher="Springer New York",
address="New York, NY",
pages="119--150",
abstract="Recently, high-dimensional classification problems have been ubiquitous due to significant advances in technology. High dimensionality poses significant statistical challenges and renders many traditional classification algorithms impractical to use. In this chapter, we present a comprehensive overview of different classifiers that have been highly successful in handling high-dimensional data classification problems. We start with popular methods such as Support Vector Machines and variants of discriminant functions and discuss in detail their applications and modifications to several problems in high-dimensional settings. We also examine regularization techniques and their integration to several existing algorithms. We then discuss more recent methods, namely the hybrid classifiers and the ensemble classifiers. Feature selection techniques, as a part of hybrid classifiers, are introduced and their relative merits and drawbacks are examined. Lastly, we describe AdaBoost and Random Forests in the ensemble classifiers and discuss their recent surge as useful algorithms for solving high-dimensional data problems.",
isbn="978-1-4939-0742-7",
doi="10.1007/978-1-4939-0742-7_8",
url="https://doi.org/10.1007/978-1-4939-0742-7_8"
}



@misc{Salaro2018MultiLogitHD,
      title={Multinomial Logistic Regression With High Dimensional Data},
      author={Rossana Salaro and Cristiano Varin},
      year={2018},
      note         = {Available at \url{http://dspace.unive.it/bitstream/handle/10579/13814/847168-1223506.pdf?sequence=2}},
      eprint={1906.09489},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{RaeisiShahraki2017KIN,
  title={K Important Neighbors: A Novel Approach to Binary Classification in High Dimensional Data},
  author={Hadi Raeisi Shahraki and Saeedeh Pourahmad and Najaf Zare},
  journal={BioMed Research International},
  year={2017},
  volume={2017},
  url={https://api.semanticscholar.org/CorpusID:23353660}
}
@article{cannings2017random,
  title={Random-Projection Ensemble Classification},
  author={Cannings, Timothy I and Samworth, Richard J},
  journal={Journal of the Royal Statistical Society B},
  volume={79},
  doi = {10.1111/rssb.12228},
  number={4},
  pages={959--1035},
  year={2017},
  publisher={Oxford University Press}
}

@article{xie2016comparison,
  title={Comparison among dimensionality reduction techniques based on Random Projection for cancer classification},
  author={Xie, Haozhe and Li, Jie and Zhang, Qiaosheng and Wang, Yadong},
  journal={Computational biology and chemistry},
  volume={65},
  pages={165--172},
  year={2016},
  publisher={Elsevier},
  doi = {https://doi.org/10.1016/j.compbiolchem.2016.09.010}
}

@article{cook2019partial,
  title={Partial least squares prediction in high-dimensional regression},
  author={Cook, R Dennis and Forzani, Liliana},
  journal={The Annals of Statistics},
  volume={47},
  number={2},
  pages={884--908},
  year={2019},
  publisher={JSTOR}
}

@article{cannings2021overview,
	abstract = {Abstract Random projections offer an appealing and flexible approach to a wide range of large-scale statistical problems. They are particularly useful in high-dimensional settings, where we have many covariates recorded for each observation. In classification problems, there are two general techniques using random projections. The first involves many projections in an ensemble---the idea here is to aggregate the results after applying different random projections, with the aim of achieving superior statistical accuracy. The second class of methods include hashing and sketching techniques, which are straightforward ways to reduce the complexity of a problem, perhaps therefore with a huge computational saving, while approximately preserving the statistical efficiency. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences > Clustering and Classification Statistical and Graphical Methods of Data Analysis > Analysis of High Dimensional Data Statistical Models > Classification Models},
	author = {Cannings, Timothy I.},
	doi = {https://doi.org/10.1002/wics.1499},
	eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1499},
	journal = {WIREs Computational Statistics},
	keywords = {classification, data perturbation, ensemble, high dimensional, large scale, random projection, sketching},
	number = {1},
	pages = {e1499},
	title = {Random projections: Data perturbation for classification problems},
	url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1499},
	volume = {13},
	year = {2021},
	bdsk-url-1 = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1499},
	bdsk-url-2 = {https://doi.org/10.1002/wics.1499}}

@article{Wang2019HDRinPracticeStudy,
   title={High-dimensional regression in practice: an empirical study of finite-sample prediction, variable selection and ranking},
   volume={30},
   ISSN={1573-1375},
   url={http://dx.doi.org/10.1007/s11222-019-09914-9},
   DOI={10.1007/s11222-019-09914-9},
   number={3},
   journal={Statistics and Computing},
   publisher={Springer Science and Business Media LLC},
   author={Wang, Fan and Mukherjee, Sach and Richardson, Sylvia and Hill, Steven M.},
   year={2019},
   month=dec, pages={697–719} }

@Manual{plsR,
    title = {pls: Partial Least Squares and Principal Component Regression},
    author = {Kristian Hovde Liland and Bjørn-Helge Mevik and Ron Wehrens},
    year = {2022},
    note = {\proglang{R}~package version 2.8-1},
    url = {https://CRAN.R-project.org/package=pls},
}

%% Model averaging in GLMs

@article{Zhang2016optimalmodel,
	author = {Xinyu Zhang, Dalei Yu, Guohua Zou and Hua Liang},
	doi = {10.1080/01621459.2015.1115762},
	eprint = {https://doi.org/10.1080/01621459.2015.1115762},
	journal = {Journal of the American Statistical Association},
	number = {516},
	pages = {1775-1790},
	publisher = {Taylor & Francis},
	title = {Optimal Model Averaging Estimation for Generalized Linear Models and Generalized Linear Mixed-Effects Models},
	url = {https://doi.org/10.1080/01621459.2015.1115762},
	volume = {111},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2015.1115762}}


@article{ando2017weight,
  author = {Tomohiro Ando and Ker-chau Li},
title = {{A weight-relaxed model averaging approach for high-dimensional generalized linear models}},
volume = {45},
journal = {The Annals of Statistics},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {2654 -- 2679},
keywords = {asymptotic optimality, high-dimensional regression models, model averaging, model misspecification},
year = {2017},
doi = {10.1214/17-AOS1538},
URL = {https://doi.org/10.1214/17-AOS1538}
}

@article{zhang2023model,
  title={Model averaging prediction by K-fold cross-validation},
  author={Zhang, Xinyu and Liu, Chu-An},
  journal={Journal of Econometrics},
  volume={235},
  number={1},
  doi={https://doi.org/10.1016/j.jeconom.2022.04.007},
  pages={280--301},
  year={2023},
  publisher={Elsevier}
}


@article{Yuan2024modelave,
	author = {Chaoxia Yuan, Fang Fang and Jialiang Li},
	doi = {10.1080/07474938.2023.2280825},
	eprint = {https://doi.org/10.1080/07474938.2023.2280825},
	journal = {Econometric Reviews},
	number = {1},
	pages = {71-96},
	publisher = {Taylor & Francis},
	title = {Model averaging for generalized linear models in diverging model spaces with effective model size},
	url = {https://doi.org/10.1080/07474938.2023.2280825},
	volume = {43},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1080/07474938.2023.2280825}}

%% Screening for GLMs
@article{zhu2011model,
  title={Model-Free Feature Screening for Ultrahigh-Dimensional Data},
  author={Zhu, Li-Ping and Li, Lexin and Li, Runze and Zhu, Li-Xing},
  journal={Journal of the American Statistical Association},
  volume={106},
  number={496},
  pages={1464--1475},
  year={2011},
  doi={10.1198/jasa.2011.tm10563},
  publisher={Taylor \& Francis}
}
@article{Fan2010sisglms,
	author = {Jianqing Fan and Rui Song},
	doi = {10.1214/10-AOS798},
	journal = {The Annals of Statistics},
	keywords = {generalized linear models, independent learning, sure independent screening, Variable selection},
	number = {6},
	pages = {3567 -- 3604},
	publisher = {Institute of Mathematical Statistics},
	title = {Sure Independence Screening in Generalized Linear Models with {NP}-Dimensionality},
	volume = {38},
	year = {2010},
	}

@incollection{fan2010high,
  title={High-dimensional variable selection for Cox’s proportional hazards model},
  author={Fan, Jianqing and Feng, Yang and Wu, Yichao},
  booktitle={Borrowing strength: Theory powering applications--a Festschrift for Lawrence D. Brown},
  volume={6},
  pages={70--87},
  year={2010},
  doi={10.1214/10-IMSCOLL606},
  publisher={Institute of Mathematical Statistics}
}
@article{RaineyMcCaskey2021PMLLogit,
title={Estimating logit models with small samples},
volume={9},
DOI={10.1017/psrm.2021.9},
number={3},
journal={Political Science Research and Methods},
author={Rainey, Carlisle and McCaskey, Kelly},
year={2021},
pages={549–564}}

@article{fan2011nonparametric,
  title={Nonparametric Independence Screening in Sparse Ultra-High-Dimensional Additive Models},
  author={Fan, Jianqing and Feng, Yang and Song, Rui},
  journal={Journal of the American Statistical Association},
  volume={106},
  number={494},
  pages={544--557},
  doi={10.1198/jasa.2011.tm09779},
  year={2011},
  publisher={Taylor \& Francis}
}
@article{jiang2023feature,
  title={Feature Screening for High-Dimensional Variable Selection in Generalized Linear Models},
  author={Jiang, Jinzhu and Shang, Junfeng},
  journal={Entropy},
  volume={25},
  number={6},
  pages={851},
  year={2023},
  publisher={MDPI}
}

@article{mai2013kolmogorov,
  title={The Kolmogorov Filter for Variable Screening in High-Dimensional Binary Classification},
  author={Mai, Qing and Zou, Hui},
  journal={Biometrika},
  volume={100},
  doi = {10.1093/biomet/ass062},
  number={1},
  pages={229--234},
  year={2013},
  publisher={Oxford University Press}
}

@Article{glmnet2023,
    title = {Elastic Net Regularization Paths for All Generalized Linear Models},
    author = {J. Kenneth Tay and Balasubramanian Narasimhan and Trevor Hastie},
    journal = {Journal of Statistical Software},
    year = {2023},
    volume = {106},
    number = {1},
    pages = {1--31},
    doi = {10.18637/jss.v106.i01}
}
@article{mai2015fusedkolmogorov,
	author = {Qing Mai and Hui Zou},
	doi = {10.1214/14-AOS1303},
	journal = {The Annals of Statistics},
	keywords = {High-dimensional data, sure screening property, variable screening},
	number = {4},
	pages = {1471 -- 1497},
	publisher = {Institute of Mathematical Statistics},
	title = {The Fused Kolmogorov Filter: A Nonparametric Model-Free Screening Method},
	volume = {43},
	year = {2015},
}


@article{ke2023sufficient,
	author = {Chenlu Ke},
	doi = {10.1214/23-EJS2150},
	journal = {Electronic Journal of Statistics},
	keywords = {Conditional independence, rank consistency, reproducing kernel Hilbert space, sure screening},
	number = {2},
	pages = {2139 -- 2179},
	publisher = {Institute of Mathematical Statistics and Bernoulli Society},
	title = {Sufficient Variable Screening With High-Dimensional Controls},
	volume = {17},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1214/23-EJS2150}}

@article{barut2016conditional,
  title={Conditional Sure Independence Screening},
  author={Barut, Emre and Fan, Jianqing and Verhasselt, Anneleen},
  journal={Journal of the American Statistical Association},
  volume={111},
  number={515},
  pages={1266--1277},
  year={2016},
  publisher={Taylor \& Francis}
}


@article{liu2020knockoff,
	author = {Wanjun Liu, Yuan Ke, Jingyuan Liu and Runze Li},
	doi = {10.1080/01621459.2020.1783274},
	journal = {Journal of the American Statistical Association},
	number = {537},
	pages = {428-443},
	publisher = {Taylor & Francis},
	title = {Model-Free Feature Screening and FDR Control With Knockoff Features},
	volume = {117},
	year = {2022},
}

@book{mccullagh1989GLM,
  title={Generalized Linear Models, Second Edition},
  author={McCullagh, P. and Nelder, J.A.},
  isbn={9780412317606},
  lccn={99013896},
  series={Chapman \& Hall/CRC Monographs on Statistics \& Applied Probability},
  url={https://books.google.at/books?id=h9kFH2_FfBkC},
  year={1989},
  publisher={Taylor \& Francis}
}

@article{Rosset2004BoostingMaxMar,
author = {Rosset, Saharon and Zhu, Ji and Hastie, Trevor},
year = {2004},
month = {08},
pages = {941-973},
title = {Boosting as a Regularized Path to a Maximum Margin Classifier},
volume = {5},
journal = {Journal of Machine Learning Research}
}


@article{TRENCH1999EigValAR1,
title = {Asymptotic distribution of the spectra of a class of generalized Kac–Murdock–Szegö matrices},
journal = {Linear Algebra and its Applications},
volume = {294},
number = {1},
pages = {181-192},
year = {1999},
issn = {0024-3795},
doi = {https://doi.org/10.1016/S0024-3795(99)00080-4},
url = {https://www.sciencedirect.com/science/article/pii/S0024379599000804},
author = {William F. Trench},
keywords = {Toeplitz matrix, Kac–Mardock–Szegö matrix, Eigenvalues, Asymptotic distribution}
}


@article{CILIA2022darwin,
title = {Diagnosing Alzheimer’s disease from on-line handwriting: A novel dataset and performance benchmarking},
journal = {Engineering Applications of Artificial Intelligence},
volume = {111},
pages = {104822},
year = {2022},
issn = {0952-1976},
doi = {10.1016/j.engappai.2022.104822},
author = {Nicole D. Cilia and Giuseppe {De Gregorio} and Claudio {De Stefano} and Francesco Fontanella and Angelo Marcelli and Antonio Parziale},
keywords = {Neurodegenerative diseases, Health data, Alzheimer’s disease prediction, Handwriting analysis, Classification and combining strategies},
abstract = {Neurodegenerative diseases are caused by the progressive degeneration of nerve cells that affect motor skills and cognitive abilities with increasing severity. Unfortunately, there is no cure for this type of disease and their impact can only be slowed down with specific pharmacological and rehabilitative therapies. Early diagnosis, therefore, remains the primary means to delay brain damage and improve the quality of life of people affected. Neurodegenerative diseases also affect movement fine control. Consequently, the analysis of handwriting dynamics can represent an effective tool to support an early diagnosis of these diseases. While many methods have been proposed in the literature based on the use of a wide range of handwriting tasks, researchers have not yet defined a universally accepted standard experimental protocol to collect data. Furthermore, although some databases containing handwriting data have been produced, only a few of them were designed specifically for research on neurodegenerative diseases, and, in most cases, they involve a small number of participants performing a few tasks. Here, we introduce the DARWIN (Diagnosis AlzheimeR WIth haNdwriting) dataset to overcome these drawbacks, which contains handwriting samples from people affected by Alzheimer’s and a control group. The dataset includes data from 174 participants, acquired during the execution of handwriting tasks, performed according to a protocol specifically designed for the early detection of Alzheimer’s. We report the results of the experiments performed to evaluate the effectiveness of the proposed tasks and features in capturing the distinctive aspects of handwriting that support the diagnosis of Alzheimer’s disease.}
}



@article{bernanke2005measuring,
  title={{Measuring the effects of monetary policy: a factor-augmented vector autoregressive (FAVAR) approach}},
  author={Bernanke, Ben S. and Boivin, Jean and Eliasz, Piotr},
  journal={The Quarterly journal of economics},
  volume={120},
  number={1},
  pages={387--422},
  year={2005},
  publisher={MIT Press}
}

@article{BeyelerKaufmannFAVAR,
  title={{Reduced-form factor augmented VAR—Exploiting sparsity to include meaningful factors}},
  author={Beyeler, Simon and Kaufmann, Sylvia},
  journal={Journal of Applied Econometrics},
  volume={36},
  number={7},
  pages={989--1012},
  year={2021},
  doi             = {https://doi.org/10.1002/jae.2852}
  }

  @article{BayComprRegr,
author = {Rajarshi Guhaniyogi and David B. Dunson},
title = {{Bayesian Compressed Regression}},
journal = {Journal of the American Statistical Association},
volume = {110},
number = {512},
pages = {1500-1514},
year  = {2015},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1080/01621459.2014.969425},
URL = {
        https://doi.org/10.1080/01621459.2014.969425},
eprint = {
        https://doi.org/10.1080/01621459.2014.969425}
}

@misc{piironen2017iterative,
      title={{Iterative Supervised Principal Components}},
      author={Juho Piironen and Aki Vehtari},
      year={2017},
      eprint={1710.06229},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{AdragniCook2009SDR,
author = {Adragni, Kofi P.  and Cook, R. Dennis },
title = {{Sufficient dimension reduction and prediction in regression}},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
volume = {367},
number = {1906},
pages = {4385-4405},
year = {2009},
doi = {https://doi.org/10.1098/rsta.2009.0110},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2009.0110},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2009.0110},
abstract = { Dimension reduction for regression is a prominent issue today because technological advances now allow scientists to routinely formulate regressions in which the number of predictors is considerably larger than in the past. While several methods have been proposed to deal with such regressions, principal components (PCs) still seem to be the most widely used across the applied sciences. We give a broad overview of ideas underlying a particular class of methods for dimension reduction that includes PCs, along with an introduction to the corresponding methodology. New methods are proposed for prediction in regressions with many predictors. }
}

@book{hastie2009elements,
  title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author={Hastie, T. and Tibshirani, R. and Friedman, J.H.},
  isbn={9780387848846},
  lccn={2008941148},
  series={Springer series in statistics},
  url={https://books.google.at/books?id=eBSgoAEACAAJ},
  year={2009},
  publisher={Springer}
}

@article{bair2006supervisedPC,
author = {Eric Bair and Trevor Hastie and Debashis Paul and Robert Tibshirani},
title = {{Prediction by Supervised Principal Components}},
journal = {Journal of the American Statistical Association},
volume = {101},
number = {473},
pages = {119-137},
year  = {2006},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1198/016214505000000628},
URL = {
        https://doi.org/10.1198/016214505000000628},
eprint = {
        https://doi.org/10.1198/016214505000000628}
}

@article{TSIONAS2022103952,
title = {{Estimation of large dimensional time varying VARs using copulas}},
journal = {European Economic Review},
volume = {141},
pages = {103952},
year = {2022},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2021.103952},
url = {https://www.sciencedirect.com/science/article/pii/S0014292121002439},
author = {Mike G. Tsionas and Marwan Izzeldin and Lorenzo Trapani},
keywords = {Vector AutoRegression, Time-varying parameters, Heteroskedasticity, Copulas},
abstract = {This paper provides a simple, yet reliable, alternative to the (Bayesian) estimation of large multivariate VARs with time variation in the conditional mean equations and/or in the covariance structure. The original multivariate, n-dimensional model is treated as a set of n univariate estimation problems, and cross-dependence is handled through the use of a copula. This makes it possible to run the estimation of each univariate equation in parallel. Thus, only univariate distribution functions are needed when estimating the individual equations, which are often available in closed form, and easy to handle with MCMC (or other techniques). Thereafter, the individual posteriors are combined with the copula, so obtaining a joint posterior which can be easily resampled. We illustrate our approach using various examples of large time-varying parameter VARs with 129 and even 215 macroeconomic variables.}
}

@article{CookNi2005IRforDR,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/27590565},
 abstract = {A family of dimension-reduction methods, the inverse regression (IR) family, is developed by minimizing a quadratic objective function. An optimal member of this family, the inverse regression estimator (IRE), is proposed, along with inference methods and a computational algorithm. The IRE has at least three desirable properties: (1) Its estimated basis of the central dimension reduction subspace is asymptotically efficient, (2) its test statistic for dimension has an asymptotic chi-squared distribution, and (3) it provides a chi-squared test of the conditional independence hypothesis that the response is independent of a selected subset of predictors given the remaining predictors. Current methods like sliced inverse regression belong to a suboptimal class of the IR family. Comparisons of these methods are reported through simulation studies. The approach developed here also allows a relatively straightforward derivation of the asymptotic null distribution of the test statistic for dimension used in sliced average variance estimation.},
 author = {R. Dennis Cook and Liqiang Ni},
 journal = {Journal of the American Statistical Association},
 number = {470},
 pages = {410--428},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {{Sufficient Dimension Reduction via Inverse Regression: A Minimum Discrepancy Approach}},
 volume = {100},
 year = {2005}
}

@article{wang2015conditional,
  title={Conditional Distance Correlation},
  author={Wang, Xueqin and Pan, Wenliang and Hu, Wenhao and Tian, Yuan and Zhang, Heping},
  journal={Journal of the American Statistical Association},
  volume={110},
  number={512},
  pages={1726--1734},
  doi={10.1080/01621459.2014.993081},
  year={2015},
  publisher={Taylor \& Francis}
}

@article{Huber1985PP,
author = {Peter J. Huber},
title = {{Projection Pursuit}},
volume = {13},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {435 -- 475},
keywords = {Computer tomography, minimum entropy, multivariate data analysis, principal components, Projection pursuit, robust multivariate methods},
year = {1985},
doi = {https://doi.org/10.1214/aos/1176349519},
URL = {https://doi.org/10.1214/aos/1176349519}
}

@article{Reich2011SufficientDR,
  title={{Sufficient dimension reduction via bayesian mixture modeling.}},
  author={Brian J. Reich and Howard D. Bondell and Lexin Li},
  journal={Biometrics},
  year={2011},
  volume={67 3},
  pages={
          886-95
        }
}

@article{POWER2021BayMAvgSIR,
title = {{Bayesian model averaging sliced inverse regression}},
journal = {Statistics and Probability Letters},
volume = {174},
pages = {109103},
year = {2021},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2021.109103},
url = {https://www.sciencedirect.com/science/article/pii/S0167715221000651},
author = {Michael Declan Power and Yuexiao Dong},
keywords = {Central space, Markov chain Monte Carlo, Sufficient dimension reduction},
abstract = {As a popular sufficient dimension reduction method, sliced inverse regression (SIR) (Li, 1991) involves all the predictors. We propose Bayesian model averaging SIR when the central space only involves a subset of the predictors.}
}

@book{li2018sufficient,
  title={{Sufficient Dimension Reduction: Methods and Applications with R}},
  author={Li, B.},
  isbn={9781498704489},
  series={Chapman \& Hall/CRC Monographs on Statistics and Applied Probability},
  url={https://books.google.at/books?id=5pdYDwAAQBAJ},
  year={2018},
  publisher={CRC Press}
}

@article{Barshan2011SupervisedKPCA,
  title={{Supervised principal component analysis: Visualization, classification and regression on subspaces and submanifolds}},
  author={Elnaz Barshan and Ali Ghodsi and Zohreh Azimifar and Mansoor Zolghadri Jahromi},
  journal={Pattern Recognit.},
  year={2011},
  volume={44},
  pages={1357-1371}
}

@misc{ghojogh2021SDR_survey,
      title={{Sufficient Dimension Reduction for High-Dimensional Regression and Low-Dimensional Embedding: Tutorial and Survey}},
      author={Benyamin Ghojogh and Ali Ghodsi and Fakhri Karray and Mark Crowley},
      year={2021},
      eprint={2110.09620},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{BARCARU2019_SPP,
title = {{Supervised projection pursuit – A dimensionality reduction technique optimized for probabilistic classification}},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {194},
pages = {103867},
year = {2019},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2019.103867},
url = {https://www.sciencedirect.com/science/article/pii/S0169743919300309},
author = {Andrei Barcaru},
abstract = {An important step in multivariate analysis is the dimensionality reduction, which allows for a better classification and easier visualization of the class structures in the data. Techniques like PCA, PLS-DA and LDA are most often used to explore the patterns in the data and to reduce the dimensions. Yet the data does not always reveal properly the structures wen these techniques are applied. To this end, a supervised projection pursuit (SuPP) is proposed in this article, based on Jensen-Shannon divergence. The combination of this metric with powerful Monte Carlo based optimization algorithm, yielded a versatile dimensionality reduction technique capable of working with highly dimensional data and missing observations. Combined with Naïve Bayes (NB) classifier, SuPP proved to be a powerful preprocessing tool for classification. Namely, on the Iris data set, the prediction accuracy of SuPP-NB is significantly higher than the prediction accuracy of PCA-NB, (p-value ≤ 4.02E-05 in a 2D latent space, p-value ≤ 3.00E-03 in a 3D latent space) and significantly higher than the prediction accuracy of PLS-DA (p-value ≤ 1.17E-05 in a 2D latent space and p-value ≤ 3.08E-03 in a 3D latent space). The significantly higher accuracy for this particular data set is a strong evidence of a better class separation in the latent spaces obtained with SuPP.}
}
@article{matouvsek2008variants,
  title={On Variants of the Johnson--Lindenstrauss Lemma},
  author={Matou{\v{s}}ek, Ji{\v{r}}{\'\i}},
  journal={Random Structures \& Algorithms},
  volume={33},
  number={2},
  pages={142--156},
  year={2008},
  doi={10.1002/rsa.20218},
  publisher={Wiley Online Library}
}
@article{ESPEZUA2015PP_HD_smalldata,
title = {A Projection Pursuit framework for supervised dimension reduction of high dimensional small sample datasets},
journal = {Neurocomputing},
volume = {149},
pages = {767-776},
year = {2015},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2014.07.057},
url = {https://www.sciencedirect.com/science/article/pii/S0925231214010091},
author = {Soledad Espezua and Edwin Villanueva and Carlos D. Maciel and André Carvalho},
keywords = {Projection Pursuit, Classification, Gene expression, Dimension reduction},
abstract = {The analysis and interpretation of datasets with large number of features and few examples has remained as a challenging problem in the scientific community, owing to the difficulties associated with the curse-of-the-dimensionality phenomenon. Projection Pursuit (PP) has shown promise in circumventing this phenomenon by searching low-dimensional projections of the data where meaningful structures are exposed. However, PP faces computational difficulties in dealing with datasets containing thousands of features (typical in genomics and proteomics) due to the vast quantity of parameters to optimize. In this paper we describe and evaluate a PP framework aimed at relieving such difficulties and thus ease the construction of classifier systems. The framework is a two-stage approach, where the first stage performs a rapid compaction of the data and the second stage implements the PP search using an improved version of the SPP method (Guo et al., 2000, [32]). In an experimental evaluation with eight public microarray datasets we showed that some configurations of the proposed framework can clearly overtake the performance of eight well-established dimension reduction methods in their ability to pack more discriminatory information into fewer dimensions.}
}

@article{Friedman1981PPR,
author = { Jerome H.   Friedman  and  Werner   Stuetzle },
title = {{Projection Pursuit Regression}},
journal = {Journal of the American Statistical Association},
volume = {76},
number = {376},
pages = {817-823},
year  = {1981},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1080/01621459.1981.10477729},
URL = {
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1981.10477729},
eprint = {
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1981.10477729  }
}

@article{HUANG2006489ELM,
title = {{Extreme learning machine: Theory and applications}},
journal = {Neurocomputing},
volume = {70},
number = {1},
pages = {489-501},
year = {2006},
note = {Neural Networks},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2005.12.126},
url = {https://www.sciencedirect.com/science/article/pii/S0925231206000385},
author = {Guang-Bin Huang and Qin-Yu Zhu and Chee-Kheong Siew},
keywords = {Feedforward neural networks, Back-propagation algorithm, Extreme learning machine, Support vector machine, Real-time learning, Random node},
abstract = {It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: (1) the slow gradient-based learning algorithms are extensively used to train neural networks, and (2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these conventional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide good generalization performance at extremely fast learning speed. The experimental results based on a few artificial and real benchmark function approximation and classification problems including very large complex applications show that the new algorithm can produce good generalization performance in most cases and can learn thousands of times faster than conventional popular learning algorithms for feedforward neural networks.11For the preliminary idea of the ELM algorithm, refer to “Extreme Learning Machine: A New Learning Scheme of Feedforward Neural Networks”, Proceedings of International Joint Conference on Neural Networks (IJCNN2004), Budapest, Hungary, 25–29 July, 2004.}
}

@ARTICLE{Xinwei2012LVMGP,
  author={Jiang, Xinwei and Gao, Junbin and Wang, Tianjiang and Zheng, Lihong},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  title={{Supervised Latent Linear Gaussian Process Latent Variable Model for Dimensionality Reduction}},
  year={2012},
  volume={42},
  number={6},
  pages={1620-1632},
  doi={https://doi.org/10.1109/TSMCB.2012.2196995}}

@article{Dunson2016ComprGP,
  author  = {Rajarshi Guhaniyogi and David B. Dunson},
  title   = {{Compressed Gaussian Process for Manifold Regression}},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {69},
  pages   = {1-26},
  url     = {http://jmlr.org/papers/v17/14-230.html}
}

@article{GPJulia,
 title={GaussianProcesses.jl: A Nonparametric Bayes Package for the Julia Language},
 volume={102},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v102i01},
 doi={https://doi.org/10.18637/jss.v102.i01},
 abstract={&amp;lt;p&amp;gt;Gaussian processes are a class of flexible nonparametric Bayesian tools that are widely used across the sciences, and in industry, to model complex data sources. Key to applying Gaussian process models is the availability of well-developed open source software, which is available in many programming languages. In this paper, we present a tutorial of the GaussianProcesses.jl package that has been developed for the Julia programming language. GaussianProcesses.jl utilizes the inherent computational benefits of the Julia language, including multiple dispatch and just-in-time compilation, to produce a fast, flexible and user-friendly Gaussian processes package. The package provides many mean and kernel functions with supporting inference tools to fit exact Gaussian process models, as well as a range of alternative likelihood functions to handle non-Gaussian data (e.g., binary classification models) and sparse approximations for scalable Gaussian processes. The package makes efficient use of existing Julia packages to provide users with a range of optimization and plotting tools.&amp;lt;/p&amp;gt;},
 number={1},
 journal={Journal of Statistical Software},
 author={Fairbrother, Jamie and Nemeth, Christopher and Rischard, Maxime and Brea, Johanni and Pinder, Thomas},
 year={2022},
 pages={1–36}
}

@book{Rasmussen2006GP,
  added-at = {2020-07-17T00:00:00.000+0200},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  biburl = {https://www.bibsonomy.org/bibtex/2670a576a21065048f7ddede17e09b6b4/dblp},
  ee = {https://www.worldcat.org/oclc/61285753},
  interhash = {72c030472023000e0bdeeb06081c3764},
  intrahash = {670a576a21065048f7ddede17e09b6b4},
  isbn = {026218253X},
  keywords = {dblp},
  pages = {I-XVIII, 1-248},
  publisher = {MIT Press},
  series = {Adaptive computation and machine learning},
  timestamp = {2020-07-24T00:45:17.000+0200},
  title = {{Gaussian processes for machine learning.}},
  year = 2006
}


@article{Gagnon2021LPTN,
author = {Philippe Gagnon and Mylène Bédard and Alain Desgagné},
title = {{An automatic robust Bayesian approach to principal component regression}},
journal = {Journal of Applied Statistics},
volume = {48},
number = {1},
pages = {84-104},
year  = {2021},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1080/02664763.2019.1710478},

URL = {
        https://doi.org/10.1080/02664763.2019.1710478

},
eprint = {
        https://doi.org/10.1080/02664763.2019.1710478

}

}

@article{GagnonBayOutlRobLPTN,
author = {Philippe Gagnon and Alain Desgagné and Mylène Bédard},
title = {{A New Bayesian Approach to Robustness Against Outliers in Linear Regression}},
volume = {15},
journal = {Bayesian Analysis},
number = {2},
publisher = {International Society for Bayesian Analysis},
pages = {389 -- 414},
keywords = {ANCOVA, ANOVA, Built-in robustness, maximum likelihood estimation, super heavy-tailed distributions, Variable selection, whole robustness},
year = {2020},
doi = {https://doi.org/10.1214/19-BA1157},
URL = {https://doi.org/10.1214/19-BA1157}
}


@article{HAMURA2022107517,
	abstract = {Linear regression that employs the assumption of normality for the error distribution may lead to an undesirable posterior inference of regression coefficients due to potential outliers. A finite mixture of two components, one with thin and one with heavy tails, is considered as the error distribution in this study. For the heavily-tailed component, the novel class of distributions is introduced; their densities are log-regularly varying and have heavier tails than the Cauchy distribution. Yet, they are expressed as a scale mixture of normals which enables the efficient posterior inference when using a Gibbs sampler. The robustness of the posterior distributions is proved under the proposed models using a minimal set of assumptions, which justifies the use of shrinkage priors with unbounded densities for the coefficient vector in the presence of outliers. An extensive comparison with the existing methods via simulation study shows the improved performance of the proposed model in point and interval estimation, as well as its computational efficiency. Further, the posterior robustness of the proposed method is confirmed in an empirical study with shrinkage priors for regression coefficients.},
	author = {Yasuyuki Hamura and Kaoru Irie and Shonosuke Sugasawa},
	doi = {https://doi.org/10.1016/j.csda.2022.107517},
	issn = {0167-9473},
	journal = {Computational Statistics \& Data Analysis},
	keywords = {Robust statistics, Linear regression, Heavily-tailed distribution, Scale mixture of normals, Log-regularly varying density, Gibbs sampler},
	pages = {107517},
	title = {Log-regularly varying scale mixture of normals for robust regression},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947322000974},
	volume = {173},
	year = {2022}
}


@ARTICLE{Rabc,
  title     = "abc: an {R} package for approximate Bayesian computation
               ({ABC)}: {R} package: abc",
  author    = "Csill{\'e}ry, Katalin and Fran{\c c}ois, Olivier and Blum,
               Michael G B",
  journal   = "Methods Ecol. Evol.",
  publisher = "Wiley",
  volume    =  3,
  number    =  3,
  pages     = "475--479",
  month     =  jun,
  year      =  2012,
  url       = "http://dx.doi.org/10.1111/j.2041-210x.2011.00179.x",
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en",
  issn      = "2041-210X",
  doi       = "https://doi.org/10.1111/j.2041-210x.2011.00179.x"
}

@article{Rhmclearn,
author = {Samuel Thomas and Wanzhu Tu},
title = {{Learning Hamiltonian Monte Carlo in R}},
journal = {The American Statistician},
volume = {75},
number = {4},
pages = {403-413},
year  = {2021},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1080/00031305.2020.1865198},

URL = {
        https://doi.org/10.1080/00031305.2020.1865198

},
eprint = {
        https://doi.org/10.1080/00031305.2020.1865198

}

}


@Article{Vogelstein2021LOL,
author={Vogelstein, Joshua T.
and Bridgeford, Eric W.
and Tang, Minh
and Zheng, Da
and Douville, Christopher
and Burns, Randal
and Maggioni, Mauro},
title={{Supervised dimensionality reduction for big data}},
journal={Nature Communications},
year={2021},
month={5},
day={17},
volume={12},
number={1},
pages={2872},
abstract={To solve key biomedical problems, experimentalists now routinely measure millions or billions of features (dimensions) per sample, with the hope that data science techniques will be able to build accurate data-driven inferences. Because sample sizes are typically orders of magnitude smaller than the dimensionality of these data, valid inferences require finding a low-dimensional representation that preserves the discriminating information (e.g., whether the individual suffers from a particular disease). There is a lack of interpretable supervised dimensionality reduction methods that scale to millions of dimensions with strong statistical theoretical guarantees. We introduce an approach to extending principal components analysis by incorporating class-conditional moment estimates into the low-dimensional projection. The simplest version, Linear Optimal Low-rank projection, incorporates the class-conditional means. We prove, and substantiate with both synthetic and real data benchmarks, that Linear Optimal Low-Rank Projection and its generalizations lead to improved data representations for subsequent classification, while maintaining computational efficiency and scalability. Using multiple brain imaging datasets consisting of more than 150 million features, and several genomics datasets with more than 500,000 features, Linear Optimal Low-Rank Projection outperforms other scalable linear dimensionality reduction techniques in terms of accuracy, while only requiring a few minutes on a standard desktop computer.},
issn={2041-1723},
doi={https://doi.org/10.1038/s41467-021-23102-2},
url={https://doi.org/10.1038/s41467-021-23102-2}
}


@InProceedings{Skorski2021RP,
  title = 	 {Johnson-Lindenstrauss Transforms with Best Confidence},
  author =       {Skorski, Maciej},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {3989--4007},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {8},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v134/skorski21a/skorski21a.pdf},
  url = 	 {https://proceedings.mlr.press/v134/skorski21a.html},
  abstract = 	 {The seminal result of Johnson and Lindenstrauss on random embeddings has been intensively studied in applied and theoretical computer science. Despite that vast body of literature, we still lack of complete understanding of statistical properties of random projections; a particularly intriguing question is: why are the theoretical bounds that far behind the empirically observed performance? Motivated by this question, this work develops Johnson-Lindenstrauss distributions with optimal, data-oblivious, statistical confidence bounds. These bounds are numerically best possible, for any given data dimension, embedding dimension, and distortion tolerance. They improve upon prior works in terms of statistical accuracy, as well as exactly determine the no-go regimes for data-oblivious approaches. Furthermore, the projection matrices are efficiently samplable.  The construction relies on orthogonal matrices, and the proof uses certain elegant properties of the unit sphere. In particular, the following techniques introduced in this work are of independent interest: a) a compact expression for the projection distortion in terms of singular eigenvalues of the projection matrix, b) a parametrization linking the unit sphere and the Dirichlet distribution and c) anti-concentration bounds for the Dirichlet distribution.  Besides the technical contribution, the paper presents applications and numerical evaluation along with working implementation in Python (shared as a GitHub repository).}
}

@misc{Wang2019RobModAvg,
  doi = {https://doi.org/10.48550/ARXIV.1910.12210},

  url = {https://arxiv.org/abs/1910.12210},

  author = {Wang, Miaomiao and Zou, Guohua},

  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {An outlier-robust model averaging approach by Mallows-type criterion},

  publisher = {arXiv},

  year = {2019},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{KURNAZ2018enetLTS,
title = {Robust and sparse estimation methods for high-dimensional linear and logistic regression},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {172},
pages = {211-222},
year = {2018},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2017.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0169743917301247},
author = {Fatma Sevinç Kurnaz and Irene Hoffmann and Peter Filzmoser},
keywords = {Elastic net penalty, Least trimmed squares, C-step algorithm, High-dimensional data, Robustness, Sparse estimation},
abstract = {Fully robust versions of the elastic net estimator are introduced for linear and logistic regression. The algorithms used to compute the estimators are based on the idea of repeatedly applying the non-robust classical estimators to data subsets only. It is shown how outlier-free subsets can be identified efficiently, and how appropriate tuning parameters for the elastic net penalties can be selected. A final reweighting step improves the efficiency of the estimators. Simulation studies compare with non-robust and other competing robust estimators and reveal the superiority of the newly proposed methods. This is also supported by a reasonable computation time and by good performance in real data examples.}
}

@Article{BRMS2017Buerkner,
  title = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {80},
  number = {1},
  pages = {1--28},
  doi = {https://doi.org/10.18637/jss.v080.i01},
  encoding = {UTF-8},
}

@Article{SCAD2011Breheny,
  author = {Patrick Breheny and Jian Huang},
  title = {Coordinate descent algorithms for nonconvex penalized regression,
	with applications to biological feature selection},
  journal = {Annals of Applied Statistics},
  year = {2011},
  volume = {5},
  pages = {232--253},
  number = {1},
}




@article{LanEtAl2006MiceGene,
    doi = {10.1371/journal.pgen.0020006},
    author = {Lan, Hong AND Chen, Meng AND Flowers, Jessica B AND Yandell, Brian S AND Stapleton, Donnie S AND Mata, Christine M AND Mui, Eric Ton-Keen AND Flowers, Matthew T AND Schueler, Kathryn L AND Manly, Kenneth F AND Williams, Robert W AND Kendziorski, Christina AND Attie, Alan D},
    journal = {PLOS Genetics},
    publisher = {Public Library of Science},
    title = {Combined Expression Trait Correlations and Expression Quantitative Trait Locus Mapping},
    year = {2006},
    month = {01},
    volume = {2},
    url = {https://doi.org/10.1371/journal.pgen.0020006},
    pages = {1-11},
    abstract = {Coordinated regulation of gene expression levels across a series of experimental conditions provides valuable information about the functions of correlated transcripts. The consideration of gene expression correlation over a time or tissue dimension has proved valuable in predicting gene function. Here, we consider correlations over a genetic dimension. In addition to identifying coregulated genes, the genetic dimension also supplies us with information about the genomic locations of putative regulatory loci. We calculated correlations among approximately 45,000 expression traits derived from 60 individuals in an F2 sample segregating for obesity and diabetes. By combining the correlation results with linkage mapping information, we were able to identify regulatory networks, make functional predictions for uncharacterized genes, and characterize novel members of known pathways. We found evidence of coordinate regulation of 174 G protein–coupled receptor protein signaling pathway expression traits. Of the 174 traits, 50 had their major LOD peak within 10 cM of a locus on Chromosome 2, and 81 others had a secondary peak in this region. We also characterized a Riken cDNA clone that showed strong correlation with stearoyl-CoA desaturase 1 expression. Experimental validation confirmed that this clone is involved in the regulation of lipid metabolism. We conclude that trait correlation combined with linkage mapping can reveal regulatory networks that would otherwise be missed if we studied only mRNA traits with statistically significant linkages in this small cross. The combined analysis is more sensitive compared with linkage mapping alone.},
    number = {1},

}

@ARTICLE{Beale2007PEPCK,
  title     = "{PCK1} and {PCK2} as candidate diabetes and obesity genes",
  author    = "Beale, Elmus G and Harvey, Brandy J and Forest, Claude",
  abstract  = "The PCK1 gene (Pck1 in rodents) encodes the cytosolic isozyme of
               phosphoenolpyruvate carboxykinase (PEPCK-C), which is well-known
               for its function as a gluconeogenic enzyme in the liver and
               kidney. Mouse studies involving whole body and tissue-specific
               Pck1 knockouts as well as tissue-specific over-expression of
               PEPCK-C have resulted in type 2 diabetes as well as several
               surprising phenotypes including obesity, lipodystrophy, fatty
               liver, and death. These phenotypes arise from perturbations not
               only in gluconeogenesis but in two additional metabolic
               functions of PEPCK-C: (1) cataplerosis which maintains metabolic
               flux through the Krebs cycle by removing excess oxaloacetate,
               and (2) glyceroneogenesis which produces glycerol-3-phosphate as
               a precursor for fatty acid esterification into triglycerides.
               PEPCK-C catalyzes the conversion of oxaloacetate + GTP to
               phosphoenolpyruvate + GDP + CO2. It is in part the
               tissue-specificity of this simple reaction that results in the
               variety of phenotypes listed above. Briefly: (1) A 7-fold
               over-expression of PEPCK-C in the livers of mice causes
               excessive glucose production. (2) Mice with a whole-body
               knockout of Pck1 die within 2-3 days of birth, not from
               hypoglycemia, but probably because the Krebs cycle slows to
               approximately 10\% of normal in the absence of cataplerosis. (3)
               Mice with a liver-specific knockout have an inability to remove
               oxaloacetate from the Krebs cycle, which leads to a fatty liver
               following a fast. (4) An adipose-specific knockout of Pck1
               results in a fraction of the mice developing lipodystrophy due
               to lost glyceroneogenesis and a consequent decrease in fatty
               acid re-esterification. (5) Finally, disregulated
               over-expression of PEPCK-C in adipose tissue increases fatty
               acid re-esterification leading to obesity. These varied
               experimental phenotypes in mice have led us to postulate that
               abnormal production of PEPCK isozymes encoded by two PEPCK
               genes, PCK1 and PCK2, in humans could have similar consequences
               (Beale, E. G. et al. (2004). Trends in Endocrinology and
               Metabolism, 15, 129-135). The purpose of this review is to
               further explore these possibilities.",
  journal   = "Cell Biochem. Biophys.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  48,
  number    = "2-3",
  pages     = "89--95",
  year      =  2007,
  language  = "en"
}

@ARTICLE{Mendez-Lucas2014-PEPCK,
  title     = "Mitochondrial phosphoenolpyruvate carboxykinase ({PEPCK-M}) is a
               pro-survival, endoplasmic reticulum ({ER}) stress response gene
               involved in tumor cell adaptation to nutrient availability",
  author    = "M{\'e}ndez-Lucas, Andr{\'e}s and Hyro{\v s}{\v s}ov{\'a}, Petra
               and Novellasdemunt, Laura and Vi{\~n}als, Francesc and Perales,
               Jose C",
  abstract  = "Mitochondrial phosphoenolpyruvate carboxykinase (PEPCK-M),
               encoded by the nuclear PCK2 gene, links TCA cycle intermediates
               and glycolytic pools through the conversion of mitochondrial
               oxaloacetate into phosphoenolpyruvate. In the liver PEPCK-M
               adjoins its profusely studied cytosolic isoform (PEPCK-C)
               potentiating gluconeogenesis and TCA flux. However, PEPCK-M is
               present in a variety of non-gluconeogenic tissues, including
               tumors of several origins. Despite its potential relevance to
               cancer metabolism, the mechanisms responsible for PCK2 gene
               regulation have not been elucidated. The present study
               demonstrates PEPCK-M overexpression in tumorigenic cells as well
               as the mechanism for the modulation of PCK2 abundance under
               several stress conditions. Amino acid limitation and ER stress
               inducers, conditions that activate the amino acid response (AAR)
               and the unfolded protein response (UPR), stimulate PCK2 gene
               transcription. Both the AAR and UPR lead to increased synthesis
               of ATF4, which mediates PCK2 transcriptional up-regulation
               through its binding to a putative ATF/CRE composite site within
               the PCK2 promoter functioning as an amino acid response element.
               In addition, activation of the GCN2-eIF2$\alpha$-ATF4 and
               PERK-eIF2$\alpha$-ATF4 signaling pathways are responsible for
               increased PEPCK-M levels. Finally, PEPCK-M knockdown using
               either siRNA or shRNA were sufficient to reduce MCF7 mammary
               carcinoma cell growth and increase cell death under glutamine
               deprivation or ER stress conditions. Our data demonstrate that
               this enzyme has a critical role in the survival program
               initiated upon stress and shed light on an unexpected and
               important role of mitochondrial PEPCK in cancer metabolism.",
  journal   = "J. Biol. Chem.",
  publisher = "Elsevier BV",
  volume    =  289,
  number    =  32,
  pages     = "22090--22102",
  month     =  aug,
  year      =  2014,
  keywords  = "AAR; AARE; ATF4; Cell Metabolism; ER Stress; PEPCK-M;
               Transcription Regulation; Tumor Metabolism; UPR; Unfolded
               Protein Response (UPR)",
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Hakimi2007-PEPCK_Mice,
  title     = "Overexpression of the cytosolic form of phosphoenolpyruvate
               carboxykinase ({GTP}) in skeletal muscle repatterns energy
               metabolism in the mouse",
  author    = "Hakimi, Parvin and Yang, Jianqi and Casadesus, Gemma and
               Massillon, Duna and Tolentino-Silva, Fatima and Nye, Colleen K
               and Cabrera, Marco E and Hagen, David R and Utter, Christopher B
               and Baghdy, Yacoub and Johnson, David H and Wilson, David L and
               Kirwan, John P and Kalhan, Satish C and Hanson, Richard W",
  abstract  = "Transgenic mice, containing a chimeric gene in which the cDNA
               for phosphoenolpyruvate carboxykinase (GTP) (PEPCK-C) (EC
               4.1.1.32) was linked to the alpha-skeletal actin gene promoter,
               express PEPCK-C in skeletal muscle (1-3 units/g). Breeding two
               founder lines together produced mice with an activity of PEPCK-C
               of 9 units/g of muscle (PEPCK-C(mus) mice). These mice were
               seven times more active in their cages than controls. On a mouse
               treadmill, PEPCK-C(mus) mice ran up to 6 km at a speed of 20
               m/min, whereas controls stopped at 0.2 km. PEPCK-C(mus) mice had
               an enhanced exercise capacity, with a VO(2max) of 156 +/- 8.0
               ml/kg/min, a maximal respiratory exchange ratio of 0.91 +/-
               0.03, and a blood lactate concentration of 3.7 +/- 1.0 mm after
               running for 32 min at a 25 degrees grade; the values for control
               animals were 112 +/- 21 ml/kg/min, 0.99 +/- 0.08, and 8.1 +/-
               5.0 mm respectively. The PEPCK-C(mus) mice ate 60\% more than
               controls but had half the body weight and 10\% the body fat as
               determined by magnetic resonance imaging. In addition, the
               number of mitochondria and the content of triglyceride in the
               skeletal muscle of PEPCK-C(mus) mice were greatly increased as
               compared with controls. PEPCK-C(mus) mice had an extended life
               span relative to control animals; mice up to an age of 2.5 years
               ran twice as fast as 6-12-month-old control animals. We conclude
               that overexpression of PEPCK-C repatterns energy metabolism and
               leads to greater longevity.",
  journal   = "J. Biol. Chem.",
  publisher = "Elsevier BV",
  volume    =  282,
  number    =  45,
  pages     = "32844--32855",
  month     =  nov,
  year      =  2007,
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Burgess2007-PEPCK_NotEasy,
  title     = "Cytosolic phosphoenolpyruvate carboxykinase does not solely
               control the rate of hepatic gluconeogenesis in the intact mouse
               liver",
  author    = "Burgess, Shawn C and He, Tianteng and Yan, Zheng and Lindner,
               Jill and Sherry, A Dean and Malloy, Craig R and Browning,
               Jeffrey D and Magnuson, Mark A",
  abstract  = "When dietary carbohydrate is unavailable, glucose required to
               support metabolism in vital tissues is generated via
               gluconeogenesis in the liver. Expression of phosphoenolpyruvate
               carboxykinase (PEPCK), commonly considered the control point for
               liver gluconeogenesis, is normally regulated by circulating
               hormones to match systemic glucose demand. However, this
               regulation fails in diabetes. Because other molecular and
               metabolic factors can also influence gluconeogenesis, the
               explicit role of PEPCK protein content in the control of
               gluconeogenesis was unclear. In this study, metabolic control of
               liver gluconeogenesis was quantified in groups of mice with
               varying PEPCK protein content. Surprisingly, livers with a 90\%
               reduction in PEPCK content showed only a approximately 40\%
               reduction in gluconeogenic flux, indicating a lower than
               expected capacity for PEPCK protein content to control
               gluconeogenesis. However, PEPCK flux correlated tightly with TCA
               cycle activity, suggesting that under some conditions in mice,
               PEPCK expression must coordinate with hepatic energy metabolism
               to control gluconeogenesis.",
  journal   = "Cell Metab.",
  publisher = "Elsevier BV",
  volume    =  5,
  number    =  4,
  pages     = "313--320",
  month     =  apr,
  year      =  2007,
  copyright = "https://www.elsevier.com/open-access/userlicense/1.0/",
  language  = "en"
}


@incollection{TAKAHASHI2017313_ACU,
title = {Chapter 13 - Animal Models of Liver Diseases},
editor = {P. Michael Conn},
booktitle = {Animal Models for the Study of Human Disease (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {313-339},
year = {2017},
isbn = {978-0-12-809468-6},
doi = {https://doi.org/10.1016/B978-0-12-809468-6.00013-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128094686000139},
author = {Yoshihisa Takahashi and Toshio Fukusato},
keywords = {liver disease, animal model, mouse, rat, transgenic mouse, diet},
abstract = {Animal models are indispensable for the elucidation of pathogenesis mechanisms,
the identification of potential therapeutic targets, and the development of novel therapies for various
liver diseases. Although large animals, including nonhuman primates, have occasionally been used, rodents
are used most frequently. In 2010, a gold standard publication checklist (GSPC) for animal studies was
 presented, with which all future studies should comply. Studies using animal models have various advantages
  over clinical or in vitro studies. However, animal experiments also have some limitations. As currently
  used animal models only partially represent the characteristics of human diseases, it is important
  to select an animal model that is suitable for the objective of the study. In this chapter, we present
  a systematic review of currently used animal models of various liver diseases, along with their advantages
  and disadvantages.}
}


@article{Zhang2020UHDQuantileMiceGene,
  author  = {Yuankun Zhang and Heng Lian and Yan Yu},
  title   = {Ultra-High Dimensional Single-Index Quantile Regression},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {224},
  pages   = {1--25},
  url     = {http://jmlr.org/papers/v21/19-173.html}
}

@article{Song2015HDL1VarSelMiceGene,
author = {Qifan Song and Faming Liang},
title = {High-Dimensional Variable Selection With Reciprocal L1-Regularization},
journal = {Journal of the American Statistical Association},
volume = {110},
number = {512},
pages = {1607-1620},
year  = {2015},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1080/01621459.2014.984812},

URL = {
        https://doi.org/10.1080/01621459.2014.984812

},
eprint = {
        https://doi.org/10.1080/01621459.2014.984812

}

}

@article{ScheetzEtAl2006RatEye,
author = {Todd E. Scheetz  and Kwang-Youn A. Kim  and Ruth E. Swiderski  and Alisdair R. Philp  and Terry A. Braun  and Kevin L. Knudtson  and Anne M. Dorrance  and Gerald F. DiBona  and Jian Huang  and Thomas L. Casavant  and Val C. Sheffield  and Edwin M. Stone },
title = {Regulation of gene expression in the mammalian eye and its relevance to eye disease},
journal = {Proceedings of the National Academy of Sciences},
volume = {103},
number = {39},
pages = {14429-14434},
year = {2006},
doi = {https://doi.org/10.1073/pnas.0602562103},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0602562103},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0602562103}
}

@article{Trim32ScheetzRatEye,
author = {Annie P. Chiang  and John S. Beck  and Hsan-Jan Yen  and Marwan K. Tayeh  and Todd E. Scheetz  and Ruth E. Swiderski  and Darryl Y. Nishimura  and Terry A. Braun  and Kwang-Youn A. Kim  and Jian Huang  and Khalil Elbedour  and Rivka Carmi  and Diane C. Slusarski  and Thomas L. Casavant  and Edwin M. Stone  and Val C. Sheffield },
title = {Homozygosity mapping with SNP arrays identifies TRIM32, an E3 ubiquitin ligase, as a Bardet-Biedl syndrome gene (BBS11)},
journal = {Proceedings of the National Academy of Sciences},
volume = {103},
number = {16},
pages = {6287-6292},
year = {2006},
doi = {https://doi.org/10.1073/pnas.0600158103},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0600158103},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0600158103},
abstract = {The identification of mutations in genes that cause human diseases
has largely been accomplished through the use of positional cloning, which relies on
linkage mapping. In studies of rare diseases, the resolution of linkage mapping is limited by the
number of available meioses and informative marker density. One recent advance is the development of
high-density SNP microarrays for genotyping. The SNP arrays overcome low marker informativity by using a
large number of markers to achieve greater coverage at finer resolution. We used SNP microarray genotyping
for homozygosity mapping in a small consanguineous Israeli Bedouin family with autosomal recessive
Bardet–Biedl syndrome (BBS; obesity, pigmentary retinopathy, polydactyly, hypogonadism, renal and cardiac
abnormalities, and cognitive impairment) in which previous linkage studies using short tandem repeat
polymorphisms failed to identify a disease locus. SNP genotyping revealed a homozygous candidate region.
Mutation analysis in the region of homozygosity identified a conserved homozygous missense mutation in the
TRIM32 gene, a gene coding for an E3 ubiquitin ligase. Functional analysis of this gene in zebrafish
and expression correlation analyses among other BBS genes in an expression quantitative trait loci data
set demonstrate that TRIM32 is a BBS gene. This study shows the value of high-density SNP genotyping for
homozygosity mapping and the use of expression correlation data for evaluation of candidate genes and
identifies the proteasome degradation pathway as a pathway involved in BBS.}
}

@article{Huang2006AdLassoSparseRatEye,
author = {Huang, Jian and Ma, Shuangge and Zhang, Cun-Hui},
year = {2006},
month = {12},
pages = {},
title = {Adaptive LASSO for sparse high-dimensional regression},
volume = {18},
journal = {Statistica Sinica}
}

@article{FanLi2001SCAD,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/3085904},
 abstract = {Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coefficients simultaneously. Hence they enable us to construct confidence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on (0, ∞), and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications.},
 author = {Jianqing Fan and Runze Li},
 journal = {Journal of the American Statistical Association},
 number = {456},
 pages = {1348--1360},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties},
 urldate = {2022-09-14},
 volume = {96},
 year = {2001}
}

@article{Zou2006AdLASSO,
author = {Hui Zou},
title = {The Adaptive Lasso and Its Oracle Properties},
journal = {Journal of the American Statistical Association},
volume = {101},
number = {476},
pages = {1418-1429},
year  = {2006},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1198/016214506000000735},

URL = {
        https://doi.org/10.1198/016214506000000735

},
eprint = {
        https://doi.org/10.1198/016214506000000735

}

}

@article{Efron2004LARS,
	doi = {https://doi.org/10.1214/009053604000000067},

	url = {https://doi.org/10.1214%2F009053604000000067},

	year = {2004},
	month = {apr},

	publisher = {Institute of Mathematical Statistics},

	volume = {32},

	number = {2},

	author = {Bradley Efron and Trevor Hastie and Iain Johnstone and Robert Tibshirani},

	title = {Least angle regression},

	journal = {The Annals of Statistics}
}

@book{Hastie2015STwSparsityBook,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
title = {Statistical Learning with Sparsity: The Lasso and Generalizations},
year = {2015},
isbn = {1498712169},
publisher = {Chapman \& Hall/CRC},
abstract = {Discover New Methods for Dealing with High-Dimensional Data A sparse statistical model
has only a small number of nonzero parameters or weights; therefore, it is much easier to estimate
and interpret than a dense model. Statistical Learning with Sparsity: The Lasso and Generalizations
presents methods that exploit sparsity to help recover the underlying signal in a set of data. Top
experts in this rapidly evolving field, the authors describe the lasso for linear regression and a
simple coordinate descent algorithm for its computation. They discuss the application of 1 penalties
to generalized linear models and support vector machines, cover generalized penalties such as the
elastic net and group lasso, and review numerical methods for optimization. They also present
statistical inference methods for fitted (lasso) models, including the bootstrap, Bayesian methods,
and recently developed approaches. In addition, the book examines matrix decomposition,
sparse multivariate analysis, graphical models, and compressed sensing.
It concludes with a survey of theoretical results for the lasso. In this age of big data,
the number of features measured on a person or object can be large and might be larger than
the number of observations. This book shows how the sparsity assumption allows us to tackle these
problems and extract useful and reproducible patterns from big datasets. Data analysts,
computer scientists, and theorists will appreciate this thorough and up-to-date treatment of
sparse statistical modeling.}
}

@article{Kobak2020RidgeinHD,
author = {Kobak, Dmitry and Lomond, Jonathan and Sanchez, Benoit},
title = {The Optimal Ridge Penalty for Real-World High-Dimensional Data Can Be Zero or Negative
Due to the Implicit Ridge Regularization},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {A conventional wisdom in statistical learning is that large models require strong
regularization to prevent overfitting. Here we show that this rule can be violated by linear
regression in the underdetermined n ≪ p situation under realistic conditions. Using simulations
and real-life high-dimensional datasets, we demonstrate that an explicit positive ridge penalty
can fail to provide any improvement over the minimum-norm least squares estimator. Moreover,
the optimal value of ridge penalty in this situation can be negative. This happens when the
high-variance directions in the predictor space can predict the response variable, which is often
the case in the real-world high-dimensional data. In this regime, low-variance directions provide an
implicit ridge regularization and can make any further positive ridge penalty detrimental.
We prove that augmenting any linear model with random covariates and using minimum-norm estimator
is asymptotically equivalent to adding the ridge penalty. We use a spiked covariance model as an
analytically tractable example and prove that the optimal ridge penalty in this case is negative
when n ≪ p.},
journal = {J. Mach. Learn. Res.},
month = {1},
articleno = {169},
numpages = {16},
keywords = {ridge regression, regularization, high-dimensional}
}

@article{Alfons2013sparseLTS,
author = {Andreas Alfons and Christophe Croux and Sarah Gelper},
title = {{Sparse least trimmed squares regression for analyzing high-dimensional large data sets}},
volume = {7},
journal = {The Annals of Applied Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {226 -- 248},
keywords = {Breakdown point, Outliers, penalized regression, robust regression, Trimming},
year = {2013},
doi = {https://doi.org/10.1214/12-AOAS575},
URL = {https://doi.org/10.1214/12-AOAS575}
}

@article{Tibsh1996LASSO,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes
 the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant.
 Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives
 interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression.
 It produces interpretable models like subset selection and exhibits the stability of ridge regression.
 There is also an interesting relationship with recent work in adaptive function estimation by Donoho and
 Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models:
 extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2022-09-29},
 volume = {58},
 year = {1996}
}



@article{WatsonHolmes2016AppModRobDec,
 ISSN = {08834237, 21688745},
 URL = {http://www.jstor.org/stable/26408074},
 abstract = {Decisions based partly or solely on predictions from probabilistic models may be sensitive
 to model misspecification. Statisticians are taught from an early stage that "all models are wrong, but
 some are useful"; however, little formal guidance exists on how to assess the impact of model approximation
 on decision making, or how to proceed when optimal actions appear sensitive to model fidelity.
 This article presents an overview of recent developments across different disciplines to address this.
  We review diagnostic techniques, including graphical approaches and summary statistics, to help highlight
  decisions made through minimised expected loss that are sensitive to model misspecification.
  We then consider formal methods for decision making under model misspecification by quantifying
  stability of optimal actions to perturbations to the model within a neighbourhood of model space.
  This neighbourhood is defined in either one of two ways. First, in a strong sense via an information
  (Kullback–Leibler) divergence around the approximating model. Second, using a Bayesian nonparametric model
  (prior) centred on the approximating model, in order to "average out" over possible misspecifications.
   This is presented in the context of recent work in the robust control, macroeconomics and financial
   mathematics literature. We adopt a Bayesian approach throughout although the presentation is agnostic
   to this position.},
 author = {James Watson and Chris Holmes},
 journal = {Statistical Science},
 number = {4},
 pages = {465--489},
 publisher = {Institute of Mathematical Statistics},
 title = {Approximate Models and Robust Decisions},
 urldate = {2022-10-20},
 volume = {31},
 year = {2016}
}

@article{Hoeting1999BayModAvgTutorial,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/2676803},
 abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select
  a model from some class of models and then proceed as if the selected model had generated the data.
  This approach ignores the uncertainty in model selection, leading to over-confident inferences and
   decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a
   coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA
   have recently emerged. We discuss these methods and present a number of examples. In these examples,
   BMA provides improved out-of-sample predictive performance. We also provide a catalogue of currently
   available BMA software.},
 author = {Jennifer A. Hoeting and David Madigan and Adrian E. Raftery and Chris T. Volinsky},
 journal = {Statistical Science},
 number = {4},
 pages = {382--401},
 publisher = {Institute of Mathematical Statistics},
 title = {Bayesian Model Averaging: A Tutorial},
 urldate = {2022-11-03},
 volume = {14},
 year = {1999}
}

@article{Raftery1997BayModAvgLinReg,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2291462},
 abstract = {We consider the problem of accounting for model uncertainty in linear regression models.
  Conditioning on a single selected model ignores model uncertainty, and thus leads to the underestimation
   of uncertainty when making inferences about quantities of interest. A Bayesian solution to this problem
   involves averaging over all possible models (i.e., combinations of predictors) when making inferences
   about quantities of interest. This approach is often not practical. In this article we offer
   two alternative approaches. First, we describe an ad hoc procedure, "Occam's window,"
   which indicates a small set of models over which a model average can be computed.
   Second, we describe a Markov chain Monte Carlo approach that directly approximates the exact solution.
   In the presence of model uncertainty, both of these model averaging procedures provide better predictive
   performance than any single model that might reasonably have been selected. In the extreme case where
   there are many candidate predictors but no relationship between any of them and the response,
   standard variable selection procedures often choose some subset of variables that yields a high R2
    and a highly significant overall F value. In this situation, Occam's window usually indicates the
    null model (or a small number of models including the null model) as the only one (or ones) to be
    considered thus largely resolving the problem of selecting significant models when there is no signal
     in the data. Software to implement our methods is available from StatLib.},
 author = {Adrian E. Raftery and David Madigan and Jennifer A. Hoeting},
 journal = {Journal of the American Statistical Association},
 number = {437},
 pages = {179--191},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Bayesian Model Averaging for Linear Regression Models},
 urldate = {2022-11-03},
 volume = {92},
 year = {1997}
}

@article{Filzmoser2020RobLinRegHDOverview,
author = {Filzmoser, Peter and Nordhausen, Klaus},
title = {Robust linear regression for high-dimensional data: An overview},
journal = {WIREs Computational Statistics},
volume = {13},
number = {4},
pages = {e1524},
keywords = {dimension reduction, high-dimensional data, Outlier, regression, sparsity},
doi = {https://doi.org/10.1002/wics.1524},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1524},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1524},
abstract = {Abstract Digitization as the process of converting information into numbers leads to bigger
and more complex data sets, bigger also with respect to the number of measured variables.
This makes it harder or impossible for the practitioner to identify outliers or observations that are
inconsistent with an underlying model. Classical least-squares based procedures can be affected by those
outliers. In the regression context, this means that the parameter estimates are biased, with consequences
on the validity of the statistical inference, on regression diagnostics, and on the prediction accuracy.
Robust regression methods aim at assigning appropriate weights to observations that deviate from the model.
While robust regression techniques are widely known in the low-dimensional case, researchers and
practitioners might still not be very familiar with developments in this direction for high-dimensional data.
 Recently, different strategies have been proposed for robust regression in the high-dimensional case,
 typically based on dimension reduction, on shrinkage, including sparsity, and on combinations of such
 techniques. A very recent concept is downweighting single cells of the data matrix rather than complete
 observations, with the goal to make better use of the model-consistent information, and thus to
 achieve higher efficiency of the parameter estimates. This article is categorized under:
 Statistical and Graphical Methods of Data Analysis > Robust Methods Statistical and Graphical Methods of
  Data Analysis > Analysis of High Dimensional Data Statistical and Graphical Methods of Data Analysis >
  Dimension Reduction},
year = {2021}
}

@misc{Gagnon2022StudenttProperties,
  doi = {https://doi.org/10.48550/ARXIV.2204.02299},

  url = {https://arxiv.org/abs/2204.02299},

  author = {Gagnon, Philippe and Hayashi, Yoshiko},

  keywords = {Statistics Theory (math.ST), Methodology (stat.ME), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Theoretical properties of Bayesian Student-$t$ linear regression},

  publisher = {arXiv},

  year = {2022},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Vehtari2016LOO,
	doi = {https://doi.org/10.1007/s11222-016-9696-4},

	url = {https://doi.org/10.1007%2Fs11222-016-9696-4},

	year = {2016},
	month = {8},

	publisher = {Springer Science and Business Media {LLC}
},

	volume = {27},

	number = {5},

	pages = {1413--1432},

	author = {Aki Vehtari and Andrew Gelman and Jonah Gabry},

	title = {Practical Bayesian model evaluation using leave-one-out cross-validation and {WAIC}},

	journal = {Statistics and Computing}
}


@article{Wang2015HOLP,
author = {Wang, Xiangyu and Leng, Chenlei},
year = {2016},
month = {06},
pages = {589–611},
title = {High-dimensional Ordinary Least-squares Projection for Screening Variables},
volume = {78},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
doi = {10.1111/rssb.12127}
}

@misc{Wessel2015Ridge,
  doi = {https://doi.org/10.48550/ARXIV.1509.09169},

  url = {https://arxiv.org/abs/1509.09169},

  author = {van Wieringen, Wessel N.},

  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Lecture notes on ridge regression},

  publisher = {arXiv},

  year = {2015},

  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@article{Jia2015LASSOPrecond,
author = {Jinzhu Jia and Karl Rohe},
title = {{Preconditioning the Lasso for sign consistency}},
volume = {9},
journal = {Electronic Journal of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {1150 -- 1172},
keywords = {Irrepresentable Condition, preconditioning, sign consistency},
year = {2015},
doi = {https://doi.org/10.1214/15-EJS1029},
URL = {https://doi.org/10.1214/15-EJS1029}
}

@article{Buehlmann2014Riboflavin,
author = {B\"{u}hlmann, Peter and Kalisch, Markus and Meier, Lukas},
title = {High-Dimensional Statistics with a View Toward Applications in Biology},
journal = {Annual Review of Statistics and Its Application},
volume = {1},
number = {1},
pages = {255-278},
year = {2014},
doi = {https://doi.org/10.1146/annurev-statistics-022513-115545},

URL = {

        https://doi.org/10.1146/annurev-statistics-022513-115545



},
eprint = {

        https://doi.org/10.1146/annurev-statistics-022513-115545



}
,
    abstract = { We review statistical methods for high-dimensional data analysis and pay particular attention to recent developments for assessing uncertainties in terms of controlling false positive statements (type I error) and p-values. The main focus is on regression models, but we also discuss graphical modeling and causal inference based on observational data. We illustrate the concepts and methods with various packages from the statistical software using a high-throughput genomic data set about riboflavin production with Bacillus subtilis, which we make publicly available for the first time. }
}


@article{Christidis2019SplitReg,
author = {Anthony-Alexander Christidis and Laks Lakshmanan and Ezequiel Smucler and Ruben Zamar},
title = {Split Regularized Regression},
journal = {Technometrics},
volume = {62},
number = {3},
pages = {330-338},
year  = {2020},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1080/00401706.2019.1635533},
URL = {
        https://doi.org/10.1080/00401706.2019.1635533
},
eprint = {
        https://doi.org/10.1080/00401706.2019.1635533
}
}


@misc{Christidis2018Modeling,
  doi = {https://doi.org/10.48550/ARXIV.1812.05678},

  url = {https://arxiv.org/abs/1812.05678},

  author = {Christidis, Anthony and Van Aelst, Stefan and Zamar, Ruben},

  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Split Regression Modeling},

  publisher = {arXiv},

  year = {2018},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{christidis2022multimodel,
      title={Multi-Model Ensemble Optimization},
      author={Anthony-Alexander Christidis and Stefan Van Aelst and Ruben Zamar},
      year={2022},
      eprint={2204.08100},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{Fan2010OvVarSelHD,
author = {Fan, Jianqing and Lv, Jinchi},
year = {2010},
month = {01},
pages = {101-148},
title = {A Selective Overview of Variable Selection in High Dimensional Feature Space},
volume = {20},
journal = {Statistica Sinica}
}

@book{wainwright2019HDS_nonasy,
place={Cambridge},
series={Cambridge Series in Statistical and Probabilistic Mathematics},
title={High-Dimensional Statistics: A Non-Asymptotic Viewpoint},
DOI={10.1017/9781108627771},
publisher={Cambridge University Press},
author={Wainwright, Martin J.},
year={2019},
collection={Cambridge Series in Statistical and Probabilistic Mathematics}
}

@misc{SilinFan2020HDlinRegArxiv,
  doi = {https://doi.org/10.48550/ARXIV.2007.12313},

  url = {https://arxiv.org/abs/2007.12313},

  author = {Silin, Igor and Fan, Jianqing},

  keywords = {Statistics Theory (math.ST), Methodology (stat.ME), FOS: Mathematics, FOS: Mathematics,
   FOS: Computer and information sciences, FOS: Computer and information sciences, 62J05 (primary),
   62H12, 62H25 (secondary)},

  title = {Canonical thresholding for non-sparse high-dimensional linear regression},

  publisher = {arXiv},

  year = {2020},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{SilinFan2022HDlinReg,
author = {Igor Silin and Jianqing Fan},
title = {{Canonical thresholding for nonsparse high-dimensional linear regression}},
volume = {50},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {460 -- 486},
keywords = {covariance eigenvalues decay, high-dimensional linear regression, principal component regression, relative errors, thresholding},
year = {2022},
doi = {https://doi.org/10.1214/21-AOS2116},
URL = {https://doi.org/10.1214/21-AOS2116}
}


@article{Wang2009_extrCor_SuperMData,
author = {Hansheng Wang},
title = {Forward Regression for Ultra-High Dimensional Variable Screening},
journal = {Journal of the American Statistical Association},
volume = {104},
number = {488},
pages = {1512-1524},
year  = {2009},
publisher = {Taylor & Francis},
doi = {https://doi.org/10.1198/jasa.2008.tm08516},
URL = {
        https://doi.org/10.1198/jasa.2008.tm08516
},
eprint = {
        https://doi.org/10.1198/jasa.2008.tm08516
  }
}

@Manual{RLanguage,
    title = {\proglang{R}: A Language and Environment for Statistical Computing},
    author = {{\proglang{R} Core Team}},
    organization = {\proglang{R} Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2024},
    url = {https://www.R-project.org/},
  }

  @article{BOTTMER2022SparseReg,
title = {Sparse regression for large data sets with outliers},
journal = {European Journal of Operational Research},
volume = {297},
number = {2},
pages = {782-794},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2021.05.049},
url = {https://www.sciencedirect.com/science/article/pii/S037722172100477X},
author = {Lea Bottmer and Christophe Croux and Ines Wilms},
keywords = {Data science, Lasso, Outliers, Robust regression, Variable selection},
abstract = {The linear regression model remains an important workhorse for data scientists. However, many data sets contain many more predictors than observations. Besides, outliers, or anomalies, frequently occur.
This paper proposes an algorithm for regression analysis that addresses these features typical
 for big data sets, which we call “sparse shooting S”. The resulting regression coefficients are sparse, meaning that many of them are set to zero, hereby selecting the most relevant predictors. A distinct feature
 of the method is its robustness with respect to outliers in the cells of the data matrix. The excellent performance of this robust variable selection and prediction method is shown in a simulation study.
 A real data application on car fuel consumption demonstrates its usefulness.}
}

@article{li2007nonlinear,
  title={Nonlinear estimators and tail bounds for dimension reduction in {L}1 using {C}auchy random projections},
  author={Li, Ping and Hastie, Trevor J and Church, Kenneth W},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={Oct},
  url = {https://jmlr.csail.mit.edu/papers/v8/li07b.html},
  pages={2497--2532},
  year={2007}
}

@inproceedings{Clarkson2013LowRankApprox,
author = {Clarkson, Kenneth L. and Woodruff, David P.},
title = {Low Rank Approximation and Regression in Input Sparsity Time},
year = {2013},
isbn = {9781450320290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2488608.2488620},
  booktitle = {Proceedings of the Forty-Fifth Annual ACM Symposium on Theory of Computing},
  pages = {81–90},
  numpages = {10},
  keywords = {regression, low-rank approximation, leverage scores, sketching, randomized},
  series = {STOC '13}
  }

@inproceedings{wojnowicz2016projecting,
  title={Projecting" better than randomly": How to reduce the dimensionality of very large datasets in a way that outperforms random projections},
  author={Wojnowicz, Michael and Zhang, Di and Chisholm, Glenn and Zhao, Xuan and Wolff, Matt},
  booktitle={2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
  pages={184--193},
  year={2016},
  organization={IEEE}
}

@article{Fan2011UHDVarEst,
    author = {Fan, Jianqing and Guo, Shaojun and Hao, Ning},
    title = {Variance Estimation Using Refitted Cross-Validation in Ultrahigh Dimensional Regression},
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {74},
    number = {1},
    pages = {37-65},
    year = {2011},
    month = {10},
    abstract = {{Variance estimation is a fundamental problem in statistical modelling. In ultrahigh dimensional linear regression where the dimensionality is much larger than the sample size, traditional variance estimation techniques are not applicable. Recent advances in variable selection in ultrahigh dimensional linear regression make this problem accessible. One of the major problems in ultrahigh dimensional regression is the high spurious correlation between the unobserved realized noise and some of the predictors. As a result, the realized noises are actually predicted when extra irrelevant variables are selected, leading to a serious underestimate of the level of noise. We propose a two-stage refitted procedure via a data splitting technique, called refitted cross-validation, to attenuate the influence of irrelevant variables with high spurious correlations. Our asymptotic results show that the resulting procedure performs as well as the oracle estimator, which knows in advance the mean regression function. The simulation studies lend further support to our theoretical claims. The naive two-stage estimator and the plug-in one-stage estimators using the lasso and smoothly clipped absolute deviation are also studied and compared. Their performances can be improved by the refitted cross-validation method proposed.}},
    issn = {1369-7412},
    doi = {https://doi.org/10.1111/j.1467-9868.2011.01005.x},
    url = {https://doi.org/10.1111/j.1467-9868.2011.01005.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/74/1/37/49509126/jrsssb\_74\_1\_37.pdf},
}

@article{Lassance2023ShrinkageLinReg,
title = {An analytical shrinkage estimator for linear regression},
journal = {Statistics & Probability Letters},
volume = {194},
pages = {109760},
year = {2023},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2022.109760},
url = {https://www.sciencedirect.com/science/article/pii/S0167715222002735},
author = {Nathan Lassance},
keywords = {Linear regression, Prediction error, Shrinkage, Out-of-sample},
abstract = {We derive an analytical solution to the optimal shrinkage of OLS regression coefficients toward a constant target, under any first two moments of predictors. The estimator closely mimics the prediction performance of ridge penalty, which admits no general analytical solution.}
}


@inproceedings{Wang2015ConsistencyHOLP,
author = {Wang, Xiangyu and Leng, Chenlei and Dunson, David B.},
title = {On the Consistency Theory of High Dimensional Variable Screening},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2431–2439},
numpages = {9},
series = {NIPS'15}
}

@article{Liang2023ENSURE,
  author   = {Zhu, Rong and Liang, Hua and Ruppert, David},
  title    = {Ensemble Subset Regression (ENSURE): Efficient High-dimensional Prediction},
  journal  = {Statistica Sinica},
  year     = {2023},
  doi = {https://doi.org/10.5705/ss.202021.0187}
}

@article{Cook2019PLSPredinHDR,
author = {Cook, R. and Forzani, Liliana},
year = {2019},
month = {04},
pages = {884-908},
title = {Partial least squares prediction in high-dimensional regression},
volume = {47},
journal = {Annals of Statistics},
doi = {https://doi.org/10.1214/18-AOS1681}
}

@article{Hoerl1970Ridge,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1267351},
 author = {Hoerl, Arthur E. and  Kennard, Robert W.},
 journal = {Technometrics},
 number = {1},
 pages = {55--67},
 publisher = {Taylor & Francis Ltd., American Statistical Association American Society for Quality},
 title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
 urldate = {2023-04-03},
 volume = {12},
 year = {1970}
}

@article{Zou2005ElasticNet,
author = {Zou, Hui and Hastie, Trevor},
title = {Regularization and variable selection via the elastic net},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {67},
number = {2},
pages = {301-320},
keywords = {Grouping effect, LARS algorithm, Lasso, Penalization, p≫n problem, Variable selection},
doi = {https://doi.org/10.1111/j.1467-9868.2005.00503.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00503.x},
abstract = {Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
year = {2005}
}


@inproceedings{Zhou2007CompressedRegression,
 author = {Zhou, Shuheng and Wasserman, Larry and Lafferty, John},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 publisher = {Curran Associates, Inc.},
 title = {Compressed Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf},
 volume = {20},
 year = {2007}
}

@book{Fan2020StatFoundofDataScience,
author = {Fan, Jianqing and Li, Runze and Zhang, Cun-Hui and Zou, Hui},
year = {2020},
month = {09},
pages = {},
title = {Statistical Foundations of Data Science},
isbn = {9780429096280},
doi = {https://doi.org/10.1201/9780429096280}
}

@article{Zhao2006LASSOModSel,
  author  = {Peng Zhao and Bin Yu},
  title   = {On Model Selection Consistency of Lasso},
  journal = {Journal of Machine Learning Research},
  year    = {2006},
  volume  = {7},
  number  = {90},
  pages   = {2541--2563},
  url     = {http://jmlr.org/papers/v7/zhao06a.html}
}

@article{Hazimeh2020FastBSSL0,
author = {Hazimeh, Hussein and Mazumder, Rahul},
title = {Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms},
journal = {Operations Research},
volume = {68},
number = {5},
pages = {1517-1537},
year = {2020},
doi = {https://doi.org/10.1287/opre.2019.1919},
URL = {https://doi.org/10.1287/opre.2019.1919},
eprint = {https://doi.org/10.1287/opre.2019.1919},
abstract = { In several scientific and industrial applications, it is desirable to build compact, interpretable learning
models where the output depends on a small number of input features. Recent work has shown that such best-subset
selection-type problems can be solved with modern mixed integer optimization solvers. Despite their promise, such solvers
often come at a steep computational price when compared with open-source, efficient specialized solvers based on convex
optimization and greedy heuristics. In “Fast Best-Subset Selection: Coordinate Descent and Local Combinatorial Optimization
 Algorithms,” Hussein Hazimeh and Rahul Mazumder push the frontiers of computation for best-subset-type problems.
 Their algorithms deliver near-optimal solutions for problems with up to a million features—in times comparable with the
 fast convex solvers. Their work suggests that principled optimization methods play a key role in devising tools central
 to interpretable machine learning, which can help in gaining a deeper understanding of their statistical properties. }
}


@article{Fan2008HDClasFAIR,
author = {Jianqing Fan and Yingying Fan},
title = {{High-dimensional classification using features annealed independence rules}},
volume = {36},
journal = {The Annals of Statistics},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {2605 -- 2637},
keywords = {classification, feature extraction, high dimensionality, independence rule, misclassification rates},
year = {2008},
doi = {https://doi.org/10.1214/07-AOS504},
URL = {https://doi.org/10.1214/07-AOS504}
}
@misc{wang2016LSinHD,
      title={No penalty no tears: Least squares in high-dimensional linear models},
      author={Xiangyu Wang and David Dunson and Chenlei Leng},
      year={2016},
      eprint={1506.02222},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{BurnhamAnderson2004MMIAIC,
author = {Kenneth P. Burnham and David R. Anderson},
title ={Multimodel Inference: Understanding AIC and BIC in Model Selection},
journal = {Sociological Methods \& Research},
volume = {33},
number = {2},
pages = {261-304},
year = {2004},
doi = {https://doi.org/10.1177/0049124104268644},

URL = {
        https://doi.org/10.1177/0049124104268644

},
eprint = {
        https://doi.org/10.1177/0049124104268644

}
,
    abstract = { The model selection literature has been generally poor at reflecting the deep foundations of the Akaike information criterion (AIC) and at making appropriate comparisons to the Bayesian information criterion (BIC). There is a clear philosophy, a sound criterion based in information theory, and a rigorous statistical foundation for AIC. AIC can be justified as Bayesian using a “savvy” prior on models that is a function of sample size and the number of model parameters. Furthermore, BIC can be derived as a non-Bayesian result. Therefore, arguments about using AIC versus BIC for model selection cannot be from a Bayes versus frequentist perspective. The philosophical context of what is assumed about reality, approximating models, and the intent of model-based inference should determine whether AIC or BIC is used. Various facets of such multimodel inference are presented here, particularly methods of model averaging. }
}

@article{CLAESKENS2016ForecastCombPuzzle,
title = {The forecast combination puzzle: A simple theoretical explanation},
journal = {International Journal of Forecasting},
volume = {32},
number = {3},
pages = {754-762},
year = {2016},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2015.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0169207016000327},
author = {Gerda Claeskens and Jan R. Magnus and Andrey L. Vasnev and Wendun Wang},
keywords = {Forecast combination, Optimal weights},
abstract = {This paper offers a theoretical explanation for the stylized fact that forecast combinations with estimated optimal weights often perform poorly in applications. The properties of the forecast combination are typically derived under the assumption that the weights are fixed, while in practice they need to be estimated. If the fact that the weights are random rather than fixed is taken into account during the optimality derivation, then the forecast combination will be biased (even when the original forecasts are unbiased), and its variance will be larger than in the fixed-weight case. In particular, there is no guarantee that the ‘optimal’ forecast combination will be better than the equal-weight case, or even improve on the original forecasts. We provide the underlying theory, some special cases, and a numerical illustration.}
}


@misc{fan2022bridging,
      title={Bridging factor and sparse models},
      author={Jianqing Fan and Ricardo Masini and Marcelo C. Medeiros},
      year={2022},
      eprint={2102.11341},
      archivePrefix={arXiv},
      primaryClass={econ.EM}
}

@article{Zhang2017RegressionPhalanxes,
  title={Regression Phalanxes},
  author={Hongyang Zhang and William J. Welch and Ruben H. Zamar},
  journal={arXiv: Machine Learning},
  year={2017}
}


@article{meinshausen2009stabilitySelection,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/40802220},
 abstract = {Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.},
 author = {Nicolai Meinshausen and Peter Bühlmann},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {4},
 pages = {417--473},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Stability selection},
 urldate = {2023-09-05},
 volume = {72},
 year = {2010}
}



@article{barber2018knockoffFilter,
author = {Rina Foygel Barber and Emmanuel J. Candes},
title = {{A knockoff filter for high-dimensional selective inference}},
volume = {47},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {2504 -- 2537},
keywords = {false discovery rate (FDR), high-dimensional regression, Knockoffs, Variable selection},
year = {2019},
doi = {https://doi.org/10.1214/18-AOS1755},
URL = {https://doi.org/10.1214/18-AOS1755}
}


@article{Reeve2018DivAndDFinRegEnsembles,
title = {Diversity and degrees of freedom in regression ensembles},
journal = {Neurocomputing},
volume = {298},
pages = {55-68},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.12.066},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218302133},
author = {Henry WJ Reeve and Gavin Brown},
keywords = {Degrees of freedom, Negative Correlation Learning, Tikhonov regularisation, Ensembles, Stein’s unbiased risk estimate, Deep neural networks},
abstract = {Ensemble methods are a cornerstone of modern machine learning. The performance of an ensemble depends crucially upon the level of diversity between its constituent learners. This paper establishes a connection between diversity and degrees of freedom (i.e. the capacity of the model), showing that diversity may be viewed as a form of inverse regularisation. This is achieved by focusing on a previously published algorithm Negative Correlation Learning (NCL), in which model diversity is explicitly encouraged through a diversity penalty term in the loss function. We provide an exact formula for the effective degrees of freedom in an NCL ensemble with fixed basis functions, showing that it is a continuous, convex and monotonically increasing function of the diversity parameter. We demonstrate a connection to Tikhonov regularisation and show that, with an appropriately chosen diversity parameter, an NCL ensemble can always outperform the unregularised ensemble in the presence of noise. We demonstrate the practical utility of our approach by deriving a method to efficiently tune the diversity parameter. Finally, we use a Monte-Carlo estimator to extend the connection between diversity and degrees of freedom to ensembles of deep neural networks.}
}

@article{CribariNeto2000InvMomentsBinom,
title = {A Note on Inverse Moments of Binomial Variates},
author = {Cribari-Neto, Francisco and Garcia, Nancy Lopes and Vasconcellos, Klaus L. P.},
year = {2000},
journal = {Brazilian Review of Econometrics},
volume = {20},
number = {2},
url = {https://EconPapers.repec.org/RePEc:sbe:breart:v:20:y:2000:i:2:a:2760}
}

@inproceedings{Maillard2009CompressedLS,
 author = {Maillard, Odalric and Munos, R\'{e}mi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Compressed Least-Squares Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2009/file/01882513d5fa7c329e940dda99b12147-Paper.pdf},
 volume = {22},
 year = {2009}
}



@article{JohnsonLindenstrauss1984,
author = {Johnson, William and Lindenstrauss, Joram},
year = {1984},
month = {01},
pages = {189-206},
title = {Extensions of Lipschitz Maps into a Hilbert Space},
volume = {26},
isbn = {9780821850305},
journal = {Contemporary Mathematics},
doi = {10.1090/conm/026/737400}
}


@article{fan2001variable,
  title={Variable selection via nonconcave penalized likelihood and its oracle properties},
  author={Fan, Jianqing and Li, Runze},
  journal={Journal of the American statistical Association},
  volume={96},
  number={456},
  pages={1348--1360},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{yang2016,
	author = {Yun Yang and Martin J. Wainwright and Michael I. Jordan},
	doi = {10.1214/15-AOS1417},
	journal = {The Annals of Statistics},
	keywords = {Bayesian variable selection, high-dimensional inference, Markov chain, rapid mixing, spectral gap},
	number = {6},
	pages = {2497 -- 2532},
	publisher = {Institute of Mathematical Statistics},
	title = {On the computational complexity of high-dimensional {B}ayesian variable selection},
	url = {https://doi.org/10.1214/15-AOS1417},
	volume = {44},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1214/15-AOS1417}}

@article{ahfock2021statistical,
  title={Statistical Properties of Sketching Algorithms},
  author={Ahfock, Daniel C and Astle, William J and Richardson, Sylvia},
  journal={Biometrika},
  volume={108},
  number={2},
  pages={283--297},
  year={2021},
  publisher={Oxford University Press},
  url = {https://doi.org/10.1093/biomet/asaa062}
}

@MISC {Did2018StackExchRatioBinom,
    TITLE = {Ratio of two binomial distributions},
    AUTHOR = {Did (https://math.stackexchange.com/users/6179/did)},
    HOWPUBLISHED = {Mathematics Stack Exchange},
    NOTE = {URL:https://math.stackexchange.com/q/2863492 (version: 2022-10-17)},
    EPRINT = {https://math.stackexchange.com/q/2863492},
    URL = {https://math.stackexchange.com/q/2863492}
}


@article{Forsgren2001WeighLS,
author = {Forsgren, Anders and Sporre, G\"{o}ran},
title = {On Weighted Linear Least-Squares Problems Related to Interior Methods for Convex Quadratic Programming},
journal = {SIAM Journal on Matrix Analysis and Applications},
volume = {23},
number = {1},
pages = {42-56},
year = {2001},
doi = {https://doi.org/10.1137/S0895479800372298},
URL = {
        https://doi.org/10.1137/S0895479800372298
},
eprint = {
        https://doi.org/10.1137/S0895479800372298
}
,
    abstract = { It is known that the norm of the solution to a weighted linear least-squares problem is uniformly bounded for the set of diagonally dominant symmetric positive definite weight matrices. This result is extended to weight matrices that are nonnegative linear combinations of symmetric positive semidefinite matrices. Further, results are given concerning the strong connection between the boundedness of weighted projection onto a subspace and the projection onto its complementary subspace using the inverse weight matrix. In particular, explicit bounds are given for the Euclidean norm of the projections. These results are applied to the Newton equations arising in a primal-dual interior method for convex quadratic programming and boundedness is shown for the corresponding projection operator. }
}

@article{Silverstein1985SmEVWish,
 ISSN = {00911798},
 URL = {http://www.jstor.org/stable/2244186},
 author = {Jack W. Silverstein},
 journal = {The Annals of Probability},
 number = {4},
 pages = {1364--1368},
 publisher = {Institute of Mathematical Statistics},
 title = {The Smallest Eigenvalue of a Large Dimensional Wishart Matrix},
 urldate = {2023-08-31},
 volume = {13},
 year = {1985}
}



@misc{karoui2003largestWishEV,
      title={On the largest eigenvalue of Wishart matrices with identity covariance when n, p and p/n tend to infinity},
      author={El Karoui, Noureddine},
      year={2003},
      eprint={math/0309355},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}

@article{TracyWidom1996,
  doi = {https://doi.org/10.1007/bf02099545},
  url = {https://doi.org/10.1007/bf02099545},
  year = {1996},
  month = apr,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {177},
  number = {3},
  pages = {727--754},
  author = {Craig A. Tracy and Harold Widom},
  title = {On orthogonal and symplectic matrix ensembles},
  journal = {Communications in Mathematical Physics}
}

@article{Johnstone2001LarEVinPCA,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2674106},
 abstract = {Let x(1) denote the square of the largest singular value of an n × p matrix X, all of whose entries are independent standard Gaussian variates. Equivalently, x(1) is the largest principal component variance of the covariance matrix X'X, or the largest eigenvalue of a p-variate Wishart distribution on n degrees of freedom with identity covariance. Consider the limit of large p and n with n/p = γ ≥ 1. When centered by $\mu_p = (\sqrt{n - 1} + \sqrt p)^2$ and scaled by $\sigma_p = (\sqrt{n - 1} + \sqrt p)(1/\sqrt{n - 1} + 1/ \sqrt p)^{1/3}$, the distribution of x(1) approaches the Tracy-Widom law of order 1, which is defined in terms of the Painleve II differential equation and can be numerically evaluated and tabulated in software. Simulations show the approximation to be informative for n and p as small as 5. The limit is derived via a corresponding result for complex Wishart matrices using methods from random matrix theory. The result suggests that some aspects of large p multivariate distribution theory may be easier to apply in practice than their fixed p counterparts.},
 author = {Iain M. Johnstone},
 journal = {The Annals of Statistics},
 number = {2},
 pages = {295--327},
 publisher = {Institute of Mathematical Statistics},
 title = {On the Distribution of the Largest Eigenvalue in Principal Components Analysis},
 urldate = {2023-09-05},
 volume = {29},
 year = {2001}
}

@Inbook{TracyWidom2000,
author="Tracy, Craig A.
and Widom, Harold",
editor="van Diejen, Jan Felipe
and Vinet, Luc",
title="The Distribution of the Largest Eigenvalue in the Gaussian Ensembles: $\beta$ = 1, 2, 4",
bookTitle="Calogero---Moser--- Sutherland Models",
year="2000",
publisher="Springer New York",
address="New York, NY",
pages="461--472",
abstract="The focus of this survey is on the distribution function FN$\beta$ (t) for the largest eigenvalue in the finite N Gaussian Orthogonal Ensemble (GOE, $\beta$ = 1), the Gaussian Unitary Ensemble (GUE, $\beta$ = 2), and the Gaussian Symplectic Ensemble (GSE, $\beta$ = 4) in the edge scaling limit of N {\textrightarrow} ∝. These limiting distribution functions are expressible in terms of a particular Painlev{\'e} II function. Comparisons are made with finite N simulations and of the universality of these distribution functions is discussed.",
isbn="978-1-4612-1206-5",
doi="https://doi.org/10.1007/978-1-4612-1206-5_29",
url="https://doi.org/10.1007/978-1-4612-1206-5_29"
}

@article{Karoui2007LarEVComplexWish,
author = {Noureddine El Karoui},
title = {{Tracy–Widom limit for the largest eigenvalue of a large class of complex sample covariance matrices}},
volume = {35},
journal = {The Annals of Probability},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {663 -- 714},
keywords = {operator determinants, Random matrix theory, steepest descent analysis, Toeplitz matrices, trace class operators, Tracy–Widom distributions, Wishart matrices},
year = {2007},
doi = {https://doi.org/10.1214/009117906000000917},
URL = {https://doi.org/10.1214/009117906000000917}
}


@article{EDELMAN1991DistrSmEVWishart,
title = {The distribution and moments of the smallest eigenvalue of a random matrix of wishart type},
journal = {Linear Algebra and its Applications},
volume = {159},
pages = {55-80},
year = {1991},
issn = {0024-3795},
doi = {https://doi.org/10.1016/0024-3795(91)90076-9},
url = {https://www.sciencedirect.com/science/article/pii/0024379591900769},
author = {Alan Edelman},
abstract = {Given a random rectangular m × n matrix with elements from a normal distribution, what is the distribution of the smallest singular value? To pose an equivalent question in the language of multivariate statistics, what is the distribution of the smallest eigenvalue of a matrix from the central Wishart distribution in the null case? We give new results giving the distribution as a simple recursion. This includes the more difficult case when n – m is an even integer, without resorting to zonal polynomials and hypergeometric functions of matrix arguments. With the recursion, one can obtain exact expressions for the density and the moments of the distribution in terms of functions usually no more complicated than polynomials, exponentials, and at worst ordinary hypergeometric functions. We further elaborate on the special cases when n – m = 0, 1, 2, and 3 and give a numerical table of the expected values for 2 ⩽ m ⩽ 25 and 0 ⩽ n – m ⩽ 25.}
}

@article{Edelman1988CondNumberWIsh,
author = {Edelman, Alan},
title = {Eigenvalues and Condition Numbers of Random Matrices},
journal = {SIAM Journal on Matrix Analysis and Applications},
volume = {9},
number = {4},
pages = {543-560},
year = {1988},
doi = {https://doi.org/10.1137/0609045},
URL = {         https://doi.org/10.1137/0609045},
eprint = {         https://doi.org/10.1137/0609045},
abstract = { Given a random matrix, what condition number should be expected? This paper presents a proof that for real or complex \$n \times n\$ matrices with elements from a standard normal distribution, the expected value of the log of the 2-norm condition number is asymptotic to \$\log n\$ as \$n \to \infty\$. In fact, it is roughly \$\log n + 1.537\$ for real matrices and \$\log n + 0.982\$ for complex matrices as \$n \to \infty\$. The paper discusses how the distributions of the condition numbers behave for large n for real or complex and square or rectangular matrices. The exact distributions of the condition numbers of \$2 \times n\$ matrices are also given.Intimately related to this problem is the distribution of the eigenvalues of Wishart matrices. This paper studies in depth the largest and smallest eigenvalues, giving exact distributions in some cases. It also describes the behavior of all the eigenvalues, giving an exact formula for the expected characteristic polynomial. }
}

@article{Chen2005CondNumberRandMat,
author = {Chen, Zizhong and Dongarra, Jack J.},
title = {Condition Numbers of Gaussian Random Matrices},
journal = {SIAM Journal on Matrix Analysis and Applications},
volume = {27},
number = {3},
pages = {603-620},
year = {2005},
doi = {https://doi.org/10.1137/040616413},
URL = {
        https://doi.org/10.1137/040616413
},
eprint = {
        https://doi.org/10.1137/040616413
}
,
    abstract = { Let \$G\_{m \times n}\$ be an \$m \times n\$ real random matrix whose elements are independent and identically distributed standard normal random variables, and let \$\kappa\_2(G\_{m \times n})\$ be the 2-norm condition number of \$G\_{m \times n}\$. We prove that, for any \$m \geq 2\$, \$n \geq 2\$, and \$x \geq |n-m|+1\$, \$\kappa\_2(G\_{m \times n})\$ satisfies \${\scriptsize \frac{1}{\sqrt{2\pi}}} ( { c }/{x} )^{|n-m|+1} < P{\scriptsize(\frac{\kappa\_2(G\_{m \times n})} {{n}/{(|n-m|+1)}} > x )} < {\scriptsize \frac{1}{\sqrt{2\pi}}} ( { C }/{x} )^{|n-m|+1},\$ where \$0.245 \leq c \leq 2.000\$ and \$5.013\$ \$\leq C \leq 6.414\$ are universal positive constants independent of m, n, and x. Moreover, for any \$m \geq 2\$ and \$n \geq 2\$, \$E(\log\kappa\_2(G\_{m \times n})) < \log{\scriptsize \frac{n}{|n-m|+1}} + 2.258.\$ A similar pair of results for complex Gaussian random matrices is also established. }
}



@Article{glmnetR,
    title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
    author = {Jerome Friedman and Robert Tibshirani and Trevor Hastie},
    journal = {Journal of Statistical Software},
    year = {2010},
    volume = {33},
    number = {1},
    pages = {1--22},
    doi = {https://doi.org/10.18637/jss.v033.i01},
  }

  @Manual{FlareR,
    title = {flare: Family of Lasso Regression},
    author = {Xingguo Li and Tuo Zhao and Lie Wang and Xiaoming Yuan and Han Liu},
    year = {2022},
    note = {\proglang{R}~package version 1.7.0.1},
    url = {https://CRAN.R-project.org/package=flare},
  }

  @Article{SISR,
    title = {\pkg{SIS}: An \proglang{R} Package for Sure Independence Screening in Ultrahigh-Dimensional Statistical Models},
    author = {Diego Franco Saldana and Yang Feng},
    journal = {Journal of Statistical Software},
    year = {2018},
    volume = {83},
    number = {2},
    pages = {1--25},
    doi = {10.18637/jss.v083.i02},
  }

  @Manual{SplitRegR,
    title = {SplitReg: Split Regularized Regression},
    author = {Anthony Christidis and Ezequiel Smucler and Ruben Zamar},
    year = {2020},
    note = {\proglang{R}~package version 1.0.2},
    url = {https://CRAN.R-project.org/package=SplitReg},
  }

  @Book{ggplotR,
    author = {Hadley Wickham},
    title = {\pkg{ggplot2}: Elegant Graphics for Data Analysis},
    publisher = {Springer-Verlag New York},
    year = {2016},
    isbn = {978-3-319-24277-4},
    url = {https://ggplot2.tidyverse.org},
  }

  @article{Tenenbaum2000ISOMAPFaceData,
author = {Joshua B. Tenenbaum  and Vin de Silva  and John C. Langford },
title = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
journal = {Science},
volume = {290},
number = {5500},
pages = {2319-2323},
year = {2000},

URL = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2319},
abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs—30,000 auditory nerve fibers or 106 optic nerve fibers—a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.}}



@article{gruber2023forecasting,
      title={Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It depends!},
      author={Luis Gruber and Gregor Kastner},
      year={2023},
      eprint={2206.04902},
      archivePrefix={arXiv},
      primaryClass={econ.EM},
      doi = {https://doi.org/10.48550/arXiv.2206.04902},
      url = {https://doi.org/10.48550/arXiv.2206.04902}
}

@BOOK{Anderson2003MVStat,
  title     = "An introduction to multivariate statistical analysis",
  author    = "Anderson, T W",
  publisher = "John Wiley \& Sons",
  series    = "Wiley Series in Probability and Statistics",
  edition   =  3,
  month     =  jul,
  year      =  2003,
  address   = "Nashville, TN",
  language  = "en"
}

@article{Geppert2015RPforBayReg,
  doi = {https://doi.org/10.1007/s11222-015-9608-z},
  url = {https://doi.org/10.1007/s11222-015-9608-z},
  year = {2015},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {27},
  number = {1},
  pages = {79--101},
  author = {Leo N. Geppert and Katja Ickstadt and Alexander Munteanu and Jens Quedenfeld and Christian Sohler},
  title = {Random projections for Bayesian regression},
  journal = {Statistics and Computing}
}

@TechReport{parzer2024glms,
    title = {Data-Driven Random Projection and Screening for High-Dimensional Generalized Linear Models},
    author = {Roman Parzer and Peter Filzmoser and Laura Vana-Gür},
    institution = {arXiv.org E-Print Archive},
    year = {2024},
    number = {2410.00971},
    doi = {10.48550/arXiv.2410.00971}
}

@TechReport{parzer2024sparse,
    title = {{Sparse Data-Driven Random Projection in Regression for High-Dimensional Data}},
    author = {Roman Parzer and Peter Filzmoser and Laura Vana-Gür},
    institution = {arXiv.org E-Print Archive},
    year = {2024},
    number = {2312.00130},
    doi = {10.48550/arXiv.2312.00130},
}


@TechReport{ryder2019asymmetric,
      title={Asymmetric Random Projections},
      author={Nick Ryder and Zohar Karnin and Edo Liberty},
      year={2019},
      institution = {arXiv.org E-Print Archive},
    number = {1906.09489},
    doi = {10.48550/arXiv.1906.09489},
}

@misc{parzer2023sparse,
      title={Sparse Projected Averaged Regression for High-Dimensional Data},
      author={Roman Parzer and Laura Vana-Gür and Peter Filzmoser},
      year={2023},
      eprint={2312.00130},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}


@article{HOU2011PPWentzell,
title = {Fast and simple methods for the optimization of kurtosis used as a projection pursuit index},
journal = {Analytica Chimica Acta},
volume = {704},
number = {1},
pages = {1-15},
year = {2011},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2011.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0003267011010804},
author = {S. Hou and P.D. Wentzell},
keywords = {Optimization, Quasi-power method, Univariate kurtosis, Multivariate kurtosis, Projection pursuit, Independent component analysis},
abstract = {As a powerful method for exploratory data analysis, projection pursuit (PP) often outperforms principal component analysis (PCA) to discover important data structure. PP was proposed in 1970s but has not been widely used in chemistry largely because of the difficulty in the optimization of projection indices. In this work, new algorithms, referred as “quasi-power methods”, are proposed to optimize kurtosis as a projection index. The new algorithms are simple, fast, and stable, which makes the search for the global optimum more efficient in the presence of multiple local optima. Maximization of kurtosis is helpful in the detection of outliers, while minimization tends to reveal clusters in the data, so the ability to search separately for the maximum and minimum of kurtosis is desirable. The proposed algorithms can search for either with only minor changes. Unlike other methods, no optimization of step size is required and sphering or whitening of the data is not necessary. Both univariate and multivariate kurtosis can be optimized by the proposed algorithms. The performance of the algorithms is evaluated using three simulated data sets and its utility is demonstrated with three experimental data sets relevant to analytical chemistry.}
}

@INCOLLECTION{Pappu2014HDCOverview,
title = {High-Dimensional Data Classification},
author = {Pappu, Vijay and Pardalos, Panos M.},
year = {2014},
pages = {119-150},
publisher = {Springer},
abstract = {Abstract Recently, high-dimensional classification problems have been ubiquitous due to significant advances in technology. High dimensionality poses significant statistical challenges and renders many traditional classification algorithms impractical to use. In this chapter, we present a comprehensive overview of different classifiers that have been highly successful in handling high-dimensional data classification problems. We start with popular methods such as Support Vector Machines and variants of discriminant functions and discuss in detail their applications and modifications to several problems in high-dimensional settings. We also examine regularization techniques and their integration to several existing algorithms. We then discuss more recent methods, namely the hybrid classifiers and the ensemble classifiers. Feature selection techniques, as a part of hybrid classifiers, are introduced and their relative merits and drawbacks are examined. Lastly, we describe AdaBoost and Random Forests in the ensemble classifiers and discuss their recent surge as useful algorithms for solving high-dimensional data problems.},
keywords = {High dimensional data classification; Ensemble methods; Feature selection; Curse of dimensionality; Regularization},
url = {https://EconPapers.repec.org/RePEc:spr:spochp:978-1-4939-0742-7_8}
}



@misc{Salaro2018MultiLogitHD,
      title={Multinomial Logistic Regression With High Dimensional Data},
      author={Rossana Salaro and Cristiano Varin},
      year={2018},
      note         = {Available at \url{http://dspace.unive.it/bitstream/handle/10579/13814/847168-1223506.pdf?sequence=2}},
      eprint={1906.09489},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{RaeisiShahraki2017KIN,
  title={K Important Neighbors: A Novel Approach to Binary Classification in High Dimensional Data},
  author={Hadi Raeisi Shahraki and Saeedeh Pourahmad and Najaf Zare},
  journal={BioMed Research International},
  year={2017},
  volume={2017},
  url={https://api.semanticscholar.org/CorpusID:23353660}
}

@article{fan2009ultrahigh,
  title={Ultrahigh Dimensional Feature Selection: Beyond the Linear Model},
  author={Fan, Jianqing and Samworth, Richard and Wu, Yichao},
  journal={The Journal of Machine Learning Research},
  volume={10},
  pages={2013--2038},
  year={2009},
url={https://jmlr.csail.mit.edu/papers/v10/fan09a.html},
  publisher={JMLR. org}
}
@article{xie2016comparison,
  title={Comparison among dimensionality reduction techniques based on Random Projection for cancer classification},
  author={Xie, Haozhe and Li, Jie and Zhang, Qiaosheng and Wang, Yadong},
  journal={Computational biology and chemistry},
  volume={65},
  pages={165--172},
  year={2016},
  publisher={Elsevier},
  doi = {https://doi.org/10.1016/j.compbiolchem.2016.09.010}
}

@article{cook2019partial,
  title={Partial least squares prediction in high-dimensional regression},
  author={Cook, R Dennis and Forzani, Liliana},
  journal={The Annals of Statistics},
  volume={47},
  number={2},
  pages={884--908},
  year={2019},
  publisher={JSTOR}
}

@article{cannings2021overview,
	abstract = {Abstract Random projections offer an appealing and flexible approach to a wide range of large-scale statistical problems. They are particularly useful in high-dimensional settings, where we have many covariates recorded for each observation. In classification problems, there are two general techniques using random projections. The first involves many projections in an ensemble---the idea here is to aggregate the results after applying different random projections, with the aim of achieving superior statistical accuracy. The second class of methods include hashing and sketching techniques, which are straightforward ways to reduce the complexity of a problem, perhaps therefore with a huge computational saving, while approximately preserving the statistical efficiency. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences > Clustering and Classification Statistical and Graphical Methods of Data Analysis > Analysis of High Dimensional Data Statistical Models > Classification Models},
	author = {Cannings, Timothy I.},
	doi = {https://doi.org/10.1002/wics.1499},
	eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1499},
	journal = {WIREs Computational Statistics},
	keywords = {classification, data perturbation, ensemble, high dimensional, large scale, random projection, sketching},
	number = {1},
	pages = {e1499},
	title = {Random projections: Data perturbation for classification problems},
	url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1499},
	volume = {13},
	year = {2021},
	bdsk-url-1 = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1499},
	bdsk-url-2 = {https://doi.org/10.1002/wics.1499}}

@article{Wang2019HDRinPracticeStudy,
   title={High-dimensional regression in practice: an empirical study of finite-sample prediction, variable selection and ranking},
   volume={30},
   ISSN={1573-1375},
   url={http://dx.doi.org/10.1007/s11222-019-09914-9},
   DOI={10.1007/s11222-019-09914-9},
   number={3},
   journal={Statistics and Computing},
   publisher={Springer Science and Business Media LLC},
   author={Wang, Fan and Mukherjee, Sach and Richardson, Sylvia and Hill, Steven M.},
   year={2019},
   month=dec, pages={697–719} }

@Manual{plsR,
    title = {pls: Partial Least Squares and Principal Component Regression},
    author = {Kristian Hovde Liland and Bjørn-Helge Mevik and Ron Wehrens},
    year = {2022},
    note = {\proglang{R}~package version 2.8-1},
    url = {https://CRAN.R-project.org/package=pls},
}

@Manual{RPEnsembleR,
    title = {\pkg{RPEnsemble}: Random Projection Ensemble Classification},
    author = {Timothy I. Cannings and Richard J. Samworth},
    year = {2021},
    note = {\proglang{R}~package version 0.5},
    url = {10.32614/CRAN.package.RPEnsemble},
}

  @Manual{SPCAvRPR,
    title = {\pkg{SPCAvRP}: Sparse Principal Component Analysis via Random Projections
(SPCAvRP)},
    author = {Milana Gataric and Tengyao Wang and Richard J. Samworth},
    year = {2019},
    note = {\proglang{R}~package version 0.4},
    url = {10.32614/CRAN.package.SPCAvRP},
  }
@Manual{RandProR,
    title = {\pkg{RandPro}: Random Projection with Classification},
    author = {Aghila, G and Siddharth, R},
    year = {2020},
    note = {\proglang{R}~package version 0.2.2},
    doi = {10.32614/CRAN.package.RandPro},
  }

@article{ailon2009fast,
  title={The Fast Johnson--Lindenstrauss Transform and Approximate Nearest Neighbors},
  author={Ailon, Nir and Chazelle, Bernard},
  journal={SIAM Journal on computing},
  volume={39},
  number={1},
  pages={302--322},
  year={2009},
  publisher={SIAM},
  doi={10.1137/060673096}
}

@article{SIDDHARTH2020100629,
	author = {R. Siddharth and G. Aghila},
	doi = {10.1016/j.softx.2020.100629},
	issn = {2352-7110},
	journal = {SoftwareX},
	keywords = {Dimension reduction, Johnson--Lindenstrauss lemma, RandPro, R Programming, Software description},
	pages = {100629},
	title = {\pkg{RandPro} -- A Practical Implementation of Random Projection-Based Feature Extraction for High Dimensional Multivariate Data Analysis in \proglang{R}},
	volume = {12},
	year = {2020},
}

@Article{pkg:ball,
    title = {\pkg{Ball}: An \proglang{R} Package for Detecting Distribution
      Difference and Association in Metric Spaces},
    author = {Jin Zhu and Wenliang Pan and Wei Zheng and Xueqin Wang},
    journal = {Journal of Statistical Software},
    year = {2021},
    volume = {97},
    number = {6},
    pages = {1--31},
    doi = {10.18637/jss.v097.i06},
  }

  @Manual{pkg:BayesS5,
    title = {BayesS5: Bayesian Variable Selection Using Simplified Shotgun Stochastic
Search with Screening (S5)},
    author = {Minsuk Shin and Ruoxuan Tian},
    year = {2020},
    note = {\proglang{R}~package version 1.41},
    url = {https://CRAN.R-project.org/package=BayesS5},
  }
  @misc{shin2017scalablebayesianvariableselection,
      title={Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-Dimensional Settings},
      author={Minsuk Shin and Anirban Bhattacharya and Valen E. Johnson},
      year={2017},
      eprint={1507.07106},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1507.07106},
}

  @Manual{pkg:bravo,
    title = {bravo: Bayesian Screening and Variable Selection},
    author = {Dongjin Li and Debarshi Chakraborty and Somak Dutta and Vivekananda Roy},
    year = {2024},
    note = {\proglang{R}~package version 3.2.1},
    url = {10.32614/CRAN.package.bravo},
  }
@misc{wang2021bayesianiterativescreeningultrahigh,
      title={Bayesian iterative screening in ultra-high dimensional settings},
      author={Run Wang and Somak Dutta and Vivekananda Roy},
      year={2021},
      eprint={2107.10175},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2107.10175},
}

  @Manual{pkg:VariableScreening,
    title = {\pkg{VariableScreening}: High-Dimensional Screening for Semiparametric Longitudinal
Regression},
    author = {Runze Li and Liying Huang and John Dziak},
    year = {2022},
    note = {\proglang{R}~package version 0.2.1},
    doi = {10.32614/CRAN.package.VariableScreening},
  }
  @article{cui2015model,
  title={Model-Free Feature Screening for Ultrahigh Dimensional Discriminant Analysis},
  author={Cui, Hengjian and Li, Runze and Zhong, Wei},
  journal={Journal of the American Statistical Association},
  volume={110},
  number={510},
  pages={630--641},
  doi={10.1080/01621459.2014.920256},
  year={2015},
  publisher={Taylor \& Francis}
}
@article{li2012feature,
  title={Feature Screening via Distance Correlation Learning},
  author={Li, Runze and Zhong, Wei and Zhu, Liping},
  journal={Journal of the American Statistical Association},
  volume={107},
  number={499},
  pages={1129--1139},
  year={2012},
  doi={10.1080/01621459.2012.695654},
  publisher={Taylor \& Francis}
}
   @Manual{pkg:tilting,
    title = {tilting: Variable Selection via Tilted Correlation Screening Algorithm},
    author = {Haeran Cho and Piotr Fryzlewicz},
    year = {2016},
    note = {\proglang{R}~package version 1.1.1},
    doi = {10.32614/CRAN.package.tilting},
  }

  @Manual{pkg:MFSIS,
    title = {\pkg{MFSIS}: Model-Free Sure Independent Screening Procedures},
    author = {Xuewei Cheng and Hong Wang and Liping Zhu and Wei Zhong and Hanpu Zhou},
    year = {2024},
    note = {\proglang{R}~package version 0.2.1},
    doi = {10.32614/CRAN.package.MFSIS},
  }
   @Manual{pkg:cdcsis,
    title = {cdcsis: Conditional Distance Correlation Based Feature Screening and
Conditional Independence Inference},
    author = {Wenhao Hu and Mian Huang and Wenliang Pan and Xueqin Wang and Canhong Wen and Yuan Tian and Heping Zhang and Jin Zhu},
    year = {2024},
    note = {\proglang{R}~package version 2.0.4},
    doi = {10.32614/CRAN.package.cdcsis},
  }
  @Manual{pkg:QCSIS,
    title = {QCSIS: Sure Independence Screening via Quantile Correlation and
Composite Quantile Correlation},
    author = {Xuejun Ma and Jingxiao Zhang and Jingke Zhou},
    year = {2015},
    note = {R package version 0.1},
    url = {https://CRAN.R-project.org/package=QCSIS},
  }
  @Manual{pkg:fusionclust,
    title = {fusionclust: Clustering and Feature Screening using L1 Fusion Penalty},
    author = {Trambak Banerjee and Gourab Mukherjee and Peter Radchenko},
    year = {2017},
    note = {R package version 1.0.0},
    url = {https://CRAN.R-project.org/package=fusionclust},
  }
    @Manual{pkg:LqG,
    title = {LqG: Robust Group Variable Screening Based on Maximum Lq-Likelihood
Estimation},
    author = {Mingcong Wu and Yang Li and Rong Li},
    year = {2022},
    note = {R package version 0.1.0},
    url = {https://CRAN.R-project.org/package=LqG},
  }
    @Manual{pkg:SMLE,
    title = {{SMLE}:An {R} Package for Joint Feature Screening in
      Ultrahigh-dimensional GLMs},
    author = {Qianxiang Zang and Chen Xu and Kelly Burkett},
    year = {2020},
  }
  @Article{SMLE2014,
    title = {The Sparse MLE for Ultrahigh-Dimensional Feature
      Screening},
    author = {Chen Xu and Jiahua Chen},
    journal = {Journal of the American Statistical Association},
    volume = {109},
    number = {507},
    pages = {1257-1269},
    doi={10.1080/01621459.2013.879531},
    year = {2014},
  }
    @Manual{pkg:TSGSIS,
    title = {TSGSIS: Two Stage-Grouped Sure Independence Screening},
    author = {Yao-Hwei Fang and Jie-Huei Wang and Chao A. Hsiung},
    year = {2017},
    note = {R package version 0.1},
    doi = {10.32614/CRAN.package.TSGSIS},
  }

@article{10.1093/bioinformatics/btx409,
    author = {Fang, Yao-Hwei and Wang, Jie-Huei and Hsiung, Chao A},
    title = {TSGSIS: a high-dimensional grouped variable selection approach for detection of whole-genome SNP–SNP interactions},
    journal = {Bioinformatics},
    volume = {33},
    number = {22},
    pages = {3595-3602},
    year = {2017},
    month = {06},
    abstract = "{Identification of single nucleotide polymorphism (SNP) interactions is an important and challenging topic in genome-wide association studies (GWAS). Many approaches have been applied to detecting whole-genome interactions. However, these approaches to interaction analysis tend to miss causal interaction effects when the individual marginal effects are uncorrelated to trait, while their interaction effects are highly associated with the trait.A grouped variable selection technique, called two-stage grouped sure independence screening (TS-GSIS), is developed to study interactions that may not have marginal effects. The proposed TS-GSIS is shown to be very helpful in identifying not only causal SNP effects that are uncorrelated to trait but also their corresponding SNP–SNP interaction effects. The benefit of TS-GSIS are gaining detection of interaction effects by taking the joint information among the SNPs and determining the size of candidate sets in the model. Simulation studies under various scenarios are performed to compare performance of TS-GSIS and current approaches. We also apply our approach to a real rheumatoid arthritis (RA) dataset. Both the simulation and real data studies show that the TS-GSIS performs very well in detecting SNP–SNP interactions.R-package is delivered through CRAN and is available at: https://cran.r-project.org/web/packages/TSGSIS/index.html.Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btx409},
    url = {https://doi.org/10.1093/bioinformatics/btx409},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/33/22/3595/50307208/bioinformatics\_33\_22\_3595.pdf},
}


  @Manual{pkg:RaSEn,
    title = {\pkg{RaSEn}: Random Subspace Ensemble Classification and Variable Screening},
    author = {Ye Tian and Yang Feng},
    year = {2021},
    note = {\proglang{R} package version 3.0.0},
    doi = {10.32614/CRAN.package.RaSEn},
  }

@Manual{pkg:rmatlab,
    title = {\pkg{R.matlab}: Read and Write MAT Files and Call \proglang{MATLAB} from Within \proglang{R}},
    author = {Henrik Bengtsson},
    year = {2022},
    note = {R package version 3.7.0},
    doi = {10.32614/CRAN.package.R.matlab},
  }
@Manual{rcellwise,
    title = {\pkg{cellWise}: Analyzing Data with Cellwise Outliers},
    author = {Jakob Raymaekers and Peter Rousseeuw},
    year = {2023},
    note = {\proglang{R} package version 2.5.3},
    doi = {10.32614/CRAN.package.cellWise}
  }

@article{pedregosa2011scikit,
  title={\pkg{scikit-learn}: Machine Learning in \proglang{Python}},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011},
  url={https://www.jmlr.org/papers/v12/pedregosa11a.html}
}
